{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvVxxvpq2pGF"
   },
   "source": [
    "# Previsione della Qualità dell'Aria - **Allenamento con Dati Augmentati e Valutazione dei Modelli**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnXzcSTc2Z4u"
   },
   "source": [
    "## Caricamento dei Datasets ed Import Librerie\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OW2V7IPwnXAU"
   },
   "outputs": [],
   "source": [
    "!pip install pykan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o1UsULdR4Vs7"
   },
   "outputs": [],
   "source": [
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ts2wiQjcg4BV",
    "outputId": "c4cc621b-47d9-4b02-c292-48ea14f21008"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (3.0.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import inspect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    running_in_colab = True\n",
    "except ImportError:\n",
    "    running_in_colab = False\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    average_precision_score\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    TimeSeriesSplit,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    ParameterSampler\n",
    ")\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from torch.nn.functional import softmax\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "%pip install xgboost\n",
    "import xgboost as xgb\n",
    "from kan import *\n",
    "\n",
    "N_JOBS = -1\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-1nc39QrJSv"
   },
   "outputs": [],
   "source": [
    "if running_in_colab:\n",
    "    df = pd.read_csv('https://raw.githubusercontent.com/vMxster/Data_Project/main/Datasets/augmented_dataset.csv',\n",
    "                sep=',',\n",
    "                quotechar='\"',\n",
    "                dtype=None,\n",
    "                parse_dates=True,\n",
    "                low_memory=False)\n",
    "else:\n",
    "    df = pd.read_csv('augmented_dataset.csv',\n",
    "                sep=',',\n",
    "                quotechar='\"',\n",
    "                dtype=None,\n",
    "                parse_dates=True,\n",
    "                low_memory=False)\n",
    "obj_cols = df.select_dtypes(include=\"object\").columns\n",
    "for col in obj_cols:\n",
    "    df[col] = df[col].astype(\"category\")\n",
    "df.drop('date', axis=1, inplace=True)\n",
    "df = df[(df['year'] >= 2015) & (df['year'] <= 2017)]\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5J-7bPorgC5"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VHN34R2qmbEs"
   },
   "outputs": [],
   "source": [
    "class_counts = df['Class'].value_counts()\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kcQbIynRmGln"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Distribution of Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcIEc735lViq",
    "tags": []
   },
   "source": [
    "# Addestramento modelli\n",
    "A seguito dell'esplorazione e dell'omogeneizzazione dei due dataset, si può procedere all'addestramento dei modelli. I modelli verranno addestrati sulle seguenti feature indipendenti:\n",
    "- `year`: anno della misurazione\n",
    "- `month`: mese dell’anno\n",
    "- `dayofmonth`: giorno del mese\n",
    "- `dayofweek`: giorno della settimana\n",
    "- `dayofyear`: giorno dell’anno\n",
    "- `weekofyear`: settimana dell’anno\n",
    "- `quarter`: trimestre dell’anno\n",
    "- `state`: stato di misurazione\n",
    "- `pm_lag_1W`: PM2.5 ritardato di 1 settimana\n",
    "- `pm_lag_1M`: PM2.5 ritardato di 1 mese\n",
    "- `pm_lag_1Y`: PM2.5 ritardato di 1 anno\n",
    "- `pm_lag_1D`: PM2.5 ritardato di 1 giorno\n",
    "- `pm_lag_2D`: PM2.5 ritardato di 2 giorni\n",
    "- `pm_lag_3D`: PM2.5 ritardato di 3 giorni\n",
    "- `co_lag_1W`: CO ritardato di 1 settimana\n",
    "- `co_lag_1M`: CO ritardato di 1 mese\n",
    "- `co_lag_1Y`: CO ritardato di 1 anno\n",
    "- `co_lag_1D`: CO ritardato di 1 giorno\n",
    "- `co_lag_2D`: CO ritardato di 2 giorni\n",
    "- `co_lag_3D`: CO ritardato di 1 anno\n",
    "- `o3_lag_1W`: O3 ritardato di 1 settimana\n",
    "- `o3_lag_1M`: O3 ritardato di 1 mese\n",
    "- `o3_lag_1Y`: O3 ritardato di 1 anno\n",
    "- `o3_lag_1D`: O3 ritardato di 1 giorno\n",
    "- `o3_lag_2D`: O3 ritardato di 2 giorni\n",
    "- `o3_lag_3D`: O3 ritardato di 3 giorni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwC-dywTlViq"
   },
   "source": [
    "La variabile target per il nostro modello di addestramento sarà una variabile **discreta**, composta da 6 valori distinti. Questi valori rappresentano i diversi livelli di qualità dell'aria, come definiti dalla scala dell'**Environmental Protection Agency (EPA)** degli Stati Uniti per la concentrazione di PM2.5.\n",
    "\n",
    "Possiamo mappare numericamente questi livelli alle seguenti classi, mantenendo l'ordine implicito di gravità:\n",
    "\n",
    "* **1: \"Good\"** (Concentrazione di PM2.5: $0 - 9.0 \\mu g/m^3$)\n",
    "* **2: \"Moderate\"** (Concentrazione di PM2.5: $9.1 - 35.4 \\mu g/m^3$)\n",
    "* **3: \"Unhealthy for Sensitive Groups\"** (Concentrazione di PM2.5: $35.5 - 55.4 \\mu g/m^3$)\n",
    "* **4: \"Unhealthy\"** (Concentrazione di PM2.5: $55.5 - 125.4 \\mu g/m^3$)\n",
    "* **5: \"Very Unhealthy\"** (Concentrazione di PM2.5: $125.5 - 225.4 \\mu g/m^3$)\n",
    "* **6: \"Hazardous\"** (Concentrazione di PM2.5: $> 225.5 \\mu g/m^3$)\n",
    "\n",
    "Adottando questo schema, il problema si configura come un task di **classificazione multi-classe**. L'obiettivo del modello sarà prevedere a quale di questi 6 livelli di qualità dell'aria (o \"categorie di rischio\") appartiene una data osservazione, basandosi sulle caratteristiche di input fornite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GveBLWbLlViq"
   },
   "source": [
    "## Preparazione Dataset\n",
    "Per garantire un confronto equo tra tutti i modelli, alcuni dei quali non supportano i valori mancanti generati dalle lag features, elimineremo tutte le righe che li contengono. Va però tenuto presente che così facendo perdiamo un anno di dati storici. Modelli come XGBoost di scikit-learn sono in grado di gestire internamente i missing value e potrebbero beneficiarne; tuttavia, per mantenere omogenee le condizioni di allenamento, applichiamo il drop completo dei NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2u4WO1bmlViq"
   },
   "outputs": [],
   "source": [
    "target = 'Class'\n",
    "lag_features = ['pm_lag_1Y', 'pm_lag_2Y', 'pm_lag_1M', 'pm_lag_1W','co_lag_1Y', 'co_lag_2Y', 'co_lag_1M', 'co_lag_1W','o3_lag_1Y', 'o3_lag_2Y', 'o3_lag_1M', 'o3_lag_1W','pm_lag_1D','co_lag_1D','o3_lag_1D','pm_lag_2D','co_lag_2D','o3_lag_2D','pm_lag_3D','co_lag_3D','o3_lag_3D']\n",
    "date_features = ['dayofmonth', 'dayofweek', 'dayofyear', 'weekofyear', 'month', 'quarter', 'year', 'state']\n",
    "predictors = date_features + lag_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efbb3yDvuaDc"
   },
   "outputs": [],
   "source": [
    "def create_train_test_sets(dataframe, split, replace_na=False, method='none'):\n",
    "    dataframe = dataframe.copy()\n",
    "\n",
    "    if replace_na and method == 'zeros':\n",
    "      dataframe = dataframe.fillna(0)\n",
    "    elif replace_na and method == 'drop':\n",
    "      dataframe = dataframe.dropna(how='any')\n",
    "\n",
    "    train_set, test_set = np.split(dataframe, [int(len(dataframe) * split)])\n",
    "    return train_set[predictors], test_set[predictors], train_set[target], test_set[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_dgAHmhuh2i"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = create_train_test_sets(\n",
    "    df,\n",
    "    split=0.8,\n",
    "    replace_na=True,\n",
    "    method='drop'\n",
    ")\n",
    "\n",
    "# Resetta gli indici dei risultati eliminando l’indice precedente,\n",
    "# in modo da partire da zero ed avere indici continui\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Mz9jup_67xf"
   },
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gl2OvBn56_PE"
   },
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rjuwm2aHnklP"
   },
   "outputs": [],
   "source": [
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36laCti0nwg2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(y_train.value_counts(), labels=y_train.value_counts().index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Distribution of Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPNDBjlAn3gU"
   },
   "outputs": [],
   "source": [
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "595RhSWPn6LP"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(y_test.value_counts(), labels=y_test.value_counts().index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Distribution of Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqBCT7crrf9A"
   },
   "source": [
    "Nel dataset ci sono sia feature numeriche che categoriche. <br>\n",
    "Per le numeriche è necessario applicare una normalizzazione dei dati, i quali avrebbero altrimenti valori su scale molto diverse che renderebbero più difficile la convergenza del modello. <br>\n",
    "Per poter utilizzare le variabili categoriche nell'addestramento di un modello di regressione si usa un OneHotEncoder, creando nuove colonne binarie per ciascuno dei valori ammissibili dalla variabile categorica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YcK3Ma4545-a"
   },
   "outputs": [],
   "source": [
    "categorical_features = X_train.select_dtypes(include=[\"category\"]).columns.tolist()\n",
    "numerical_features   = [c for c in X_train.columns if c not in categorical_features]\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    # Standardizza tutte le colonne numeriche\n",
    "    (\"numeric\",    StandardScaler(),    numerical_features),\n",
    "    # One‑hot encoding di 'state', ignorando nuovi stati in predict\n",
    "    (\"categorical\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axPc4fYaKiDT"
   },
   "source": [
    "Inizializzazione della lista per raccogliere le metriche dopo ogni training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LfV4nYJKiPx"
   },
   "outputs": [],
   "source": [
    "all_scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_bIhHTRKAzQ"
   },
   "source": [
    "## Valutazione delle Prestazioni dei Modelli\n",
    "\n",
    "Dopo l’allenamento di ciascun modello di Machine Learning e Deep Learning, utilizziamo le funzioni `get_estimator_scores` e `get_torch_estimator_scores` per calcolare diverse metriche di valutazione, includendo per le principali anche gli **Intervalli di Confidenza al 95%** (CI95%) stimati tramite bootstrap resampling.\n",
    "\n",
    "Queste metriche sono state scelte specificamente per valutare l'efficacia dei modelli in problemi di **classificazione multi-classe, con particolare attenzione alle classi sbilanciate**.\n",
    "\n",
    "Le metriche calcolate sono:\n",
    "\n",
    "1.  **Accuracy Score**\n",
    "    Misura la percentuale di istanze classificate correttamente dal modello. Sebbene sia una metrica intuitiva, può essere fuorviante in presenza di classi sbilanciate, poiché un modello che classifica correttamente solo la classe maggioritaria può comunque mostrare un'alta accuratezza.\n",
    "    *(Più alto è, meglio è.)*\n",
    "\n",
    "2.  **F1-Score (Weighted)**\n",
    "    L'F1-Score è la media armonica della Precisione e del Recall. La versione \"weighted\" calcola la media di F1-Score per ciascuna classe, pesandola in base al numero di istanze di quella classe nel dataset. Questa metrica è particolarmente utile per dataset con classi sbilanciate, in quanto fornisce una visione più bilanciata delle prestazioni del modello su tutte le classi, evitando di essere dominata dalla classe maggioritaria.\n",
    "    *(Più alto è, meglio è.)*\n",
    "\n",
    "3.  **F1-Score (Macro)**\n",
    "    La versione \"macro\" dell'F1-Score calcola la media non pesata di F1-Score per ciascuna classe. Questo significa che ogni classe contribuisce in modo uguale alla metrica finale, indipendentemente dal suo numero di campioni. È utile quando tutte le classi, comprese quelle minoritarie, hanno la stessa importanza.\n",
    "    *(Più alto è, meglio è.)*\n",
    "\n",
    "4.  **Matrice di Confusione**\n",
    "    Una tabella che riassume le prestazioni di un algoritmo di classificazione. Mostra il numero di previsioni corrette e sbagliate per ciascuna classe, indicando dove il modello sta confondendo le diverse categorie. È fondamentale per comprendere gli errori specifici del modello.\n",
    "\n",
    "5.  **Classification Report**\n",
    "    Fornisce un riepilogo dettagliato delle metriche di Precisione, Recall e F1-Score per ciascuna classe, oltre a metriche aggregate (macro avg, weighted avg). È uno strumento essenziale per una valutazione approfondita delle prestazioni per classe.\n",
    "\n",
    "6.  **AUC-ROC (Area Under the Receiver Operating Characteristic Curve) - One-vs-Rest Weighted**\n",
    "    L'AUC-ROC misura la capacità del modello di distinguere tra le classi. Per problemi multi-classe, si calcola spesso in modalità \"one-vs-rest\", trattando ogni classe come \"positiva\" e tutte le altre come \"negative\". La versione \"weighted\" ne calcola la media pesata per la frequenza delle classi, rendendola più robusta per dataset sbilanciati. Un valore più vicino a 1 indica una maggiore capacità discriminatoria.\n",
    "    *(Più alto è, meglio è.)*\n",
    "\n",
    "7.  **AUC-PR (Area Under the Precision-Recall Curve) - One-vs-Rest Weighted**\n",
    "    L'AUC-PR è particolarmente utile per dataset con classi sbilanciate e quando la classe positiva (minoritaria) è di maggiore interesse. Misura l'area sotto la curva Precision-Recall, offrendo una valutazione più accurata della capacità del modello di identificare correttamente le istanze positive rispetto all'AUC-ROC, che può essere ottimistica in presenza di un grande numero di veri negativi. Per multi-classe, viene calcolata in modalità \"one-vs-rest\" e mediata pesando per la frequenza delle classi.\n",
    "    *(Più alto è, meglio è.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zthJQ-LRL-8J"
   },
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    if hasattr(model, 'parameters') and inspect.ismethod(model.parameters) and not isinstance(model, KAN):\n",
    "        try:\n",
    "            return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        except Exception:\n",
    "            return 0\n",
    "        \n",
    "    elif isinstance(model, KAN):\n",
    "        try:\n",
    "            if not model.width or len(model.width) < 2:\n",
    "                return 0\n",
    "            \n",
    "            sum_ab_terms = 0\n",
    "            for i in range(len(model.width) - 1):\n",
    "                a = model.width[i]\n",
    "                b = model.width[i+1]\n",
    "\n",
    "                if isinstance(a, list):\n",
    "                    a = a[0]\n",
    "                if isinstance(b, list):\n",
    "                    b = b[0]\n",
    "                \n",
    "                sum_ab_terms += (a * b)\n",
    "\n",
    "            return sum_ab_terms * (model.grid + model.k)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating KAN parameters: {e}\")\n",
    "            return 0\n",
    "\n",
    "    elif isinstance(model, RandomForestClassifier):\n",
    "        total_nodes = 0\n",
    "        if hasattr(model, 'estimators_'):\n",
    "            for tree in model.estimators_:\n",
    "                if hasattr(tree, 'tree_'):\n",
    "                    total_nodes += tree.tree_.node_count\n",
    "            return total_nodes\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    elif isinstance(model, xgb.XGBClassifier):\n",
    "        total_nodes = 0\n",
    "\n",
    "        try:\n",
    "            booster = model.get_booster()\n",
    "            tree_dumps = booster.get_dump(dump_format='json')\n",
    "\n",
    "            def count_nodes_in_json_tree(node):\n",
    "                count = 1\n",
    "                if 'children' in node:\n",
    "                    for child in node['children']:\n",
    "                        count += count_nodes_in_json_tree(child)\n",
    "                return count\n",
    "\n",
    "            for tree_dump_str in tree_dumps:\n",
    "                tree_json = json.loads(tree_dump_str)\n",
    "                total_nodes += count_nodes_in_json_tree(tree_json)\n",
    "\n",
    "            return total_nodes\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating exact XGBoost complexity: {e}\")\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def bootstrap_ci(metric_fn, y_true, y_pred, n_bootstraps=1000, alpha=0.05, **metric_kwargs):\n",
    "    y_true_arr = np.asarray(y_true)\n",
    "    y_pred_arr = np.asarray(y_pred)\n",
    "    vals = []\n",
    "    n_samples = len(y_true)\n",
    "\n",
    "    for _ in range(n_bootstraps):\n",
    "        idx = np.random.randint(0, n_samples, n_samples)\n",
    "        vals.append(metric_fn(y_true_arr[idx], y_pred_arr[idx], **metric_kwargs))\n",
    "\n",
    "    low = np.percentile(vals, 100 * (alpha / 2))\n",
    "    high = np.percentile(vals, 100 * (1 - alpha / 2))\n",
    "    return low, high\n",
    "\n",
    "def get_estimator_scores(model_name, model, X_test, y_test, X_train, y_train, all_scores_list):\n",
    "    print(f\"\\n--- Valutazione Prestazioni per {model_name} ---\")\n",
    "\n",
    "    # Calcola la Complessitá dei Modelli\n",
    "    param_count = count_params(model)\n",
    "    print(f\"Model Parameters/Nodes: {param_count}\")\n",
    "\n",
    "    # Previsioni (etichette hard) per test e train\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "\n",
    "    # Se il modello supporta predict_proba (per AUC)\n",
    "    y_proba_test = None\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_proba_test = model.predict_proba(X_test) # Probabilità per ogni classe\n",
    "\n",
    "    # --- Metriche su Training Set ---\n",
    "    accuracy_tr = accuracy_score(y_train, y_pred_train)\n",
    "    f1_weighted_tr = f1_score(y_train, y_pred_train, average='weighted', zero_division=0)\n",
    "    f1_macro_tr = f1_score(y_train, y_pred_train, average='macro', zero_division=0)\n",
    "\n",
    "    # --- Metriche su Test Set ---\n",
    "    accuracy_te = accuracy_score(y_test, y_pred_test)\n",
    "    f1_weighted_te = f1_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
    "    f1_macro_te = f1_score(y_test, y_pred_test, average='macro', zero_division=0)\n",
    "\n",
    "    # Bootstrap CI per F1-weighted (sostituisce MSE/MAE/MAPE CI)\n",
    "    f1_weighted_low, f1_weighted_high = bootstrap_ci(\n",
    "        f1_score, y_test, y_pred_test, average='weighted', zero_division=0\n",
    "    )\n",
    "    f1_macro_low, f1_macro_high = bootstrap_ci(\n",
    "        f1_score, y_test, y_pred_test, average='macro', zero_division=0\n",
    "    )\n",
    "\n",
    "    # Matrice di Confusione e Classification Report\n",
    "    print(\"\\nClassification Report (Test Set):\")\n",
    "    print(classification_report(y_test, y_pred_test, zero_division=0))\n",
    "\n",
    "    print(\"\\nConfusion Matrix (Test Set):\")\n",
    "    print(confusion_matrix(y_test, y_pred_test))\n",
    "\n",
    "    # Raccolta dei punteggi\n",
    "    # Nota: R2, MSE, MAE, MAPE, Max Error sono stati rimossi.\n",
    "    # Li abbiamo sostituiti con metriche di classificazione.\n",
    "    scores_row = [\n",
    "        model_name, param_count,\n",
    "        accuracy_tr, accuracy_te,\n",
    "        f1_weighted_tr, f1_weighted_te, f1_weighted_low, f1_weighted_high,\n",
    "        f1_macro_tr, f1_macro_te, f1_macro_low, f1_macro_high\n",
    "    ]\n",
    "\n",
    "    # Calcolo AUC se possibile (necessita probabilità)\n",
    "    if y_proba_test is not None:\n",
    "        try:\n",
    "            num_classes = len(np.unique(y_test))\n",
    "            auc_roc_ovr = roc_auc_score(y_test, y_proba_test, multi_class='ovr', average='weighted')\n",
    "            auc_pr_ovr = average_precision_score(pd.get_dummies(y_test), y_proba_test, average='weighted')\n",
    "\n",
    "            # Bootstrap per AUC-ROC e AUC-PR\n",
    "            auc_roc_low, auc_roc_high = bootstrap_ci(\n",
    "                lambda yt, yp: roc_auc_score(yt, yp, multi_class='ovr', average='weighted'),\n",
    "                y_test, y_proba_test\n",
    "            )\n",
    "            auc_pr_low, auc_pr_high = bootstrap_ci(\n",
    "                lambda yt, yp: average_precision_score(pd.get_dummies(yt), yp, average='weighted'),\n",
    "                y_test, y_proba_test\n",
    "            )\n",
    "\n",
    "            scores_row.extend([auc_roc_ovr, auc_roc_low, auc_roc_high, auc_pr_ovr, auc_pr_low, auc_pr_high])\n",
    "            print(f\"AUC-ROC (OVR, Weighted): {auc_roc_ovr:.3f}\")\n",
    "            print(f\"AUC-PR (OVR, Weighted): {auc_pr_ovr:.3f}\")\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"Errore nel calcolo di AUC/PR: {e}. Probabilmente mancano classi o y_proba non è adatto.\")\n",
    "            scores_row.extend([np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]) # Aggiungi NaN per le colonne AUC\n",
    "    else:\n",
    "        print(\"Modello non supporta predict_proba. AUC/PR non calcolabili.\")\n",
    "        scores_row.extend([np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]) # Aggiungi NaN per le colonne AUC\n",
    "\n",
    "    all_scores_list.append(scores_row)\n",
    "\n",
    "\n",
    "def predict_torch(model, X_tensor, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(X_tensor.to(device))\n",
    "    return out # Restituisce logit/probabilità, non etichette hard qui\n",
    "\n",
    "\n",
    "def get_torch_estimator_scores(model_name, model,\n",
    "                               X_train_tensor, y_train_tensor,\n",
    "                               X_test_tensor, y_test_tensor,\n",
    "                               device, all_scores_list):\n",
    "    print(f\"\\n--- Valutazione Prestazioni per {model_name} (PyTorch) ---\")\n",
    "\n",
    "    # Calcola il numero di Parametri dei Modelli\n",
    "    param_count = count_params(model)\n",
    "    print(f\"Model Parameters: {param_count}\")\n",
    "\n",
    "    # Ottieni output (logits/probabilità) dal modello\n",
    "    y_pred_proba_train = predict_torch(model, X_train_tensor, device)\n",
    "    y_pred_proba_test = predict_torch(model, X_test_tensor, device)\n",
    "\n",
    "    # Converti le etichette vere (tensori) in numpy array per Scikit-learn\n",
    "    y_train_np = y_train_tensor.detach().cpu().numpy()\n",
    "    y_test_np = y_test_tensor.detach().cpu().numpy()\n",
    "\n",
    "    # Se il modello emette logit, converti in probabilità e poi in etichette\n",
    "    y_pred_labels_train = torch.argmax(softmax(y_pred_proba_train, dim=1), dim=1).detach().cpu().numpy()\n",
    "    y_pred_labels_test = torch.argmax(softmax(y_pred_proba_test, dim=1), dim=1).detach().cpu().numpy()\n",
    "\n",
    "    # --- Metriche su Training Set ---\n",
    "    accuracy_tr = accuracy_score(y_train_np, y_pred_labels_train)\n",
    "    f1_weighted_tr = f1_score(y_train_np, y_pred_labels_train, average='weighted', zero_division=0)\n",
    "    f1_macro_tr = f1_score(y_train_np, y_pred_labels_train, average='macro', zero_division=0)\n",
    "\n",
    "    # --- Metriche su Test Set ---\n",
    "    accuracy_te = accuracy_score(y_test_np, y_pred_labels_test)\n",
    "    f1_weighted_te = f1_score(y_test_np, y_pred_labels_test, average='weighted', zero_division=0)\n",
    "    f1_macro_te = f1_score(y_test_np, y_pred_labels_test, average='macro', zero_division=0)\n",
    "\n",
    "    # Bootstrap CI\n",
    "    f1_weighted_low, f1_weighted_high = bootstrap_ci(\n",
    "        f1_score, y_test_np, y_pred_labels_test, average='weighted', zero_division=0\n",
    "    )\n",
    "    f1_macro_low, f1_macro_high = bootstrap_ci(\n",
    "        f1_score, y_test_np, y_pred_labels_test, average='macro', zero_division=0\n",
    "    )\n",
    "\n",
    "    print(\"\\nClassification Report (Test Set):\")\n",
    "    print(classification_report(y_test_np, y_pred_labels_test, zero_division=0))\n",
    "\n",
    "    print(\"\\nConfusion Matrix (Test Set):\")\n",
    "    print(confusion_matrix(y_test_np, y_pred_labels_test))\n",
    "\n",
    "    scores_row = [\n",
    "        model_name, param_count,\n",
    "        accuracy_tr, accuracy_te,\n",
    "        f1_weighted_tr, f1_weighted_te, f1_weighted_low, f1_weighted_high,\n",
    "        f1_macro_tr, f1_macro_te, f1_macro_low, f1_macro_high\n",
    "    ]\n",
    "\n",
    "    # Calcolo AUC (necessita probabilità)\n",
    "    try:\n",
    "        auc_roc_ovr = roc_auc_score(y_test_np, softmax(y_pred_proba_test, dim=1).detach().cpu().numpy(),\n",
    "                                    multi_class='ovr', average='weighted')\n",
    "        auc_pr_ovr = average_precision_score(pd.get_dummies(y_test_np), softmax(y_pred_proba_test, dim=1).detach().cpu().numpy(),\n",
    "                                            average='weighted')\n",
    "\n",
    "        # Bootstrap per AUC-ROC e AUC-PR\n",
    "        auc_roc_low, auc_roc_high = bootstrap_ci(\n",
    "            lambda yt, yp: roc_auc_score(yt, softmax(torch.tensor(yp), dim=1).numpy(), multi_class='ovr', average='weighted'),\n",
    "            y_test_np, y_pred_proba_test.detach().cpu().numpy()\n",
    "        )\n",
    "        auc_pr_low, auc_pr_high = bootstrap_ci(\n",
    "            lambda yt, yp: average_precision_score(pd.get_dummies(yt), softmax(torch.tensor(yp), dim=1).numpy(), average='weighted'),\n",
    "            y_test_np, y_pred_proba_test.detach().cpu().numpy()\n",
    "        )\n",
    "\n",
    "        scores_row.extend([auc_roc_ovr, auc_roc_low, auc_roc_high, auc_pr_ovr, auc_pr_low, auc_pr_high])\n",
    "        print(f\"AUC-ROC (OVR, Weighted): {auc_roc_ovr:.3f}\")\n",
    "        print(f\"AUC-PR (OVR, Weighted): {auc_pr_ovr:.3f}\")\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Errore nel calcolo di AUC/PR: {e}. Probabilmente mancano classi o y_proba non è adatto.\")\n",
    "        scores_row.extend([np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]) # Aggiungi NaN per le colonne AUC\n",
    "    except Exception as e:\n",
    "        print(f\"Errore generico nel calcolo di AUC/PR per PyTorch: {e}\")\n",
    "        scores_row.extend([np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]) # Aggiungi NaN\n",
    "\n",
    "    all_scores_list.append(scores_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmQB-Bd0rMVH"
   },
   "source": [
    "## Cross Validation TimeSeriesSplit\n",
    "La validazione viene eseguita con `TimeSeriesSplit`, una tecnica di cross-validation adatta alle Serie Temporali, che preserva l’ordine cronologico dividendo il dataset in fold sequenziali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUTICD2IrPpu"
   },
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtC45BuOrZzt"
   },
   "source": [
    "Nel blocco seguente viene visualizzata la suddivisione del dataset nei 5 fold della Time Series Cross-Validation.  \n",
    "Questa rappresentazione è utile per verificare che la sequenza temporale sia rispettata nella divisione dei dati tra training e validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCdJycTCrcU3"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(tscv.n_splits, 1, figsize=(12, 12), sharex=True)\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "for index, (train_fold, validation_fold) in enumerate(tscv.split(y_train)): # Usa y_train qui\n",
    "    sns.lineplot(data=y_train.iloc[train_fold], label='Training Set', ax=axes[index])\n",
    "    sns.lineplot(data=y_train.iloc[validation_fold], label='Validation Set', ax=axes[index])\n",
    "    axes[index].set_title(f'Time Series Split #{index}')\n",
    "    axes[index].set(xlabel=None, ylabel=None)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sM7vW1UglVi8"
   },
   "source": [
    "## Random forest\n",
    "Il modello Random Forest è un ensemble di alberi decisionali che migliora la stabilità e la capacità predittiva rispetto a un singolo albero. Ogni albero viene addestrato su un sottoinsieme casuale del dataset (bagging) e valuta solo una parte delle feature, rendendo l’insieme più robusto a overfitting e variazioni nei dati.\n",
    "\n",
    "In questa configurazione iniziale, definiamo una pipeline che include anche una tecnica di **oversampling (SMOTE)** per bilanciare le classi nel dataset di addestramento.\n",
    "\n",
    "La pipeline è così configurata:\n",
    "- `max_samples=0.2`: ogni albero è addestrato su un campione casuale del 20% dei dati originali (con ripetizione);\n",
    "- `max_features='sqrt'`: ogni nodo dell’albero valuta solo un sottoinsieme di feature pari alla radice quadrata del numero totale di feature disponibili;\n",
    "- `n_estimators=200`: il modello è composto da 200 alberi decisionali;\n",
    "- `max_depth=None`: gli alberi possono crescere fino a foglie pure, senza una profondità massima prefissata;\n",
    "- `n_jobs=-1`: sfrutta tutti i core CPU disponibili per il training parallelo;\n",
    "- `random_state=RANDOM_STATE`: Per la riproducibilità.\n",
    "- `class_weight='balanced'`: attribuisce un peso inversamente proporzionale alla frequenza delle classi per gestire eventuali sbilanciamenti nel dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eieu-i1lVi9"
   },
   "outputs": [],
   "source": [
    "model = ImbPipeline([\n",
    "    (\"preproc\", preprocessor),\n",
    "    (\"sampler\", SMOTE(random_state=RANDOM_STATE)),\n",
    "    (\"tree\", RandomForestClassifier(max_samples=0.2, max_features=\"sqrt\",\n",
    "                                    n_estimators=200, max_depth=None,\n",
    "                                    n_jobs=N_JOBS, random_state=RANDOM_STATE,\n",
    "                                    class_weight='balanced'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lB_F1HTDlVi9"
   },
   "outputs": [],
   "source": [
    "%time model.fit(X_train, y_train)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jx9HoQcolVi9"
   },
   "source": [
    "Possiamo ricavare le 10 feature più importanti per la Random Forest, ovvero le variabili che sono state più utilizzate nella creazione degli alberi decisionali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQVy9ajGlVi9"
   },
   "outputs": [],
   "source": [
    "pd.Series(model.named_steps[\"tree\"].feature_importances_, preprocessor.get_feature_names_out(X_train.columns)).sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fY3JUaW7lVi9"
   },
   "source": [
    "### Hyperparameter Tuning per Random Forest\n",
    "\n",
    "Eseguiamo una ricerca randomizzata (`RandomizedSearchCV`) per ottimizzare un insieme ampio di iperparametri fondamentali del modello Random Forest:\n",
    "\n",
    "- `n_estimators`: numero di alberi nella foresta;\n",
    "- `max_samples`: frazione massima di campioni usata per addestrare ogni singolo albero;\n",
    "- `max_depth`: profondità massima degli alberi;\n",
    "- `min_samples_leaf`: numero minimo di campioni richiesti per una foglia;\n",
    "- `max_features`: numero massimo di feature da considerare per ogni split.\n",
    "\n",
    "In questa fase di tuning, la pipeline include anche la tecnica di oversampling **SMOTE**, applicata correttamente all'interno di ogni fold di **Time Series Cross-Validation** per evitare il data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_FPmQDJsIrd"
   },
   "outputs": [],
   "source": [
    "class_labels = np.unique(y_train)\n",
    "initial_weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=y_train)\n",
    "initial_class_weight_dict = dict(zip(class_labels, initial_weights))\n",
    "plt.bar(class_labels, initial_weights)\n",
    "plt.title('Pesi delle Classi')\n",
    "plt.xlabel('Classe')\n",
    "plt.ylabel('Peso')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S22MF1LolVi-"
   },
   "outputs": [],
   "source": [
    "grid = {\n",
    "    'tree__n_estimators': [150, 200, 250],\n",
    "    'tree__max_samples': [0.5, 0.8, 1.0],\n",
    "    'tree__max_depth': [5, 10, 20],\n",
    "    'tree__min_samples_leaf': [2, 5, 10],\n",
    "    'tree__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "model_ht = ImbPipeline([\n",
    "    (\"preproc\", preprocessor),\n",
    "    (\"sampler\", SMOTE(random_state=RANDOM_STATE)),\n",
    "    (\"tree\", RandomForestClassifier(random_state=RANDOM_STATE, class_weight='balanced'))\n",
    "])\n",
    "gs = RandomizedSearchCV(model_ht, grid, n_iter=36, cv=tscv, scoring='f1_weighted', n_jobs=N_JOBS, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBzY1Fk3Codg"
   },
   "source": [
    "**Scelta del numero di iterazioni per RandomizedSearchCV con questo grid**\n",
    "\n",
    "Il grid ha:\n",
    "\n",
    "- Configurazioni Totali:\n",
    "$$\n",
    "M = 162\n",
    "$$\n",
    "\n",
    "Supponiamo di voler avere una probabilità \\( P = 0.90 \\) di includere almeno una delle migliori \\( k = 10 \\) configurazioni tra queste 162.\n",
    "\n",
    "Usiamo la formula:\n",
    "\n",
    "$$\n",
    "n = \\frac{\\ln(1 - P)}{\\ln\\left(1 - \\frac{k}{M}\\right)}\n",
    "$$\n",
    "\n",
    "Calcoliamo:\n",
    "\n",
    "$$\n",
    "n = \\frac{\\ln(1 - 0.90)}{\\ln\\left(1 - \\frac{10}{162}\\right)} = \\frac{\\ln(0.10)}{\\ln\\left(\\frac{152}{162}\\right)} = \\frac{-2.3026}{\\ln(152/162)} \\approx \\frac{2.3026}{0.0637} \\approx 36.2\n",
    "$$\n",
    "\n",
    "Quindi, con **36 iterazioni** di Randomized Search, si ha circa il 90% di probabilità di testare almeno una delle 10 migliori configurazioni, risparmiando molto rispetto a un Grid Search completo con 162 combinazioni.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRF8N_GYCte1"
   },
   "source": [
    "**Da dove viene la formula per stimare il numero di iterazioni nel Randomized Search?**\n",
    "\n",
    "Per stimare quante iterazioni (`n`) sono necessarie per avere una certa probabilità \\(P\\) di includere almeno una configurazione tra le \\(k\\) migliori (su \\(M\\) totali), usiamo la seguente logica probabilistica:\n",
    "\n",
    "1. Probabilità di *non* pescare una top-\\(k\\) in un singolo tentativo.\n",
    "Se ci sono \\(M\\) configurazioni totali e \\(k\\) di esse sono “quasi ottimali”, la probabilità di *non* sceglierne una buona è:\n",
    "$$\n",
    "1 - \\frac{k}{M}\n",
    "$$\n",
    "\n",
    "2. Probabilità di non pescarne *nessuna* in \\(n\\) tentativi indipendenti\n",
    "$$\n",
    "\\left(1 - \\frac{k}{M} \\right)^n\n",
    "$$\n",
    "\n",
    "3. Probabilità di pescare **almeno una** delle top-\\(k\\)\n",
    "$$\n",
    "P(\\text{≥1 top-}k) = 1 - \\left(1 - \\frac{k}{M} \\right)^n\n",
    "$$\n",
    "\n",
    "4. Ricavare \\(n\\) dalla formula\n",
    "$$\n",
    "1 - \\left(1 - \\frac{k}{M} \\right)^n = P\n",
    "\\quad \\Longrightarrow \\quad\n",
    "n = \\frac{\\ln(1 - P)}{\\ln\\left(1 - \\frac{k}{M} \\right)}\n",
    "$$\n",
    "\n",
    "5. Approssimazione per $$ k \\ll M $$\n",
    "Poiché $$ \\ln(1 - x) \\approx -x $$ per \\(x\\) piccolo:\n",
    "$$\n",
    "n \\approx - \\frac{\\ln(1 - P)}{k/M}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqxUYeNElVi-"
   },
   "outputs": [],
   "source": [
    "%time gs.fit(X_train, y_train)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ML7A1bPflVi-"
   },
   "outputs": [],
   "source": [
    "get_estimator_scores(\"random_forest\", gs.best_estimator_, X_test, y_test, X_train, y_train, all_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5R2e4gWXXJp"
   },
   "source": [
    "Il grafico illustra l’andamento dello score medio di validazione in funzione del numero di alberi, suddiviso per profondità massima degli alberi. Questa analisi aiuta a bilanciare la complessità degli alberi e la quantità di alberi per ottenere le migliori performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "srMzWDr1lVi-"
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(gs.cv_results_)[[\n",
    "    \"param_tree__n_estimators\",\n",
    "    \"param_tree__max_samples\",\n",
    "    \"param_tree__max_depth\",\n",
    "    \"mean_test_score\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HO-ajTvZlVi-"
   },
   "outputs": [],
   "source": [
    "results[\"param_tree__n_estimators\"] = results[\"param_tree__n_estimators\"].astype(int)\n",
    "results[\"param_tree__max_depth\"] = results[\"param_tree__max_depth\"].astype(int)\n",
    "results[\"param_tree__max_samples\"] = results[\"param_tree__max_samples\"].astype(float)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "for depth in sorted(results[\"param_tree__max_depth\"].unique()):\n",
    "    for sample in sorted(results[\"param_tree__max_samples\"].unique()):\n",
    "        subset = results[\n",
    "            (results[\"param_tree__max_depth\"] == depth) &\n",
    "            (results[\"param_tree__max_samples\"] == sample)\n",
    "        ].sort_values(\"param_tree__n_estimators\")\n",
    "\n",
    "        label = f\"max_depth={depth}, max_samples={sample}\"\n",
    "        plt.plot(\n",
    "            subset[\"param_tree__n_estimators\"],\n",
    "            subset[\"mean_test_score\"],\n",
    "            label=label\n",
    "        )\n",
    "\n",
    "plt.title(\"Grafico al variare del numero di alberi\")\n",
    "plt.xlabel(\"Numero di Alberi (n_estimators)\")\n",
    "plt.ylabel(\"F1-weighted Score Medio\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xPxYN70lVi_"
   },
   "source": [
    "## XGBoost\n",
    "\n",
    "XGBoost è un'implementazione ottimizzata di algoritmi di gradient boosting. A differenza del Random Forest che costruisce alberi indipendenti e poi ne aggrega i risultati, XGBoost costruisce alberi in sequenza, con ogni nuovo albero che corregge gli errori degli alberi precedenti.\n",
    "\n",
    "In questa configurazione iniziale, definiamo una pipeline che include anche una tecnica di **oversampling (SMOTE)** per bilanciare le classi nel dataset di addestramento.\n",
    "\n",
    "La pipeline è così configurata:\n",
    "- `sampler`: **SMOTE (Synthetic Minority Over-sampling Technique)**, che genera nuovi campioni sintetici per le classi minoritarie, rendendo il dataset più bilanciato per l'addestramento. `random_state` garantisce la riproducibilità.\n",
    "- `xgb`: il classificatore XGBoost con i seguenti iperparametri iniziali:\n",
    "    - `objective='multi:softprob'`: Specifica la funzione obiettivo di classificazione multi-classe, dove l'output è un array di probabilità per ogni classe. Questo è necessario per le metriche AUC.\n",
    "    - `num_class`: numero totale di classi.\n",
    "    - `n_estimators=200`: numero di alberi di boosting da costruire.\n",
    "    - `learning_rate=0.1`: La dimensione del passo di ridimensionamento del contributo di ogni albero.\n",
    "    - `use_label_encoder=False`: parametro deprecato e quindi va disabilitato per evitare warning.\n",
    "    - `eval_metric='mlogloss'`: metrica di valutazione da usare durante l'addestramento.\n",
    "    - `n_jobs=N_JOBS`: Sfrutta tutti i core CPU disponibili per il training parallelo.\n",
    "    - `random_state=RANDOM_STATE`: Per la riproducibilità.\n",
    "\n",
    "**Nota Importante:** Per XGBoost con multi-classe, le etichette della classe devono essere convertite in indici che partono da 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1A4je0oplVjA"
   },
   "outputs": [],
   "source": [
    "y_train_xgb = y_train - 1\n",
    "y_test_xgb = y_test - 1\n",
    "num_classes = len(np.unique(y_train_xgb))\n",
    "\n",
    "model = ImbPipeline([\n",
    "    (\"preproc\", preprocessor),\n",
    "    (\"sampler\", SMOTE(random_state=RANDOM_STATE)),\n",
    "    (\"xgb\", xgb.XGBClassifier(objective='multi:softprob',\n",
    "                              num_class=num_classes,\n",
    "                              n_estimators=200,\n",
    "                              learning_rate=0.1,\n",
    "                              use_label_encoder=False, # Deprecato, imposto a False\n",
    "                              eval_metric='mlogloss',\n",
    "                              n_jobs=N_JOBS,\n",
    "                              random_state=RANDOM_STATE))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8A3332KolVjA"
   },
   "outputs": [],
   "source": [
    "%time model.fit(X_train, y_train_xgb)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUxDT_-nlVjA"
   },
   "source": [
    "Possiamo ricavare le 10 feature più importanti per l'XGBoost Classifier, ovvero le variabili che sono state più utilizzate nella creazione degli alberi di boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "us6Z4OmqlVjA"
   },
   "outputs": [],
   "source": [
    "pd.Series(model.named_steps[\"xgb\"].feature_importances_, preprocessor.get_feature_names_out(X_train.columns)).sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uapeT7xKlVjA"
   },
   "source": [
    "### Hyperparameter Tuning per XGBoost\n",
    "Eseguiamo una ricerca esaustiva (GridSearchCV) per ottimizzare iperparametri fondamentali del modello XGBoost, combinandoli con la tecnica di resampling SMOTE. Ottimizzeremo:\n",
    "- `n_estimators`: Numero di alberi di boosting.\n",
    "- `learning_rate`: La dimensione del passo che riduce il contributo di ogni nuovo albero.\n",
    "- `max_depth`: La profondità massima di un albero.\n",
    "\n",
    "In questa fase di tuning, la pipeline includerà anche la tecnica di oversampling SMOTE, che verrà applicata in modo appropriato a ogni fold di cross-validation per garantire che il modello sia addestrato su un dataset bilanciato senza data leakage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Scelta del numero di iterazioni per RandomizedSearchCV con questo grid**\n",
    "\n",
    "Il grid ha:\n",
    "\n",
    "- Configurazioni Totali:\n",
    "$$\n",
    "M = 27\n",
    "$$\n",
    "\n",
    "Supponiamo di voler avere una probabilità \\( P = 0.90 \\) di includere almeno una delle migliori \\( k = 10 \\) configurazioni tra queste 27.\n",
    "\n",
    "Usiamo la formula:\n",
    "\n",
    "$$\n",
    "n = \\frac{\\ln(1 - P)}{\\ln\\left(1 - \\frac{k}{M}\\right)}\n",
    "$$\n",
    "\n",
    "Calcoliamo:\n",
    "\n",
    "$$\n",
    "n = \\frac{\\ln(1 - 0.90)}{\\ln\\left(1 - \\frac{10}{27}\\right)} = \\frac{\\ln(0.10)}{\\ln\\left(\\frac{17}{27}\\right)} = \\frac{-2.3026}{\\ln(\\frac{17}{27})} \\approx \\frac{2.3026}{0.4626} \\approx 4.98\n",
    "$$\n",
    "\n",
    "Quindi, con **5 iterazioni** di Randomized Search, si ha circa il 90% di probabilità di testare almeno una delle 10 migliori configurazioni, risparmiando molto rispetto a un Grid Search completo con 27 combinazioni.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwAwlSAxlVjA"
   },
   "outputs": [],
   "source": [
    "grid = {\n",
    "    \"xgb__max_depth\": [3, 5, 7],\n",
    "    \"xgb__learning_rate\": [0.1, 0.2, 0.3],\n",
    "    \"xgb__n_estimators\": [100, 200, 300],\n",
    "}\n",
    "model_ht = ImbPipeline([\n",
    "    (\"preproc\", preprocessor),\n",
    "    (\"sampler\", SMOTE(random_state=RANDOM_STATE)),\n",
    "    (\"xgb\", xgb.XGBClassifier(objective='multi:softprob',\n",
    "                              num_class=num_classes,\n",
    "                              use_label_encoder=False, # Deprecato\n",
    "                              eval_metric='mlogloss',\n",
    "                              n_jobs=N_JOBS,\n",
    "                              random_state=RANDOM_STATE))\n",
    "])\n",
    "gs = RandomizedSearchCV(model_ht, grid, n_iter=5, cv=tscv, scoring='f1_weighted', n_jobs=N_JOBS, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRafW1bilVjA"
   },
   "outputs": [],
   "source": [
    "%time gs.fit(X_train, y_train_xgb)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECO1JyGKlVjB"
   },
   "outputs": [],
   "source": [
    "get_estimator_scores(\"xgboost\", gs.best_estimator_, X_test, y_test_xgb, X_train, y_train_xgb, all_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9ynR1lhYC5i"
   },
   "source": [
    "Il grafico illustra l’andamento dello score medio di validazione in funzione del numero di alberi, suddiviso per learning rate e profondità massima, tenendo conto dell'effetto del resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8uK3_8j4lVjB"
   },
   "outputs": [],
   "source": [
    "results_xgb = pd.DataFrame(gs.cv_results_)[[\n",
    "    \"param_xgb__n_estimators\",\n",
    "    \"param_xgb__learning_rate\",\n",
    "    \"param_xgb__max_depth\",\n",
    "    \"mean_test_score\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DD3ex6kOlVjB"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "for depth in sorted(results_xgb[\"param_xgb__max_depth\"].unique()):\n",
    "    for lr in sorted(results_xgb[\"param_xgb__learning_rate\"].unique()):\n",
    "        subset = results_xgb[(results_xgb[\"param_xgb__max_depth\"] == depth) &\n",
    "                             (results_xgb[\"param_xgb__learning_rate\"] == lr)]\n",
    "        subset = subset.sort_values(\"param_xgb__n_estimators\")\n",
    "        plt.plot(\n",
    "            subset[\"param_xgb__n_estimators\"],\n",
    "            subset[\"mean_test_score\"],\n",
    "            label=f\"max_depth={depth}, learning_rate={lr}\"\n",
    "        )\n",
    "\n",
    "plt.xlabel('Numero di Alberi (n_estimators)')\n",
    "plt.ylabel(f'F1-weighted Score Medio di Validazione')\n",
    "plt.title(f\"Grafico al variare di Numero di Alberi del XGBoost\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlwH7ptOzQwq"
   },
   "source": [
    "## MLP and KAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKVjVeNVzTPO"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device utilizzato: {device}\")\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_sizes, dropout, num_classes):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        dim = input_dim\n",
    "        for hs in hidden_sizes:\n",
    "            layers.append(nn.Linear(dim, hs))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            dim = hs\n",
    "        layers.append(nn.Linear(dim, num_classes))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def build_kan(input_dim, width, grid, k, num_classes, seed=0):\n",
    "    model = KAN(\n",
    "        width=[input_dim] + list(width) + [num_classes],\n",
    "        grid=grid,\n",
    "        k=k,\n",
    "        seed=seed,\n",
    "        device=device\n",
    "    )\n",
    "    model.speed()  # enable efficient mode: disable symbolic branch\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1Ljjq9rZJZo"
   },
   "source": [
    "### Implementazione dell’Early Stopping\n",
    "\n",
    "La classe `EarlyStopper` implementa una logica di early stopping che interrompe anticipatamente l’addestramento se la performance sul validation set non migliora oltre una soglia (min_delta) per un numero consecutivo di epoche (patience).\n",
    "Questo approccio aiuta a evitare l’overfitting e riduce i tempi di addestramento, salvando il modello con la miglior loss di validazione osservata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8sQny3mZPSf"
   },
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=3, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, val_loss):\n",
    "        # Se la loss migliora (di almeno min_delta), resettiamo il counter\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            # Se la loss non migliora da 'patience' epoche, dobbiamo fermarci\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Etc1WebYZlib"
   },
   "source": [
    "### Funzioni per il training e la valutazione\n",
    "\n",
    "- `train_epoch`: esegue una singola epoca di training. Calcola le previsioni, applica la loss function, esegue il backpropagation e aggiorna i pesi del modello. La loss viene aggregata e normalizzata sulla dimensione del dataset.\n",
    "\n",
    "- `eval_loss`: calcola la loss media del modello sul validation set in modalità eval, disabilitando l’aggiornamento dei pesi. Questo è fondamentale per valutare le prestazioni in modo affidabile durante il training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljnxu4EwZrSG"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, l2_lambda=0.0):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for Xb, yb in loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(Xb), yb)\n",
    "\n",
    "        if l2_lambda > 0:\n",
    "            l2_reg = torch.tensor(0.).to(device)\n",
    "            for param in model.parameters():\n",
    "                l2_reg += torch.norm(param, 2)\n",
    "            loss += l2_lambda * l2_reg\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * Xb.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def eval_loss(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            total_loss += criterion(model(Xb), yb.long()).item() * Xb.size(0)\n",
    "    return total_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcnVXcxkzVxm"
   },
   "source": [
    "### Random Search con Cross-Validation temporale e Early Stopping\n",
    "\n",
    "La funzione `random_search` implementa una strategia di ottimizzazione degli iperparametri che:\n",
    "\n",
    "- Estrae in modo casuale combinazioni di iperparametri dallo spazio definito (param_dist);\n",
    "- Valuta ogni configurazione tramite Cross Validation TimeSeriesSplit per rispettare l’ordine temporale dei dati;\n",
    "- Addestra un modello MLP per ciascun fold monitorando la loss di validazione;\n",
    "- Applica early stopping durante l’allenamento per evitare overfitting;\n",
    "- Calcola la media delle validation loss su tutti i fold per ogni configurazione.\n",
    "\n",
    "La combinazione con la miglior media viene salvata come modello ottimale, insieme agli iperparametri migliori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iumk8NyQzZco"
   },
   "outputs": [],
   "source": [
    "def random_search(model_builder, param_dist, dataset,\n",
    "                  n_iter=10, cv_folds=5,\n",
    "                  early_patience=5,\n",
    "                  early_min_delta=1e-4,\n",
    "                  class_weights=None):\n",
    "    train_keys = ['lr', 'l2_lambda']\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_params, best_train_params = None, None\n",
    "    best_model = None\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=cv_folds)\n",
    "\n",
    "    print(\"Avvio Random Search con Time Series Cross Validation...\")\n",
    "\n",
    "    for param_id, params in enumerate(ParameterSampler(param_dist, n_iter=n_iter, random_state=RANDOM_STATE)):\n",
    "        print(f\"Testing parameter set {param_id+1}/{n_iter}\")\n",
    "\n",
    "        model_params = {k: v for k, v in params.items() if k not in train_keys}\n",
    "        train_params = {k: v for k, v in params.items() if k in train_keys}\n",
    "        val_losses = []\n",
    "\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(tscv.split(range(len(dataset)))):\n",
    "            print(f\"  Fold {fold_idx+1}/{cv_folds}\")\n",
    "\n",
    "            sub_train_dataset = Subset(dataset, train_idx)\n",
    "            val_set = Subset(dataset, val_idx)\n",
    "\n",
    "            # Calcola i pesi dei campioni per il training set del fold corrente\n",
    "            # Basato sulle etichette originali (0-indexed) del sotto-dataset di training\n",
    "            train_labels_fold = np.array([dataset.tensors[1][i].item() for i in train_idx])\n",
    "            unique_labels, counts = np.unique(train_labels_fold, return_counts=True)\n",
    "            # Pesi inversamente proporzionali alla frequenza\n",
    "            # Gestisce il caso in cui una classe possa mancare in un fold\n",
    "            sample_weights = np.zeros(len(train_labels_fold))\n",
    "            for i, label in enumerate(unique_labels):\n",
    "                if class_weights is not None and label in class_weights:\n",
    "                    # Usa il peso globale calcolato prima\n",
    "                    weight_for_label = class_weights[label]\n",
    "                else:\n",
    "                    # Fallback a un peso bilanciato locale se non specificato globalmente o classe mancante\n",
    "                    weight_for_label = 1. / counts[np.where(unique_labels == label)[0][0]]\n",
    "                sample_weights[train_labels_fold == label] = weight_for_label\n",
    "\n",
    "            sampler = WeightedRandomSampler(\n",
    "                weights=sample_weights,\n",
    "                num_samples=len(sample_weights), # Num samples è len(sub_train_dataset) per oversampling proporzionale\n",
    "                replacement=True # Necessario per oversampling\n",
    "            )\n",
    "            # DataLoader con il sampler ponderato per il training\n",
    "            train_loader = DataLoader(sub_train_dataset, batch_size=32, sampler=sampler)\n",
    "            # Val loader rimane normale\n",
    "            val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n",
    "\n",
    "            model = model_builder(**model_params)\n",
    "            if hasattr(model, 'speed'):\n",
    "                model.speed()\n",
    "            model.to(device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=train_params['lr'])\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            stopper = EarlyStopper(patience=early_patience, min_delta=early_min_delta)\n",
    "\n",
    "            for epoch in range(1000):\n",
    "                train_loss = train_epoch(model, train_loader, optimizer, criterion, l2_lambda=train_params.get('l2_lambda', 0.0))\n",
    "                val_loss = eval_loss(model, val_loader, criterion)\n",
    "\n",
    "                if epoch % 10 == 0:\n",
    "                    print(f\"    Epoch {epoch}: train_loss = {train_loss:.6f}, val_loss = {val_loss:.6f}\")\n",
    "\n",
    "                if stopper.early_stop(val_loss):\n",
    "                    print(f\"    Early stopping at epoch {epoch}, best_val_loss: {stopper.best_loss:.6f}\")\n",
    "                    break\n",
    "\n",
    "            final_val_loss = eval_loss(model, val_loader, criterion)\n",
    "            val_losses.append(final_val_loss)\n",
    "\n",
    "        mean_val = np.mean(val_losses)\n",
    "        print(f\"  Mean validation loss: {mean_val:.6f}\")\n",
    "\n",
    "        if mean_val < best_val_loss:\n",
    "            best_val_loss = mean_val\n",
    "            best_model_params = model_params\n",
    "            best_train_params = train_params\n",
    "            best_model = model_builder(**best_model_params).to(device)\n",
    "            best_model.load_state_dict(model.state_dict())\n",
    "            print(f\"  New best validation loss: {best_val_loss:.6f}\")\n",
    "\n",
    "    print(f\"\\nBest validation loss: {best_val_loss:.6f}\")\n",
    "    return best_model, best_model_params, best_train_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVD5006Oza85"
   },
   "source": [
    "### Preparazione dati e spazio di ricerca iperparametri\n",
    "\n",
    "In questa sezione:\n",
    "\n",
    "- Applichiamo la trasformazione dei dati tramite il preprocessor già definito in precedenza;\n",
    "\n",
    "- Convertiamo le etichette y_train e y_test in formato 0-indexed, come richiesto dalla CrossEntropyLoss di PyTorch;\n",
    "\n",
    "- Creiamo un dataset PyTorch (TensorDataset) con input e target;\n",
    "\n",
    "- Definiamo lo spazio di ricerca per gli iperparametri, tra cui:\n",
    "\n",
    " Architettura della rete (hidden_sizes);\n",
    "\n",
    " Dropout;\n",
    "\n",
    " Tasso di apprendimento (lr);\n",
    "\n",
    " Numero di classi.\n",
    "\n",
    "Calcoliamo infine i pesi delle classi per bilanciare la loss in presenza di squilibri nella distribuzione delle etichette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHuw7xi2zemy"
   },
   "outputs": [],
   "source": [
    "X_train_processed = preprocessor.fit_transform(X_train) # Fit and transform on training data\n",
    "X_test_processed = preprocessor.transform(X_test) # Only transform on test data\n",
    "\n",
    "y_train_0_indexed = y_train.values - 1\n",
    "y_test_0_indexed = y_test.values - 1\n",
    "\n",
    "X_tensor = torch.tensor(X_train_processed, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_train_0_indexed, dtype=torch.long)\n",
    "\n",
    "full_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "input_dim = X_tensor.shape[1]\n",
    "num_classes = len(np.unique(y_train_0_indexed))\n",
    "\n",
    "mlp_param_dist = {\n",
    "    'input_dim': [input_dim],\n",
    "    'hidden_sizes': [(32,32), (64,64), (128,)],\n",
    "    'dropout': [0.0, 0.2, 0.5],\n",
    "    'lr': [1e-3, 1e-4],\n",
    "    'num_classes': [num_classes],\n",
    "    'l2_lambda': [0.0, 1e-5, 1e-4, 1e-3]\n",
    "}\n",
    "kan_param_dist = {\n",
    "    'input_dim': [input_dim],\n",
    "    'width': [(8,4), (16,8)],\n",
    "    'grid': [5, 10],\n",
    "    'k': [2, 4],\n",
    "    'seed': [0],\n",
    "    'lr': [1e-3, 1e-4],\n",
    "    'num_classes': [num_classes],\n",
    "    'l2_lambda': [0.0, 1e-5, 1e-4, 1e-3]\n",
    "}\n",
    "\n",
    "class_labels_0_indexed = np.unique(y_train_0_indexed)\n",
    "class_weights_balanced = compute_class_weight(class_weight='balanced', classes=class_labels_0_indexed, y=y_train_0_indexed)\n",
    "class_weights_dict = dict(zip(class_labels_0_indexed, class_weights_balanced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exVLSjhpZ88b"
   },
   "source": [
    "### Avvio della ricerca e valutazione\n",
    "\n",
    "- Eseguiamo la funzione random_search, fornendo:\n",
    "\n",
    " Il costruttore della rete MLP;\n",
    "\n",
    " Lo spazio degli iperparametri;\n",
    "\n",
    " Il dataset PyTorch e i pesi delle classi calcolati in precedenza;\n",
    "\n",
    "- Al termine, il modello con la miglior media di validation loss viene selezionato e restituito;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WFrI6-zOZ_1h"
   },
   "outputs": [],
   "source": [
    "print(\"MLP Results:\")\n",
    "best_model, model_params_mlp, train_params_mlp = random_search(\n",
    "    lambda **p: MLP(**p), mlp_param_dist, full_dataset,\n",
    "    class_weights=class_weights_dict, n_iter=15\n",
    ")\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_processed, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test_0_indexed, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Scelta del numero di iterazioni per RandomizedSearchCV con MLP grid**\n",
    "\n",
    "Il grid ha:\n",
    "\n",
    "- Configurazioni Totali:\n",
    "$$\n",
    "M = 72\n",
    "$$\n",
    "\n",
    "Supponiamo di voler avere una probabilità \\( P = 0.90 \\) di includere almeno una delle migliori \\( k = 10 \\) configurazioni tra queste 72.\n",
    "\n",
    "Usiamo la formula:\n",
    "\n",
    "$$\n",
    "n = \\frac{\\ln(1 - P)}{\\ln\\left(1 - \\frac{k}{M}\\right)}\n",
    "$$\n",
    "\n",
    "Calcoliamo:\n",
    "\n",
    "$$\n",
    "n = \\frac{\\ln(1 - 0.90)}{\\ln\\left(1 - \\frac{10}{72}\\right)} = \\frac{\\ln(0.10)}{\\ln\\left(\\frac{62}{72}\\right)} = \\frac{-2.3026}{\\ln(\\frac{62}{72})} \\approx \\frac{2.3026}{0.1495} \\approx 15.4\n",
    "$$\n",
    "\n",
    "Quindi, con **15 iterazioni** di Randomized Search, si ha circa il 90% di probabilità di testare almeno una delle 10 migliori configurazioni, risparmiando molto rispetto a un Grid Search completo con 72 combinazioni.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LROMl7eRPuJA"
   },
   "outputs": [],
   "source": [
    "get_torch_estimator_scores(\"MLP\", best_model,\n",
    "                           X_tensor.to(device), y_tensor.to(device),\n",
    "                           X_test_tensor, y_test_tensor,\n",
    "                           device, all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tvUB_bWyqvyH"
   },
   "outputs": [],
   "source": [
    "print(\"KAN Results:\")\n",
    "best_model_kan, model_params_kan, train_params_kan = random_search(\n",
    "    lambda **p: build_kan(**p), kan_param_dist, full_dataset,\n",
    "    class_weights=class_weights_dict, n_iter=14\n",
    ")\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_processed, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test_0_indexed, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Scelta del numero di iterazioni per RandomizedSearchCV con KAN grid**\n",
    "\n",
    "Il grid ha:\n",
    "\n",
    "- Configurazioni Totali:\n",
    "$$\n",
    "M = 64\n",
    "$$\n",
    "\n",
    "Supponiamo di voler avere una probabilità \\( P = 0.90 \\) di includere almeno una delle migliori \\( k = 10 \\) configurazioni tra queste 64.\n",
    "\n",
    "Usiamo la formula:\n",
    "\n",
    "$$\n",
    "n = \\frac{\\ln(1 - P)}{\\ln\\left(1 - \\frac{k}{M}\\right)}\n",
    "$$\n",
    "\n",
    "Calcoliamo:\n",
    "\n",
    "$$\n",
    "n = \\frac{\\ln(1 - 0.90)}{\\ln\\left(1 - \\frac{10}{64}\\right)} = \\frac{\\ln(0.10)}{\\ln\\left(\\frac{54}{64}\\right)} = \\frac{-2.3026}{\\ln(\\frac{54}{64})} \\approx \\frac{2.3026}{0.1699} \\approx 13.55\n",
    "$$\n",
    "\n",
    "Quindi, con **14 iterazioni** di Randomized Search, si ha circa il 90% di probabilità di testare almeno una delle 10 migliori configurazioni, risparmiando molto rispetto a un Grid Search completo con 64 combinazioni.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "omigerVfqxl2"
   },
   "outputs": [],
   "source": [
    "get_torch_estimator_scores(\"KAN\", best_model_kan,\n",
    "                           X_tensor.to(device), y_tensor.to(device),\n",
    "                           X_test_tensor, y_test_tensor,\n",
    "                           device, all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F1iwpUfztgPM"
   },
   "outputs": [],
   "source": [
    "best_model_kan.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OA3j9OaWthoL"
   },
   "outputs": [],
   "source": [
    "best_model_kan.prune()\n",
    "best_model_kan.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uIugxCaMC9R"
   },
   "source": [
    "# Confronto Visivo delle Prestazioni dei Modelli\n",
    "\n",
    "La funzione `plot_estimator_scores` consente di visualizzare in modo sintetico ed intuitivo le metriche di valutazione di tutti i modelli allenati.\n",
    "\n",
    "Questa visualizzazione finale è utile per trarre conclusioni sulla bontà predittiva di ciascun modello e guidare la scelta del miglior approccio da adottare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5vxh_8pW9qkC"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def plot_estimator_scores(scores):\n",
    "    # Prepariamo i dati per il plot di Accuracy (Train vs Test)\n",
    "    melted_accuracy = (\n",
    "        scores[['Model', 'Accuracy_Train', 'Accuracy_Test']]\n",
    "        .rename(columns={'Accuracy_Train': 'Train', 'Accuracy_Test': 'Test'})\n",
    "        .melt(id_vars='Model', var_name='Set', value_name='Score')\n",
    "    )\n",
    "\n",
    "    model_order = scores['Model'].tolist()\n",
    "\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.tight_layout(pad=4.0)\n",
    "\n",
    "    # --- Plot 1: Accuracy (Train vs Test) ---\n",
    "    axs[0, 0].set_title('Accuracy')\n",
    "    sns.barplot(data=melted_accuracy, x='Score', y='Model', hue='Set', ax=axs[0, 0], order=model_order)\n",
    "    axs[0, 0].set_xlabel('Accuracy Score')\n",
    "    axs[0, 0].legend(loc='lower right', title='Set')\n",
    "    axs[0, 0].set_xlim(0, 1)\n",
    "\n",
    "    # --- Funzione helper per disegnare barh con CI in modo consistente ---\n",
    "    def plot_barh_with_ci(ax, data, metric_col, ci_low_col, ci_high_col, title, model_order):\n",
    "        ax.set_title(title)\n",
    "        # Assicurati che i dati siano ordinati per il plot coerente\n",
    "        data_ordered = data.set_index('Model').loc[model_order].reset_index()\n",
    "\n",
    "        for i, row in data_ordered.iterrows():\n",
    "            # Gestisci i valori NaN per i CI\n",
    "            val = row[metric_col]\n",
    "            if pd.isna(val):\n",
    "                continue # Salta il modello se il valore della metrica è NaN\n",
    "\n",
    "            err_low = [val - row[ci_low_col]] if not pd.isna(row[ci_low_col]) else [0]\n",
    "            err_high = [row[ci_high_col] - val] if not pd.isna(row[ci_high_col]) else [0]\n",
    "\n",
    "            ax.barh(\n",
    "                row['Model'], val,\n",
    "                xerr=[err_low, err_high],\n",
    "                capsize=5,\n",
    "                color=sns.color_palette(\"viridis\")[i % len(sns.color_palette(\"viridis\"))] # Colore coerente\n",
    "            )\n",
    "        ax.set_xlabel(title.split(' ')[0]) # Estrae il nome della metrica dal titolo\n",
    "        ax.invert_yaxis() # Per avere lo stesso ordine dei modelli sull'asse y\n",
    "        ax.set_xlim(0, 1) # Imposta i limiti per le metriche [0, 1]\n",
    "\n",
    "    # --- Plot 2: F1-Weighted Test Score ± CI95% ---\n",
    "    plot_barh_with_ci(axs[0, 1], scores, 'F1_Weighted_Test', 'F1_Weighted_CI_Low', 'F1_Weighted_CI_High',\n",
    "                      'F1-Weighted Test ± CI95%', model_order)\n",
    "\n",
    "    # --- Plot 3: F1-Macro Test Score ± CI95% ---\n",
    "    plot_barh_with_ci(axs[0, 2], scores, 'F1_Macro_Test', 'F1_Macro_CI_Low', 'F1_Macro_CI_High',\n",
    "                      'F1-Macro Test ± CI95%', model_order)\n",
    "\n",
    "    # --- Plot 4: AUC-ROC OVR Weighted Test Score ± CI95% ---\n",
    "    plot_barh_with_ci(axs[1, 0], scores, 'AUC_ROC_OVR_Weighted', 'AUC_ROC_CI_Low', 'AUC_ROC_CI_High',\n",
    "                      'AUC-ROC (OVR Weighted) Test ± CI95%', model_order)\n",
    "\n",
    "    # --- Plot 5: AUC-PR OVR Weighted Test Score ± CI95% ---\n",
    "    plot_barh_with_ci(axs[1, 1], scores, 'AUC_PR_OVR_Weighted', 'AUC_PR_CI_Low', 'AUC_PR_CI_High',\n",
    "                      'AUC-PR (OVR Weighted) Test ± CI95%', model_order)\n",
    "\n",
    "    # --- Plot 6: Model Complexity (Parameter/Node Count) ---\n",
    "    axs[1, 2].set_title('Model Complexity (Parameters/Nodes)', fontsize=12, fontweight='bold')\n",
    "\n",
    "    # Order data by model order for consistency\n",
    "    scores_ordered = scores.set_index('Model').loc[model_order].reset_index()\n",
    "\n",
    "    # Create color palette\n",
    "    colors = sns.color_palette(\"plasma\", len(scores_ordered))\n",
    "\n",
    "    bars = axs[1, 2].barh(scores_ordered['Model'], scores_ordered['Param_Count'], color=colors)\n",
    "    axs[1, 2].set_xlabel('Parameter/Node Count')\n",
    "    axs[1, 2].invert_yaxis()\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for i, (bar, count) in enumerate(zip(bars, scores_ordered['Param_Count'])):\n",
    "        if not pd.isna(count) and count > 0:\n",
    "            axs[1, 2].text(bar.get_width() + max(scores_ordered['Param_Count']) * 0.01,\n",
    "                          bar.get_y() + bar.get_height()/2,\n",
    "                          f'{int(count):,}',\n",
    "                          va='center', fontsize=9)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTHU3LQ49qkD"
   },
   "outputs": [],
   "source": [
    "estimator_scores_df = pd.DataFrame(\n",
    "    all_scores,\n",
    "    columns = [\n",
    "        'Model', 'Param_Count'\n",
    "        'Accuracy_Train', 'Accuracy_Test',\n",
    "        'F1_Weighted_Train', 'F1_Weighted_Test', 'F1_Weighted_CI_Low', 'F1_Weighted_CI_High',\n",
    "        'F1_Macro_Train', 'F1_Macro_Test', 'F1_Macro_CI_Low', 'F1_Macro_CI_High',\n",
    "        'AUC_ROC_OVR_Weighted', 'AUC_ROC_CI_Low', 'AUC_ROC_CI_High',\n",
    "        'AUC_PR_OVR_Weighted', 'AUC_PR_CI_Low', 'AUC_PR_CI_High'\n",
    "    ]\n",
    ")\n",
    "plot_estimator_scores(estimator_scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvcGaDcg8Q8R"
   },
   "source": [
    "# Selezione del Miglior Modello\n",
    "Calcoliamo un punteggio complessivo per ciascun modello basandoci sulle metriche di valutazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0DEhNMS8M8S"
   },
   "outputs": [],
   "source": [
    "# 1) Define metrics and their optimization direction\n",
    "metrics = {\n",
    "    'Accuracy_Test': 'max',\n",
    "    'F1_Weighted_Test': 'max',\n",
    "    'F1_Macro_Test': 'max',\n",
    "    'AUC_ROC_OVR_Weighted': 'max',\n",
    "    'AUC_PR_OVR_Weighted': 'max'\n",
    "}\n",
    "\n",
    "# 2) Build ranking DataFrame\n",
    "df_ranks = estimator_scores_df.set_index('Model')\n",
    "ranks = pd.DataFrame(index=df_ranks.index)\n",
    "\n",
    "# Calculate ranks for performance metrics\n",
    "for metric, direction in metrics.items():\n",
    "    if direction == 'max':\n",
    "        # Per metriche 'max' (più alto è meglio), rank in ordine decrescente (rank 1 al migliore)\n",
    "        ranks[f\"{metric}_rank\"] = df_ranks[metric].rank(ascending=False, method='average')\n",
    "    elif direction == 'min':\n",
    "        # Per metriche 'min' (più basso è meglio), rank in ordine crescente (rank 1 al migliore)\n",
    "        ranks[f\"{metric}_rank\"] = df_ranks[metric].rank(ascending=True, method='average')\n",
    "\n",
    "# Calculate complexity rank (lower parameter count is better)\n",
    "ranks['Complexity_rank'] = df_ranks['Param_Count'].rank(ascending=True, method='average')\n",
    "\n",
    "# 3) Calculate weighted scores\n",
    "# Performance score (average of performance ranks)\n",
    "performance_cols = [col for col in ranks.columns if col.endswith('_rank') and col != 'Complexity_rank']\n",
    "ranks['performance_score'] = ranks[performance_cols].mean(axis=1)\n",
    "\n",
    "# Method 1: Equal weighting\n",
    "ranks['equal_weight_score'] = ranks['performance_score'] + ranks['Complexity_rank']\n",
    "\n",
    "# Method 2: Complexity heavily weighted (complexity counts 2x)\n",
    "ranks['complexity_weighted_score'] = ranks['performance_score'] + (2 * ranks['Complexity_rank'])\n",
    "\n",
    "# Method 3: Extreme complexity weighting (complexity counts 3x)\n",
    "ranks['extreme_complexity_score'] = ranks['performance_score'] + (3 * ranks['Complexity_rank'])\n",
    "\n",
    "# Method 4: Pareto efficiency approach (performance vs complexity)\n",
    "# Normalize scores to [0,1] for fair comparison\n",
    "performance_norm = (ranks['performance_score'] - ranks['performance_score'].min()) / (ranks['performance_score'].max() - ranks['performance_score'].min())\n",
    "complexity_norm = (ranks['Complexity_rank'] - ranks['Complexity_rank'].min()) / (ranks['Complexity_rank'].max() - ranks['Complexity_rank'].min())\n",
    "ranks['pareto_score'] = 0.4 * performance_norm + 0.6 * complexity_norm  # 60% weight on complexity\n",
    "\n",
    "# Display results for each method\n",
    "methods = {\n",
    "    'Equal Weight (1:1)': 'equal_weight_score',\n",
    "    'Complexity Weighted (1:2)': 'complexity_weighted_score',\n",
    "    'Extreme Complexity (1:3)': 'extreme_complexity_score',\n",
    "    'Pareto Approach (40:60)': 'pareto_score'\n",
    "}\n",
    "\n",
    "results_summary = pd.DataFrame(index=df_ranks.index)\n",
    "results_summary['Performance_Score'] = ranks['performance_score']\n",
    "results_summary['Complexity_Rank'] = ranks['Complexity_rank']\n",
    "results_summary['Param_Count'] = df_ranks['Param_Count']\n",
    "\n",
    "for method_name, score_col in methods.items():\n",
    "    best_model = ranks[score_col].idxmin() if 'pareto' not in score_col else ranks[score_col].idxmin()\n",
    "    best_score = ranks.loc[best_model, score_col]\n",
    "    results_summary[method_name] = ranks[score_col]\n",
    "    print(f\"{method_name:25} -> {best_model:15} (score: {best_score:.3f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DETAILED RANKING TABLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create comprehensive ranking table\n",
    "ranking_display = pd.DataFrame(index=df_ranks.index)\n",
    "ranking_display['Param_Count'] = df_ranks['Param_Count'].astype(int)\n",
    "ranking_display['Avg_Performance'] = ranks['performance_score'].round(2)\n",
    "ranking_display['Complexity_Rank'] = ranks['Complexity_rank'].astype(int)\n",
    "\n",
    "for method_name, score_col in methods.items():\n",
    "    ranking_display[f'{method_name.split()[0]}_Rank'] = ranks[score_col].rank().astype(int)\n",
    "\n",
    "# Sort by complexity-weighted score (our recommended approach)\n",
    "ranking_display_sorted = ranking_display.sort_values('Complexity_Rank')\n",
    "display(ranking_display_sorted)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Our recommended model (complexity weighted approach)\n",
    "recommended_model = ranks['complexity_weighted_score'].idxmin()\n",
    "recommended_score = ranks.loc[recommended_model, 'complexity_weighted_score']\n",
    "recommended_params = df_ranks.loc[recommended_model, 'Param_Count']\n",
    "recommended_f1 = df_ranks.loc[recommended_model, 'F1_Weighted_Test']\n",
    "\n",
    "print(f\"   RECOMMENDED MODEL: {recommended_model}\")\n",
    "print(f\"   Reason: Best balance between performance and complexity\")\n",
    "print(f\"   Parameters: {int(recommended_params):,}\")\n",
    "print(f\"   F1-Weighted Score: {recommended_f1:.4f}\")\n",
    "print(f\"   Complexity-Weighted Rank Score: {recommended_score:.3f}\")\n",
    "\n",
    "# Show top 3 models for comparison\n",
    "print(f\"\\n  TOP 3 MODELS (Complexity-Weighted Ranking):\")\n",
    "top_3 = ranks.sort_values('complexity_weighted_score').head(3)\n",
    "for i, (model, row) in enumerate(top_3.iterrows(), 1):\n",
    "    params = int(df_ranks.loc[model, 'Param_Count'])\n",
    "    f1_score = df_ranks.loc[model, 'F1_Weighted_Test']\n",
    "    print(f\"   {i}. {model:15} | Params: {params:>8,} | F1: {f1_score:.4f} | Score: {row['complexity_weighted_score']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "mvVxxvpq2pGF",
    "vnXzcSTc2Z4u",
    "pcIEc735lViq",
    "GveBLWbLlViq",
    "H_bIhHTRKAzQ",
    "ZmQB-Bd0rMVH",
    "sM7vW1UglVi8",
    "7xPxYN70lVi_",
    "xlwH7ptOzQwq",
    "W1Ljjq9rZJZo",
    "Etc1WebYZlib",
    "CcnVXcxkzVxm",
    "OVD5006Oza85",
    "5uIugxCaMC9R",
    "EvcGaDcg8Q8R"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
