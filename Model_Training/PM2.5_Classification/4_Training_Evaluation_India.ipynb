{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvVxxvpq2pGF"
      },
      "source": [
        "# Previsione della Qualità dell'Aria - **Allenamento e Valutazione dei Modelli India**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnXzcSTc2Z4u"
      },
      "source": [
        "## Caricamento dei Datasets ed Import Librerie\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW2V7IPwnXAU"
      },
      "outputs": [],
      "source": [
        "!pip install pykan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1UsULdR4Vs7"
      },
      "outputs": [],
      "source": [
        "%env CUDA_LAUNCH_BLOCKING=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ts2wiQjcg4BV"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import inspect\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "import seaborn as sns\n",
        "import types\n",
        "sns.set_theme()\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    running_in_colab = True\n",
        "except ImportError:\n",
        "    running_in_colab = False\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    roc_auc_score,\n",
        "    average_precision_score\n",
        ")\n",
        "from sklearn.model_selection import (\n",
        "    TimeSeriesSplit,\n",
        "    RandomizedSearchCV,\n",
        "    ParameterSampler\n",
        ")\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
        "import torch.nn.utils.prune as prune\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "%pip install xgboost\n",
        "import xgboost as xgb\n",
        "from kan import *\n",
        "\n",
        "N_JOBS = -1\n",
        "RANDOM_STATE = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-1nc39QrJSv"
      },
      "outputs": [],
      "source": [
        "github_url = 'https://raw.githubusercontent.com/vMxster/Data_Project/main/Datasets/original_india_dataset.csv'\n",
        "local_path = '/scratch.hpc/martin.tomassi/datasets_pm'\n",
        "local_file = os.path.join(local_path, 'original_india_dataset.csv')\n",
        "\n",
        "if not os.path.exists(local_path):\n",
        "    os.makedirs(local_path)\n",
        "\n",
        "if not os.path.exists(local_file):\n",
        "    df = pd.read_csv(github_url,\n",
        "                     sep=',',\n",
        "                     quotechar='\"',\n",
        "                     dtype=None,\n",
        "                     parse_dates=True,\n",
        "                     low_memory=False)\n",
        "    df.to_csv(local_file, index=False)\n",
        "    print(\"File scaricato e salvato in locale.\")\n",
        "else:\n",
        "    print(\"Il file esiste già in locale, lo carico...\")\n",
        "    df = pd.read_csv(local_file,\n",
        "                     sep=',',\n",
        "                     quotechar='\"',\n",
        "                     dtype=None,\n",
        "                     parse_dates=True,\n",
        "                     low_memory=False)\n",
        "\n",
        "obj_cols = df.select_dtypes(include=\"object\").columns\n",
        "for col in obj_cols:\n",
        "    df[col] = df[col].astype(\"category\")\n",
        "df.drop('date', axis=1, inplace=True)\n",
        "df = df[(df['year'] >= 2018)]\n",
        "df = df.reset_index(drop=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5J-7bPorgC5"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHN34R2qmbEs"
      },
      "outputs": [],
      "source": [
        "class_counts = df['Class'].value_counts()\n",
        "print(class_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcQbIynRmGln"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Distribution of Classes')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcIEc735lViq",
        "tags": []
      },
      "source": [
        "# Addestramento modelli\n",
        "A seguito dell'esplorazione e dell'omogeneizzazione dei due dataset, si può procedere all'addestramento dei modelli. I modelli verranno addestrati sulle seguenti feature indipendenti:\n",
        "- `year`: anno della misurazione\n",
        "- `month`: mese dell’anno\n",
        "- `dayofmonth`: giorno del mese\n",
        "- `dayofweek`: giorno della settimana\n",
        "- `dayofyear`: giorno dell’anno\n",
        "- `weekofyear`: settimana dell’anno\n",
        "- `quarter`: trimestre dell’anno\n",
        "- `state`: stato di misurazione\n",
        "- `pm_lag_1W`: PM2.5 ritardato di 1 settimana\n",
        "- `pm_lag_1M`: PM2.5 ritardato di 1 mese\n",
        "- `pm_lag_1Y`: PM2.5 ritardato di 1 anno\n",
        "- `pm_lag_1D`: PM2.5 ritardato di 1 giorno\n",
        "- `pm_lag_2D`: PM2.5 ritardato di 2 giorni\n",
        "- `pm_lag_3D`: PM2.5 ritardato di 3 giorni\n",
        "- `co_lag_1W`: CO ritardato di 1 settimana\n",
        "- `co_lag_1M`: CO ritardato di 1 mese\n",
        "- `co_lag_1Y`: CO ritardato di 1 anno\n",
        "- `co_lag_1D`: CO ritardato di 1 giorno\n",
        "- `co_lag_2D`: CO ritardato di 2 giorni\n",
        "- `co_lag_3D`: CO ritardato di 1 anno\n",
        "- `o3_lag_1W`: O3 ritardato di 1 settimana\n",
        "- `o3_lag_1M`: O3 ritardato di 1 mese\n",
        "- `o3_lag_1Y`: O3 ritardato di 1 anno\n",
        "- `o3_lag_1D`: O3 ritardato di 1 giorno\n",
        "- `o3_lag_2D`: O3 ritardato di 2 giorni\n",
        "- `o3_lag_3D`: O3 ritardato di 3 giorni"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwC-dywTlViq"
      },
      "source": [
        "La variabile target per il nostro modello di addestramento sarà una variabile **discreta**, composta da 6 valori distinti. Questi valori rappresentano i diversi livelli di qualità dell'aria, come definiti dalla scala dell'**Environmental Protection Agency (EPA)** degli Stati Uniti per la concentrazione di PM2.5.\n",
        "\n",
        "Possiamo mappare numericamente questi livelli alle seguenti classi, mantenendo l'ordine implicito di gravità:\n",
        "\n",
        "* **1: \"Good\"** (Concentrazione di PM2.5: $0 - 9.0 \\mu g/m^3$)\n",
        "* **2: \"Moderate\"** (Concentrazione di PM2.5: $9.1 - 35.4 \\mu g/m^3$)\n",
        "* **3: \"Unhealthy for Sensitive Groups\"** (Concentrazione di PM2.5: $35.5 - 55.4 \\mu g/m^3$)\n",
        "* **4: \"Unhealthy\"** (Concentrazione di PM2.5: $55.5 - 125.4 \\mu g/m^3$)\n",
        "* **5: \"Very Unhealthy\"** (Concentrazione di PM2.5: $125.5 - 225.4 \\mu g/m^3$)\n",
        "* **6: \"Hazardous\"** (Concentrazione di PM2.5: $> 225.5 \\mu g/m^3$)\n",
        "\n",
        "Adottando questo schema, il problema si configura come un task di **classificazione multi-classe**. L'obiettivo del modello sarà prevedere a quale di questi 6 livelli di qualità dell'aria (o \"categorie di rischio\") appartiene una data osservazione, basandosi sulle caratteristiche di input fornite."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GveBLWbLlViq"
      },
      "source": [
        "## Preparazione Dataset\n",
        "Per garantire un confronto equo tra tutti i modelli, alcuni dei quali non supportano i valori mancanti generati dalle lag features, elimineremo tutte le righe che li contengono. Va però tenuto presente che così facendo perdiamo un anno di dati storici. Modelli come XGBoost di scikit-learn sono in grado di gestire internamente i missing value e potrebbero beneficiarne; tuttavia, per mantenere omogenee le condizioni di allenamento, applichiamo il drop completo dei NaN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2u4WO1bmlViq"
      },
      "outputs": [],
      "source": [
        "target = 'Class'\n",
        "lag_features = ['pm_lag_1Y', 'pm_lag_2Y', 'pm_lag_1M', 'pm_lag_1W','co_lag_1Y', 'co_lag_2Y', 'co_lag_1M', 'co_lag_1W','o3_lag_1Y', 'o3_lag_2Y', 'o3_lag_1M', 'o3_lag_1W','pm_lag_1D','co_lag_1D','o3_lag_1D','pm_lag_2D','co_lag_2D','o3_lag_2D','pm_lag_3D','co_lag_3D','o3_lag_3D']\n",
        "date_features = ['dayofmonth', 'dayofweek', 'dayofyear', 'weekofyear', 'month', 'quarter', 'year', 'state']\n",
        "predictors = date_features + lag_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efbb3yDvuaDc"
      },
      "outputs": [],
      "source": [
        "def create_train_test_sets(dataframe, split, replace_na=False, method='none'):\n",
        "    dataframe = dataframe.copy()\n",
        "\n",
        "    if replace_na and method == 'zeros':\n",
        "      dataframe = dataframe.fillna(0)\n",
        "    elif replace_na and method == 'drop':\n",
        "      dataframe = dataframe.dropna(how='any')\n",
        "\n",
        "    train_set, test_set = np.split(dataframe, [int(len(dataframe) * split)])\n",
        "    return train_set[predictors], test_set[predictors], train_set[target], test_set[target]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_dgAHmhuh2i"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = create_train_test_sets(\n",
        "    df,\n",
        "    split=0.8,\n",
        "    replace_na=True,\n",
        "    method='drop'\n",
        ")\n",
        "\n",
        "# Resetta gli indici dei risultati eliminando l’indice precedente,\n",
        "# in modo da partire da zero ed avere indici continui\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "y_train = y_train.reset_index(drop=True)\n",
        "y_test = y_test.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Mz9jup_67xf"
      },
      "outputs": [],
      "source": [
        "X_train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gl2OvBn56_PE"
      },
      "outputs": [],
      "source": [
        "X_test.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rjuwm2aHnklP"
      },
      "outputs": [],
      "source": [
        "print(y_train.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36laCti0nwg2"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(y_train.value_counts(), labels=y_train.value_counts().index, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Distribution of Classes')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPNDBjlAn3gU"
      },
      "outputs": [],
      "source": [
        "print(y_test.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "595RhSWPn6LP"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(y_test.value_counts(), labels=y_test.value_counts().index, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Distribution of Classes')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqBCT7crrf9A"
      },
      "source": [
        "Nel dataset ci sono sia feature numeriche che categoriche. <br>\n",
        "Per le numeriche è necessario applicare una normalizzazione dei dati, i quali avrebbero altrimenti valori su scale molto diverse che renderebbero più difficile la convergenza del modello. <br>\n",
        "Per poter utilizzare le variabili categoriche nell'addestramento di un modello di regressione si usa un OneHotEncoder, creando nuove colonne binarie per ciascuno dei valori ammissibili dalla variabile categorica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcK3Ma4545-a"
      },
      "outputs": [],
      "source": [
        "categorical_features = X_train.select_dtypes(include=[\"category\"]).columns.tolist()\n",
        "numerical_features   = [c for c in X_train.columns if c not in categorical_features]\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    # Standardizza tutte le colonne numeriche\n",
        "    (\"numeric\",    StandardScaler(),    numerical_features),\n",
        "    # One‑hot encoding di 'state', ignorando nuovi stati in predict\n",
        "    (\"categorical\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axPc4fYaKiDT"
      },
      "source": [
        "Inizializzazione della lista per raccogliere le metriche dopo ogni training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LfV4nYJKiPx"
      },
      "outputs": [],
      "source": [
        "all_scores = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_bIhHTRKAzQ"
      },
      "source": [
        "## Valutazione delle Prestazioni dei Modelli\n",
        "\n",
        "Dopo l’allenamento di ciascun modello di Machine Learning e Deep Learning, utilizziamo le funzioni `get_estimator_scores` e `get_torch_estimator_scores` per calcolare diverse metriche di valutazione, includendo per le principali anche gli **Intervalli di Confidenza al 95%** (CI95%) stimati tramite bootstrap resampling.\n",
        "\n",
        "Queste metriche sono state scelte specificamente per valutare l'efficacia dei modelli in problemi di **classificazione multi-classe, con particolare attenzione alle classi sbilanciate**.\n",
        "\n",
        "Le metriche calcolate sono:\n",
        "\n",
        "1.  **Accuracy Score**\n",
        "    Misura la percentuale di istanze classificate correttamente dal modello. Sebbene sia una metrica intuitiva, può essere fuorviante in presenza di classi sbilanciate, poiché un modello che classifica correttamente solo la classe maggioritaria può comunque mostrare un'alta accuratezza.\n",
        "    *(Più alto è, meglio è.)*\n",
        "\n",
        "2.  **F1-Score (Weighted)**\n",
        "    L'F1-Score è la media armonica della Precisione e del Recall. La versione \"weighted\" calcola la media di F1-Score per ciascuna classe, pesandola in base al numero di istanze di quella classe nel dataset. Questa metrica è particolarmente utile per dataset con classi sbilanciate, in quanto fornisce una visione più bilanciata delle prestazioni del modello su tutte le classi, evitando di essere dominata dalla classe maggioritaria.\n",
        "    *(Più alto è, meglio è.)*\n",
        "\n",
        "3.  **F1-Score (Macro)**\n",
        "    La versione \"macro\" dell'F1-Score calcola la media non pesata di F1-Score per ciascuna classe. Questo significa che ogni classe contribuisce in modo uguale alla metrica finale, indipendentemente dal suo numero di campioni. È utile quando tutte le classi, comprese quelle minoritarie, hanno la stessa importanza.\n",
        "    *(Più alto è, meglio è.)*\n",
        "\n",
        "4.  **Matrice di Confusione**\n",
        "    Una tabella che riassume le prestazioni di un algoritmo di classificazione. Mostra il numero di previsioni corrette e sbagliate per ciascuna classe, indicando dove il modello sta confondendo le diverse categorie. È fondamentale per comprendere gli errori specifici del modello.\n",
        "\n",
        "5.  **Classification Report**\n",
        "    Fornisce un riepilogo dettagliato delle metriche di Precisione, Recall e F1-Score per ciascuna classe, oltre a metriche aggregate (macro avg, weighted avg). È uno strumento essenziale per una valutazione approfondita delle prestazioni per classe.\n",
        "\n",
        "6.  **AUC-ROC (Area Under the Receiver Operating Characteristic Curve) - One-vs-Rest Weighted**\n",
        "    L'AUC-ROC misura la capacità del modello di distinguere tra le classi. Per problemi multi-classe, si calcola spesso in modalità \"one-vs-rest\", trattando ogni classe come \"positiva\" e tutte le altre come \"negative\". La versione \"weighted\" ne calcola la media pesata per la frequenza delle classi, rendendola più robusta per dataset sbilanciati. Un valore più vicino a 1 indica una maggiore capacità discriminatoria.\n",
        "    *(Più alto è, meglio è.)*\n",
        "\n",
        "7.  **AUC-PR (Area Under the Precision-Recall Curve) - One-vs-Rest Weighted**\n",
        "    L'AUC-PR è particolarmente utile per dataset con classi sbilanciate e quando la classe positiva (minoritaria) è di maggiore interesse. Misura l'area sotto la curva Precision-Recall, offrendo una valutazione più accurata della capacità del modello di identificare correttamente le istanze positive rispetto all'AUC-ROC, che può essere ottimistica in presenza di un grande numero di veri negativi. Per multi-classe, viene calcolata in modalità \"one-vs-rest\" e mediata pesando per la frequenza delle classi.\n",
        "    *(Più alto è, meglio è.)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zthJQ-LRL-8J"
      },
      "outputs": [],
      "source": [
        "def count_params(model):\n",
        "    if isinstance(model, ImbPipeline):\n",
        "        model = model.steps[-1][1]\n",
        "\n",
        "    if hasattr(model, 'parameters') and inspect.ismethod(model.parameters) and not isinstance(model, KAN):\n",
        "        try:\n",
        "            return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        except Exception:\n",
        "            return 0\n",
        "\n",
        "    elif isinstance(model, KAN):\n",
        "        try:\n",
        "            if not model.width or len(model.width) < 2:\n",
        "                return 0\n",
        "            else:\n",
        "                sum_edge_terms = 0\n",
        "                for i in range(len(model.width) - 1):\n",
        "                    Nl = model.width[i]\n",
        "                    Nl_plus_1 = model.width[i+1]\n",
        "                    if isinstance(Nl, list): Nl = Nl[0]\n",
        "                    if isinstance(Nl_plus_1, list): Nl_plus_1 = Nl_plus_1[0]\n",
        "                    G = model.grid\n",
        "                    k = model.k\n",
        "                    sum_edge_terms += Nl * Nl_plus_1 * (G + k - 1)\n",
        "                return sum_edge_terms\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating KAN parameters: {e}\")\n",
        "            return 0\n",
        "\n",
        "    elif isinstance(model, RandomForestClassifier):\n",
        "        total_nodes = 0\n",
        "        if hasattr(model, 'estimators_'):\n",
        "            for tree in model.estimators_:\n",
        "                if hasattr(tree, 'tree_'):\n",
        "                    total_nodes += tree.tree_.node_count\n",
        "            return total_nodes\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    elif isinstance(model, xgb.XGBClassifier):\n",
        "        total_nodes = 0\n",
        "\n",
        "        try:\n",
        "            booster = model.get_booster()\n",
        "            tree_dumps = booster.get_dump(dump_format='json')\n",
        "\n",
        "            def count_nodes_in_json_tree(node):\n",
        "                count = 1\n",
        "                if 'children' in node:\n",
        "                    for child in node['children']:\n",
        "                        count += count_nodes_in_json_tree(child)\n",
        "                return count\n",
        "\n",
        "            for tree_dump_str in tree_dumps:\n",
        "                tree_json = json.loads(tree_dump_str)\n",
        "                total_nodes += count_nodes_in_json_tree(tree_json)\n",
        "\n",
        "            return total_nodes\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating exact XGBoost complexity: {e}\")\n",
        "            return 0\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def bootstrap_ci(metric_fn, y_true, y_pred, n_bootstraps=1000, alpha=0.05, **metric_kwargs):\n",
        "    y_true_arr = np.asarray(y_true)\n",
        "    y_pred_arr = np.asarray(y_pred)\n",
        "    vals = []\n",
        "    n_samples = len(y_true)\n",
        "\n",
        "    for _ in range(n_bootstraps):\n",
        "        idx = np.random.randint(0, n_samples, n_samples)\n",
        "        vals.append(metric_fn(y_true_arr[idx], y_pred_arr[idx], **metric_kwargs))\n",
        "\n",
        "    low = np.percentile(vals, 100 * (alpha / 2))\n",
        "    high = np.percentile(vals, 100 * (1 - alpha / 2))\n",
        "    return low, high\n",
        "\n",
        "def get_estimator_scores(model_name, model, X_test, y_test, X_train, y_train, all_scores_list):\n",
        "    print(f\"\\n--- Valutazione Prestazioni per {model_name} ---\")\n",
        "\n",
        "    # Calcola la Complessitá dei Modelli\n",
        "    param_count = count_params(model)\n",
        "    print(f\"Model Parameters/Nodes: {param_count}\")\n",
        "\n",
        "    # Previsioni (etichette hard) per test e train\n",
        "    y_pred_test = model.predict(X_test)\n",
        "    y_pred_train = model.predict(X_train)\n",
        "\n",
        "    # Se il modello supporta predict_proba (per AUC)\n",
        "    y_proba_test = None\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        y_proba_test = model.predict_proba(X_test) # Probabilità per ogni classe\n",
        "\n",
        "    # --- Metriche su Training Set ---\n",
        "    accuracy_tr = accuracy_score(y_train, y_pred_train)\n",
        "    f1_weighted_tr = f1_score(y_train, y_pred_train, average='weighted', zero_division=0)\n",
        "    f1_macro_tr = f1_score(y_train, y_pred_train, average='macro', zero_division=0)\n",
        "\n",
        "    # --- Metriche su Test Set ---\n",
        "    accuracy_te = accuracy_score(y_test, y_pred_test)\n",
        "    f1_weighted_te = f1_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
        "    f1_macro_te = f1_score(y_test, y_pred_test, average='macro', zero_division=0)\n",
        "\n",
        "    # Bootstrap CI per F1-weighted\n",
        "    f1_weighted_low, f1_weighted_high = bootstrap_ci(\n",
        "        f1_score, y_test, y_pred_test, average='weighted', zero_division=0\n",
        "    )\n",
        "    f1_macro_low, f1_macro_high = bootstrap_ci(\n",
        "        f1_score, y_test, y_pred_test, average='macro', zero_division=0\n",
        "    )\n",
        "\n",
        "    # Matrice di Confusione e Classification Report\n",
        "    print(\"\\nClassification Report (Test Set):\")\n",
        "    print(classification_report(y_test, y_pred_test, zero_division=0))\n",
        "\n",
        "    print(\"\\nConfusion Matrix (Test Set):\")\n",
        "    print(confusion_matrix(y_test, y_pred_test))\n",
        "\n",
        "    # Raccolta dei punteggi\n",
        "    scores_row = [\n",
        "        model_name, param_count,\n",
        "        accuracy_tr, accuracy_te,\n",
        "        f1_weighted_tr, f1_weighted_te, f1_weighted_low, f1_weighted_high,\n",
        "        f1_macro_tr, f1_macro_te, f1_macro_low, f1_macro_high\n",
        "    ]\n",
        "\n",
        "    # Calcolo AUC se possibile (necessita probabilità)\n",
        "    if y_proba_test is not None:\n",
        "        try:\n",
        "            num_classes = len(np.unique(y_test))\n",
        "            auc_roc_ovr = roc_auc_score(y_test, y_proba_test, multi_class='ovr', average='weighted')\n",
        "            auc_pr_ovr = average_precision_score(pd.get_dummies(y_test), y_proba_test, average='weighted')\n",
        "\n",
        "            # Bootstrap per AUC-ROC e AUC-PR\n",
        "            auc_roc_low, auc_roc_high = bootstrap_ci(\n",
        "                lambda yt, yp: roc_auc_score(yt, yp, multi_class='ovr', average='weighted'),\n",
        "                y_test, y_proba_test\n",
        "            )\n",
        "            auc_pr_low, auc_pr_high = bootstrap_ci(\n",
        "                lambda yt, yp: average_precision_score(pd.get_dummies(yt), yp, average='weighted'),\n",
        "                y_test, y_proba_test\n",
        "            )\n",
        "\n",
        "            scores_row.extend([auc_roc_ovr, auc_roc_low, auc_roc_high, auc_pr_ovr, auc_pr_low, auc_pr_high])\n",
        "            print(f\"AUC-ROC (OVR, Weighted): {auc_roc_ovr:.3f}\")\n",
        "            print(f\"AUC-PR (OVR, Weighted): {auc_pr_ovr:.3f}\")\n",
        "\n",
        "        except ValueError as e:\n",
        "            print(f\"Errore nel calcolo di AUC/PR: {e}. Probabilmente mancano classi o y_proba non è adatto.\")\n",
        "            scores_row.extend([np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]) # Aggiungi NaN per le colonne AUC\n",
        "    else:\n",
        "        print(\"Modello non supporta predict_proba. AUC/PR non calcolabili.\")\n",
        "        scores_row.extend([np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]) # Aggiungi NaN per le colonne AUC\n",
        "\n",
        "    all_scores_list.append(scores_row)\n",
        "\n",
        "\n",
        "def predict_torch(model, X_tensor, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(X_tensor.to(device))\n",
        "    return out # Restituisce logit/probabilità, non etichette hard qui\n",
        "\n",
        "\n",
        "def get_torch_estimator_scores(model_name, model,\n",
        "                               X_train_tensor, y_train_tensor,\n",
        "                               X_test_tensor, y_test_tensor,\n",
        "                               device, all_scores_list):\n",
        "    print(f\"\\n--- Valutazione Prestazioni per {model_name} (PyTorch) ---\")\n",
        "\n",
        "    # Calcola il numero di Parametri dei Modelli\n",
        "    param_count = count_params(model)\n",
        "    print(f\"Model Parameters: {param_count}\")\n",
        "\n",
        "    # Ottieni output (logits/probabilità) dal modello\n",
        "    y_pred_proba_train = predict_torch(model, X_train_tensor, device)\n",
        "    y_pred_proba_test = predict_torch(model, X_test_tensor, device)\n",
        "\n",
        "    # Converti le etichette vere (tensori) in numpy array per Scikit-learn\n",
        "    y_train_np = y_train_tensor.detach().cpu().numpy()\n",
        "    y_test_np = y_test_tensor.detach().cpu().numpy()\n",
        "\n",
        "    # Se il modello emette logit, converti in probabilità e poi in etichette\n",
        "    y_pred_labels_train = torch.argmax(softmax(y_pred_proba_train, dim=1), dim=1).detach().cpu().numpy()\n",
        "    y_pred_labels_test = torch.argmax(softmax(y_pred_proba_test, dim=1), dim=1).detach().cpu().numpy()\n",
        "\n",
        "    # --- Metriche su Training Set ---\n",
        "    accuracy_tr = accuracy_score(y_train_np, y_pred_labels_train)\n",
        "    f1_weighted_tr = f1_score(y_train_np, y_pred_labels_train, average='weighted', zero_division=0)\n",
        "    f1_macro_tr = f1_score(y_train_np, y_pred_labels_train, average='macro', zero_division=0)\n",
        "\n",
        "    # --- Metriche su Test Set ---\n",
        "    accuracy_te = accuracy_score(y_test_np, y_pred_labels_test)\n",
        "    f1_weighted_te = f1_score(y_test_np, y_pred_labels_test, average='weighted', zero_division=0)\n",
        "    f1_macro_te = f1_score(y_test_np, y_pred_labels_test, average='macro', zero_division=0)\n",
        "\n",
        "    # Bootstrap CI\n",
        "    f1_weighted_low, f1_weighted_high = bootstrap_ci(\n",
        "        f1_score, y_test_np, y_pred_labels_test, average='weighted', zero_division=0\n",
        "    )\n",
        "    f1_macro_low, f1_macro_high = bootstrap_ci(\n",
        "        f1_score, y_test_np, y_pred_labels_test, average='macro', zero_division=0\n",
        "    )\n",
        "\n",
        "    print(\"\\nClassification Report (Test Set):\")\n",
        "    print(classification_report(y_test_np, y_pred_labels_test, zero_division=0))\n",
        "\n",
        "    print(\"\\nConfusion Matrix (Test Set):\")\n",
        "    print(confusion_matrix(y_test_np, y_pred_labels_test))\n",
        "\n",
        "    scores_row = [\n",
        "        model_name, param_count,\n",
        "        accuracy_tr, accuracy_te,\n",
        "        f1_weighted_tr, f1_weighted_te, f1_weighted_low, f1_weighted_high,\n",
        "        f1_macro_tr, f1_macro_te, f1_macro_low, f1_macro_high\n",
        "    ]\n",
        "\n",
        "    # Calcolo AUC (necessita probabilità)\n",
        "    try:\n",
        "        auc_roc_ovr = roc_auc_score(y_test_np, softmax(y_pred_proba_test, dim=1).detach().cpu().numpy(),\n",
        "                                    multi_class='ovr', average='weighted')\n",
        "        auc_pr_ovr = average_precision_score(pd.get_dummies(y_test_np), softmax(y_pred_proba_test, dim=1).detach().cpu().numpy(),\n",
        "                                            average='weighted')\n",
        "\n",
        "        # Bootstrap per AUC-ROC e AUC-PR\n",
        "        auc_roc_low, auc_roc_high = bootstrap_ci(\n",
        "            lambda yt, yp: roc_auc_score(yt, softmax(torch.tensor(yp), dim=1).numpy(), multi_class='ovr', average='weighted'),\n",
        "            y_test_np, y_pred_proba_test.detach().cpu().numpy()\n",
        "        )\n",
        "        auc_pr_low, auc_pr_high = bootstrap_ci(\n",
        "            lambda yt, yp: average_precision_score(pd.get_dummies(yt), softmax(torch.tensor(yp), dim=1).numpy(), average='weighted'),\n",
        "            y_test_np, y_pred_proba_test.detach().cpu().numpy()\n",
        "        )\n",
        "\n",
        "        scores_row.extend([auc_roc_ovr, auc_roc_low, auc_roc_high, auc_pr_ovr, auc_pr_low, auc_pr_high])\n",
        "        print(f\"AUC-ROC (OVR, Weighted): {auc_roc_ovr:.3f}\")\n",
        "        print(f\"AUC-PR (OVR, Weighted): {auc_pr_ovr:.3f}\")\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"Errore nel calcolo di AUC/PR: {e}. Probabilmente mancano classi o y_proba non è adatto.\")\n",
        "        scores_row.extend([np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]) # Aggiungi NaN per le colonne AUC\n",
        "    except Exception as e:\n",
        "        print(f\"Errore generico nel calcolo di AUC/PR per PyTorch: {e}\")\n",
        "        scores_row.extend([np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]) # Aggiungi NaN\n",
        "\n",
        "    all_scores_list.append(scores_row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmQB-Bd0rMVH"
      },
      "source": [
        "## Cross Validation TimeSeriesSplit\n",
        "La validazione viene eseguita con `TimeSeriesSplit`, una tecnica di cross-validation adatta alle Serie Temporali, che preserva l’ordine cronologico dividendo il dataset in fold sequenziali."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUTICD2IrPpu"
      },
      "outputs": [],
      "source": [
        "tscv = TimeSeriesSplit(n_splits=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtC45BuOrZzt"
      },
      "source": [
        "Nel blocco seguente viene visualizzata la suddivisione del dataset nei 5 fold della Time Series Cross-Validation.  \n",
        "Questa rappresentazione è utile per verificare che la sequenza temporale sia rispettata nella divisione dei dati tra training e validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCdJycTCrcU3"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(tscv.n_splits, 1, figsize=(12, 12), sharex=True)\n",
        "fig.tight_layout(pad=3.0)\n",
        "\n",
        "for index, (train_fold, validation_fold) in enumerate(tscv.split(y_train)):\n",
        "    sns.lineplot(data=y_train.iloc[train_fold], label='Training Set', ax=axes[index])\n",
        "    sns.lineplot(data=y_train.iloc[validation_fold], label='Validation Set', ax=axes[index])\n",
        "    axes[index].set_title(f'Time Series Split #{index}')\n",
        "    axes[index].set(xlabel=None, ylabel=None)\n",
        "\n",
        "    print(f\"Fold {index+1}:\")\n",
        "    print(\"-- Training set class distribution: --\")\n",
        "    print(y_train.iloc[train_fold].value_counts().sort_index())\n",
        "    print(\"-- Validation set class distribution: --\")\n",
        "    print(y_train.iloc[validation_fold].value_counts().sort_index())\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM7vW1UglVi8"
      },
      "source": [
        "## Random forest\n",
        "Il modello Random Forest è un ensemble di alberi decisionali che migliora la stabilità e la capacità predittiva rispetto a un singolo albero. Ogni albero viene addestrato su un sottoinsieme casuale del dataset (bagging) e valuta solo una parte delle feature, rendendo l’insieme più robusto a overfitting e variazioni nei dati.\n",
        "\n",
        "In questa configurazione iniziale, definiamo una pipeline che include anche una tecnica di **oversampling (SMOTE)** per bilanciare le classi nel dataset di addestramento.\n",
        "\n",
        "La pipeline è così configurata:\n",
        "- `max_samples=0.2`: ogni albero è addestrato su un campione casuale del 20% dei dati originali (con ripetizione);\n",
        "- `max_features='sqrt'`: ogni nodo dell’albero valuta solo un sottoinsieme di feature pari alla radice quadrata del numero totale di feature disponibili;\n",
        "- `n_estimators=200`: il modello è composto da 200 alberi decisionali;\n",
        "- `max_depth=None`: gli alberi possono crescere fino a foglie pure, senza una profondità massima prefissata;\n",
        "- `n_jobs=-1`: sfrutta tutti i core CPU disponibili per il training parallelo;\n",
        "- `random_state=RANDOM_STATE`: Per la riproducibilità.\n",
        "- `class_weight='balanced'`: attribuisce un peso inversamente proporzionale alla frequenza delle classi per gestire eventuali sbilanciamenti nel dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eieu-i1lVi9"
      },
      "outputs": [],
      "source": [
        "model = ImbPipeline([\n",
        "    (\"preproc\", preprocessor),\n",
        "    (\"sampler\", SMOTE(random_state=RANDOM_STATE)),\n",
        "    (\"tree\", RandomForestClassifier(max_samples=0.2, max_features=\"sqrt\",\n",
        "                                    n_estimators=200, max_depth=None,\n",
        "                                    n_jobs=N_JOBS, random_state=RANDOM_STATE,\n",
        "                                    class_weight='balanced'))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lB_F1HTDlVi9"
      },
      "outputs": [],
      "source": [
        "%time model.fit(X_train, y_train)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx9HoQcolVi9"
      },
      "source": [
        "Possiamo ricavare le 10 feature più importanti per la Random Forest, ovvero le variabili che sono state più utilizzate nella creazione degli alberi decisionali."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQVy9ajGlVi9"
      },
      "outputs": [],
      "source": [
        "pd.Series(model.named_steps[\"tree\"].feature_importances_, preprocessor.get_feature_names_out(X_train.columns)).sort_values(ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY3JUaW7lVi9"
      },
      "source": [
        "### Hyperparameter Tuning per Random Forest\n",
        "\n",
        "Eseguiamo una ricerca randomizzata (`RandomizedSearchCV`) per ottimizzare un insieme ampio di iperparametri fondamentali del modello Random Forest:\n",
        "\n",
        "- `n_estimators`: numero di alberi nella foresta;\n",
        "- `max_samples`: frazione massima di campioni usata per addestrare ogni singolo albero;\n",
        "- `max_depth`: profondità massima degli alberi;\n",
        "- `min_samples_leaf`: numero minimo di campioni richiesti per una foglia;\n",
        "- `max_features`: numero massimo di feature da considerare per ogni split.\n",
        "\n",
        "In questa fase di tuning, la pipeline include anche la tecnica di oversampling **SMOTE**, applicata correttamente all'interno di ogni fold di **Time Series Cross-Validation** per evitare il data leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_FPmQDJsIrd"
      },
      "outputs": [],
      "source": [
        "class_labels = np.unique(y_train)\n",
        "initial_weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=y_train)\n",
        "initial_class_weight_dict = dict(zip(class_labels, initial_weights))\n",
        "plt.bar(class_labels, initial_weights)\n",
        "plt.title('Pesi delle Classi')\n",
        "plt.xlabel('Classe')\n",
        "plt.ylabel('Peso')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S22MF1LolVi-"
      },
      "outputs": [],
      "source": [
        "grid = {\n",
        "    'tree__n_estimators': [150, 200, 250],\n",
        "    'tree__max_samples': [0.5, 0.8, 1.0],\n",
        "    'tree__max_depth': [5, 10, 20],\n",
        "    'tree__min_samples_split': [2, 5, 10],\n",
        "    'tree__min_samples_leaf': [2, 5, 10],\n",
        "    'tree__max_features': ['sqrt', 'log2']\n",
        "}\n",
        "model_ht = ImbPipeline([\n",
        "    (\"preproc\", preprocessor),\n",
        "    (\"sampler\", SMOTE(random_state=RANDOM_STATE, n_neighbors=2)),\n",
        "    (\"tree\", RandomForestClassifier(random_state=RANDOM_STATE, class_weight='balanced'))\n",
        "])\n",
        "gs_rf = RandomizedSearchCV(model_ht, grid, n_iter=111, cv=tscv, scoring='f1_weighted', n_jobs=N_JOBS, verbose=0, random_state=RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBzY1Fk3Codg"
      },
      "source": [
        "**Scelta del numero di iterazioni per RandomizedSearchCV con questo grid**\n",
        "\n",
        "Il grid ha:\n",
        "\n",
        "- Configurazioni Totali:\n",
        "$$\n",
        "M = 486\n",
        "$$\n",
        "\n",
        "Supponiamo di voler avere una probabilità \\( P = 0.90 \\) di includere almeno una delle migliori \\( k = 10 \\) configurazioni tra queste 162.\n",
        "\n",
        "Usiamo la formula:\n",
        "\n",
        "$$\n",
        "n = \\frac{\\ln(1 - P)}{\\ln\\left(1 - \\frac{k}{M}\\right)}\n",
        "$$\n",
        "\n",
        "Calcoliamo:\n",
        "\n",
        "$$\n",
        "n = \\frac{\\ln(1 - 0.90)}{\\ln\\left(1 - \\frac{10}{486}\\right)} = \\frac{\\ln(0.10)}{\\ln\\left(\\frac{476}{486}\\right)} = \\frac{-2.3026}{\\ln(476/486)} \\approx \\frac{2.3026}{0.0208} \\approx 110.70\n",
        "$$\n",
        "\n",
        "Quindi, con **111 iterazioni** di Randomized Search, si ha circa il 90% di probabilità di testare almeno una delle 10 migliori configurazioni, risparmiando molto rispetto a un Grid Search completo con 162 combinazioni.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRF8N_GYCte1"
      },
      "source": [
        "**Da dove viene la formula per stimare il numero di iterazioni nel Randomized Search?**\n",
        "\n",
        "Per stimare quante iterazioni (`n`) sono necessarie per avere una certa probabilità \\(P\\) di includere almeno una configurazione tra le \\(k\\) migliori (su \\(M\\) totali), usiamo la seguente logica probabilistica:\n",
        "\n",
        "1. Probabilità di *non* pescare una top-\\(k\\) in un singolo tentativo.\n",
        "Se ci sono \\(M\\) configurazioni totali e \\(k\\) di esse sono “quasi ottimali”, la probabilità di *non* sceglierne una buona è:\n",
        "$$\n",
        "1 - \\frac{k}{M}\n",
        "$$\n",
        "\n",
        "2. Probabilità di non pescarne *nessuna* in \\(n\\) tentativi indipendenti\n",
        "$$\n",
        "\\left(1 - \\frac{k}{M} \\right)^n\n",
        "$$\n",
        "\n",
        "3. Probabilità di pescare **almeno una** delle top-\\(k\\)\n",
        "$$\n",
        "P(\\text{≥1 top-}k) = 1 - \\left(1 - \\frac{k}{M} \\right)^n\n",
        "$$\n",
        "\n",
        "4. Ricavare \\(n\\) dalla formula\n",
        "$$\n",
        "1 - \\left(1 - \\frac{k}{M} \\right)^n = P\n",
        "\\quad \\Longrightarrow \\quad\n",
        "n = \\frac{\\ln(1 - P)}{\\ln\\left(1 - \\frac{k}{M} \\right)}\n",
        "$$\n",
        "\n",
        "5. Approssimazione per $$ k \\ll M $$\n",
        "Poiché $$ \\ln(1 - x) \\approx -x $$ per \\(x\\) piccolo:\n",
        "$$\n",
        "n \\approx - \\frac{\\ln(1 - P)}{k/M}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqxUYeNElVi-"
      },
      "outputs": [],
      "source": [
        "%time gs_rf.fit(X_train, y_train)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ML7A1bPflVi-"
      },
      "outputs": [],
      "source": [
        "get_estimator_scores(\"random_forest\", gs_rf.best_estimator_, X_test, y_test, X_train, y_train, all_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xPxYN70lVi_"
      },
      "source": [
        "## XGBoost\n",
        "\n",
        "XGBoost è un'implementazione ottimizzata di algoritmi di gradient boosting. A differenza del Random Forest che costruisce alberi indipendenti e poi ne aggrega i risultati, XGBoost costruisce alberi in sequenza, con ogni nuovo albero che corregge gli errori degli alberi precedenti.\n",
        "\n",
        "In questa configurazione iniziale, definiamo una pipeline che include anche una tecnica di **oversampling (SMOTE)** per bilanciare le classi nel dataset di addestramento.\n",
        "\n",
        "La pipeline è così configurata:\n",
        "- `sampler`: **SMOTE (Synthetic Minority Over-sampling Technique)**, che genera nuovi campioni sintetici per le classi minoritarie, rendendo il dataset più bilanciato per l'addestramento. `random_state` garantisce la riproducibilità.\n",
        "- `xgb`: il classificatore XGBoost con i seguenti iperparametri iniziali:\n",
        "    - `objective='multi:softprob'`: Specifica la funzione obiettivo di classificazione multi-classe, dove l'output è un array di probabilità per ogni classe. Questo è necessario per le metriche AUC.\n",
        "    - `num_class`: numero totale di classi.\n",
        "    - `n_estimators=200`: numero di alberi di boosting da costruire.\n",
        "    - `learning_rate=0.1`: La dimensione del passo di ridimensionamento del contributo di ogni albero.\n",
        "    - `use_label_encoder=False`: parametro deprecato e quindi va disabilitato per evitare warning.\n",
        "    - `eval_metric='mlogloss'`: metrica di valutazione da usare durante l'addestramento.\n",
        "    - `n_jobs=N_JOBS`: Sfrutta tutti i core CPU disponibili per il training parallelo.\n",
        "    - `random_state=RANDOM_STATE`: Per la riproducibilità.\n",
        "\n",
        "**Nota Importante:** Per XGBoost con multi-classe, le etichette della classe devono essere convertite in indici che partono da 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1A4je0oplVjA"
      },
      "outputs": [],
      "source": [
        "y_train_xgb = y_train - 1\n",
        "y_test_xgb = y_test - 1\n",
        "num_classes = len(np.unique(y_train_xgb))\n",
        "\n",
        "model = ImbPipeline([\n",
        "    (\"preproc\", preprocessor),\n",
        "    (\"sampler\", SMOTE(random_state=RANDOM_STATE)),\n",
        "    (\"xgb\", xgb.XGBClassifier(objective='multi:softprob',\n",
        "                              num_class=num_classes,\n",
        "                              n_estimators=200,\n",
        "                              learning_rate=0.1,\n",
        "                              use_label_encoder=False, # Deprecato, imposto a False\n",
        "                              eval_metric='mlogloss',\n",
        "                              n_jobs=N_JOBS,\n",
        "                              verbosity=0,\n",
        "                              random_state=RANDOM_STATE))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8A3332KolVjA"
      },
      "outputs": [],
      "source": [
        "%time model.fit(X_train, y_train_xgb)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUxDT_-nlVjA"
      },
      "source": [
        "Possiamo ricavare le 10 feature più importanti per l'XGBoost Classifier, ovvero le variabili che sono state più utilizzate nella creazione degli alberi di boosting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "us6Z4OmqlVjA"
      },
      "outputs": [],
      "source": [
        "pd.Series(model.named_steps[\"xgb\"].feature_importances_, preprocessor.get_feature_names_out(X_train.columns)).sort_values(ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uapeT7xKlVjA"
      },
      "source": [
        "### Hyperparameter Tuning per XGBoost\n",
        "Eseguiamo una ricerca esaustiva (GridSearchCV) per ottimizzare iperparametri fondamentali del modello XGBoost, combinandoli con la tecnica di resampling SMOTE. Ottimizzeremo:\n",
        "- `n_estimators`: Numero di alberi di boosting.\n",
        "- `learning_rate`: La dimensione del passo che riduce il contributo di ogni nuovo albero.\n",
        "- `max_depth`: La profondità massima di un albero.\n",
        "\n",
        "In questa fase di tuning, la pipeline includerà anche la tecnica di oversampling SMOTE, che verrà applicata in modo appropriato a ogni fold di cross-validation per garantire che il modello sia addestrato su un dataset bilanciato senza data leakage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovXa2C2UDO2E"
      },
      "source": [
        "---\n",
        "\n",
        "**Scelta del numero di iterazioni per RandomizedSearchCV con questo grid**\n",
        "\n",
        "Il grid ha:\n",
        "\n",
        "- Configurazioni Totali:\n",
        "$$\n",
        "M = 2187\n",
        "$$\n",
        "\n",
        "Supponiamo di voler avere una probabilità \\( P = 0.90 \\) di includere almeno una delle migliori \\( k = 10 \\) configurazioni tra queste 27.\n",
        "\n",
        "Usiamo la formula:\n",
        "\n",
        "$$\n",
        "n = \\frac{\\ln(1 - P)}{\\ln\\left(1 - \\frac{k}{M}\\right)}\n",
        "$$\n",
        "\n",
        "Calcoliamo:\n",
        "\n",
        "$$\n",
        "n = \\frac{\\ln(1 - 0.90)}{\\ln\\left(1 - \\frac{10}{2187}\\right)} = \\frac{\\ln(0.10)}{\\ln\\left(\\frac{2177}{2187}\\right)} = \\frac{-2.3026}{\\ln(\\frac{2177}{2187})} \\approx \\frac{2.3026}{0.0046} \\approx 500.57\n",
        "$$\n",
        "\n",
        "Quindi, con **501** di Randomized Search, si ha circa il 90% di probabilità di testare almeno una delle 10 migliori configurazioni, risparmiando molto rispetto a un Grid Search completo con 27 combinazioni.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwAwlSAxlVjA"
      },
      "outputs": [],
      "source": [
        "grid = {\n",
        "    \"xgb__max_depth\": [3, 5, 7],\n",
        "    \"xgb__learning_rate\": [0.05, 0.1, 0.2],\n",
        "    \"xgb__n_estimators\": [100, 200, 300],\n",
        "    \"xgb__subsample\": [0.7, 0.9, 1.0],\n",
        "    \"xgb__colsample_bytree\": [0.7, 0.9, 1.0],\n",
        "    \"xgb__gamma\": [0, 0.2, 0.4],\n",
        "    \"xgb__min_child_weight\": [1, 5, 10],\n",
        "}\n",
        "model_ht = ImbPipeline([\n",
        "    (\"preproc\", preprocessor),\n",
        "    (\"sampler\", SMOTE(random_state=RANDOM_STATE, n_neighbors=2)),\n",
        "    (\"xgb\", xgb.XGBClassifier(objective='multi:softprob',\n",
        "                              num_class=num_classes,\n",
        "                              use_label_encoder=False, # Deprecato\n",
        "                              eval_metric='mlogloss',\n",
        "                              n_jobs=N_JOBS,\n",
        "                              verbosity=0,\n",
        "                              random_state=RANDOM_STATE))\n",
        "])\n",
        "gs_xgb = RandomizedSearchCV(model_ht, grid, n_iter=501, cv=tscv, scoring='f1_weighted', n_jobs=N_JOBS, verbose=0, random_state=RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRafW1bilVjA"
      },
      "outputs": [],
      "source": [
        "%time gs_xgb.fit(X_train, y_train_xgb)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECO1JyGKlVjB"
      },
      "outputs": [],
      "source": [
        "get_estimator_scores(\"xgboost\", gs_xgb.best_estimator_, X_test, y_test_xgb, X_train, y_train_xgb, all_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlwH7ptOzQwq"
      },
      "source": [
        "## MLP and KAN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKVjVeNVzTPO"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device utilizzato: {device}\")\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_sizes, dropout, num_classes):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        dim = input_dim\n",
        "        for hs in hidden_sizes:\n",
        "            layers.append(nn.Linear(dim, hs))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "            dim = hs\n",
        "        layers.append(nn.Linear(dim, num_classes))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def build_kan(input_dim, width, grid, k, num_classes, seed=0):\n",
        "    model = KAN(\n",
        "        width=[input_dim] + list(width) + [num_classes],\n",
        "        grid=grid,\n",
        "        k=k,\n",
        "        seed=seed,\n",
        "        device=device\n",
        "    )\n",
        "    model.speed()  # enable efficient mode: disable symbolic branch\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1Ljjq9rZJZo"
      },
      "source": [
        "### Implementazione dell’Early Stopping\n",
        "\n",
        "La classe `EarlyStopper` implementa una logica di early stopping che interrompe anticipatamente l’addestramento se la performance sul validation set non migliora oltre una soglia (min_delta) per un numero consecutivo di epoche (patience).\n",
        "Questo approccio aiuta a evitare l’overfitting e riduce i tempi di addestramento, salvando il modello con la miglior loss di validazione osservata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8sQny3mZPSf"
      },
      "outputs": [],
      "source": [
        "class EarlyStopper:\n",
        "    def __init__(self, patience=3, min_delta=0.0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = float('inf')\n",
        "\n",
        "    def early_stop(self, val_loss):\n",
        "        # Se la loss migliora (di almeno min_delta), resettiamo il counter\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            # Se la loss non migliora da 'patience' epoche, dobbiamo fermarci\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Etc1WebYZlib"
      },
      "source": [
        "### Funzioni per il training e la valutazione\n",
        "\n",
        "- `train_epoch`: esegue una singola epoca di training. Calcola le previsioni, applica la loss function, esegue il backpropagation e aggiorna i pesi del modello. La loss viene aggregata e normalizzata sulla dimensione del dataset.\n",
        "\n",
        "- `eval_loss`: calcola la loss media del modello sul validation set in modalità eval, disabilitando l’aggiornamento dei pesi. Questo è fondamentale per valutare le prestazioni in modo affidabile durante il training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljnxu4EwZrSG"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, optimizer, criterion, l2_lambda=0.0):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for Xb, yb in loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(Xb), yb)\n",
        "\n",
        "        if l2_lambda > 0:\n",
        "            l2_reg = torch.tensor(0.).to(device)\n",
        "            for param in model.parameters():\n",
        "                l2_reg += torch.norm(param, 2)\n",
        "            loss += l2_lambda * l2_reg\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * Xb.size(0)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def eval_loss(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in loader:\n",
        "            Xb, yb = Xb.to(device), yb.to(device)\n",
        "            total_loss += criterion(model(Xb), yb.long()).item() * Xb.size(0)\n",
        "    return total_loss / len(loader.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcnVXcxkzVxm"
      },
      "source": [
        "### Random Search con Cross-Validation temporale e Early Stopping\n",
        "\n",
        "La funzione `random_search` implementa una strategia di ottimizzazione degli iperparametri che:\n",
        "\n",
        "- Estrae in modo casuale combinazioni di iperparametri dallo spazio definito (param_dist);\n",
        "- Valuta ogni configurazione tramite Cross Validation TimeSeriesSplit per rispettare l’ordine temporale dei dati;\n",
        "- Addestra un modello MLP per ciascun fold monitorando la loss di validazione;\n",
        "- Applica early stopping durante l’allenamento per evitare overfitting;\n",
        "- Calcola la media delle validation loss su tutti i fold per ogni configurazione.\n",
        "\n",
        "La combinazione con la miglior media viene salvata come modello ottimale, insieme agli iperparametri migliori."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iumk8NyQzZco"
      },
      "outputs": [],
      "source": [
        "def random_search(model_builder, param_dist, dataset,\n",
        "                  n_iter=10, cv_folds=5,\n",
        "                  early_patience=5,\n",
        "                  early_min_delta=1e-4,\n",
        "                  class_weights=None,\n",
        "                  smote_k_neighbors=6):\n",
        "    train_keys = ['lr', 'l2_lambda']\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_params, best_train_params = None, None\n",
        "    best_model = None\n",
        "\n",
        "    tscv = TimeSeriesSplit(n_splits=cv_folds)\n",
        "\n",
        "    print(\"Avvio Randomized Search\")\n",
        "\n",
        "    smote = SMOTE(\n",
        "        k_neighbors=smote_k_neighbors,\n",
        "        random_state=RANDOM_STATE\n",
        "    )\n",
        "\n",
        "    for param_id, params in enumerate(ParameterSampler(param_dist, n_iter=n_iter, random_state=RANDOM_STATE)):\n",
        "        print(f\"Testing parameter set {param_id+1}/{n_iter}\")\n",
        "\n",
        "        model_params = {k: v for k, v in params.items() if k not in train_keys}\n",
        "        train_params = {k: v for k, v in params.items() if k in train_keys}\n",
        "        val_losses = []\n",
        "\n",
        "        for fold_idx, (train_idx, val_idx) in enumerate(tscv.split(range(len(dataset)))):\n",
        "            print(f\"  Fold {fold_idx+1}/{cv_folds}\")\n",
        "\n",
        "            # Estrai i dati del fold di training\n",
        "            train_features = dataset.tensors[0][train_idx].cpu().numpy()\n",
        "            train_labels = dataset.tensors[1][train_idx].cpu().numpy()\n",
        "\n",
        "            # Estrai i dati di validazione\n",
        "            val_features = dataset.tensors[0][val_idx]\n",
        "            val_labels = dataset.tensors[1][val_idx]\n",
        "\n",
        "            # Applica SMOTE al training set\n",
        "            try:\n",
        "                # Verifica che ci siano almeno 2 classi nel training set\n",
        "                unique_classes = np.unique(train_labels)\n",
        "                if len(unique_classes) < 2:\n",
        "                    print(f\"    Warning: Solo {len(unique_classes)} classe/i nel fold {fold_idx+1}. Skip SMOTE.\")\n",
        "                    train_features_resampled = train_features\n",
        "                    train_labels_resampled = train_labels\n",
        "                else:\n",
        "                    # Verifica che ogni classe abbia almeno k_neighbors+1 campioni\n",
        "                    min_samples = min([np.sum(train_labels == cls) for cls in unique_classes])\n",
        "                    if min_samples <= smote_k_neighbors:\n",
        "                        print(f\"    Warning: Alcune classi hanno meno di {smote_k_neighbors+1} campioni. Riduco k_neighbors.\")\n",
        "                        smote_fold = SMOTE(\n",
        "                            k_neighbors=min(min_samples-1, 1),\n",
        "                            random_state=RANDOM_STATE\n",
        "                        )\n",
        "                    else:\n",
        "                        smote_fold = smote\n",
        "\n",
        "                    train_features_resampled, train_labels_resampled = smote_fold.fit_resample(\n",
        "                        train_features, train_labels\n",
        "                    )\n",
        "\n",
        "                    print(f\"    SMOTE applicato: {len(train_features)} -> {len(train_features_resampled)} campioni\")\n",
        "\n",
        "                    # Mostra la distribuzione delle classi dopo SMOTE\n",
        "                    unique, counts = np.unique(train_labels_resampled, return_counts=True)\n",
        "                    print(f\"    Distribuzione post-SMOTE: {{{', '.join(f'{u}: {c}' for u, c in zip(unique, counts))}}}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"    Warning: SMOTE fallito ({str(e)}). Uso dataset originale.\")\n",
        "                train_features_resampled = train_features\n",
        "                train_labels_resampled = train_labels\n",
        "\n",
        "            # Converti back a tensori PyTorch\n",
        "            train_features_tensor = torch.FloatTensor(train_features_resampled)\n",
        "            train_labels_tensor = torch.LongTensor(train_labels_resampled)\n",
        "\n",
        "            # Crea i dataset bilanciati\n",
        "            balanced_train_dataset = TensorDataset(train_features_tensor, train_labels_tensor)\n",
        "            val_dataset = TensorDataset(val_features, val_labels)\n",
        "\n",
        "            # Crea i DataLoader\n",
        "            train_loader = DataLoader(balanced_train_dataset, batch_size=32, shuffle=True)\n",
        "            val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "            # Inizializza e addestra il modello\n",
        "            model = model_builder(**model_params)\n",
        "            if hasattr(model, 'speed'):\n",
        "                model.speed()\n",
        "            model.to(device)\n",
        "            optimizer = optim.Adam(model.parameters(), lr=train_params['lr'])\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            stopper = EarlyStopper(patience=early_patience, min_delta=early_min_delta)\n",
        "\n",
        "            for epoch in range(1000):\n",
        "                train_loss = train_epoch(model, train_loader, optimizer, criterion, l2_lambda=train_params.get('l2_lambda', 0.0))\n",
        "                val_loss = eval_loss(model, val_loader, criterion)\n",
        "\n",
        "                if epoch % 10 == 0:\n",
        "                    print(f\"    Epoch {epoch}: train_loss = {train_loss:.6f}, val_loss = {val_loss:.6f}\")\n",
        "\n",
        "                if stopper.early_stop(val_loss):\n",
        "                    print(f\"    Early stopping at epoch {epoch}, best_val_loss: {stopper.best_loss:.6f}\")\n",
        "                    break\n",
        "\n",
        "            final_val_loss = eval_loss(model, val_loader, criterion)\n",
        "            val_losses.append(final_val_loss)\n",
        "\n",
        "        mean_val = np.mean(val_losses)\n",
        "        print(f\"  Mean validation loss: {mean_val:.6f}\")\n",
        "\n",
        "        if mean_val < best_val_loss:\n",
        "            best_val_loss = mean_val\n",
        "            best_model_params = model_params\n",
        "            best_train_params = train_params\n",
        "            best_model = model_builder(**best_model_params).to(device)\n",
        "            best_model.load_state_dict(model.state_dict())\n",
        "            print(f\"  New best validation loss: {best_val_loss:.6f}\")\n",
        "\n",
        "    print(f\"\\nBest validation loss: {best_val_loss:.6f}\")\n",
        "    return best_model, best_model_params, best_train_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVD5006Oza85"
      },
      "source": [
        "### Preparazione dati e spazio di ricerca iperparametri\n",
        "\n",
        "In questa sezione:\n",
        "\n",
        "- Applichiamo la trasformazione dei dati tramite il preprocessor già definito in precedenza;\n",
        "\n",
        "- Convertiamo le etichette y_train e y_test in formato 0-indexed, come richiesto dalla CrossEntropyLoss di PyTorch;\n",
        "\n",
        "- Creiamo un dataset PyTorch (TensorDataset) con input e target;\n",
        "\n",
        "- Definiamo lo spazio di ricerca per gli iperparametri, tra cui:\n",
        "\n",
        " Architettura della rete (hidden_sizes);\n",
        "\n",
        " Dropout;\n",
        "\n",
        " Tasso di apprendimento (lr);\n",
        "\n",
        " Numero di classi.\n",
        "\n",
        "Calcoliamo infine i pesi delle classi per bilanciare la loss in presenza di squilibri nella distribuzione delle etichette."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHuw7xi2zemy"
      },
      "outputs": [],
      "source": [
        "X_train_processed = preprocessor.fit_transform(X_train) # Fit and transform on training data\n",
        "X_test_processed = preprocessor.transform(X_test) # Only transform on test data\n",
        "\n",
        "y_train_0_indexed = y_train.values - 1\n",
        "y_test_0_indexed = y_test.values - 1\n",
        "\n",
        "X_tensor = torch.tensor(X_train_processed, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y_train_0_indexed, dtype=torch.long)\n",
        "\n",
        "full_dataset = TensorDataset(X_tensor, y_tensor)\n",
        "\n",
        "input_dim = X_tensor.shape[1]\n",
        "num_classes = len(np.unique(y_train_0_indexed))\n",
        "\n",
        "mlp_param_dist = {\n",
        "    'input_dim': [input_dim],\n",
        "    'hidden_sizes': [(32,32), (64,64), (128,)],\n",
        "    'dropout': [0.0, 0.2, 0.5],\n",
        "    'lr': [1e-3, 1e-4],\n",
        "    'num_classes': [num_classes],\n",
        "    'l2_lambda': [0.0, 1e-5, 1e-4, 1e-3]\n",
        "}\n",
        "kan_param_dist = {\n",
        "    'input_dim': [input_dim],\n",
        "    'width': [(8,4), (16,8)],\n",
        "    'grid': [5, 10],\n",
        "    'k': [2, 4],\n",
        "    'seed': [0],\n",
        "    'lr': [1e-3, 1e-4],\n",
        "    'num_classes': [num_classes],\n",
        "    'l2_lambda': [0.0, 1e-5, 1e-4, 1e-3]\n",
        "}\n",
        "\n",
        "class_labels_0_indexed = np.unique(y_train_0_indexed)\n",
        "class_weights_balanced = compute_class_weight(class_weight='balanced', classes=class_labels_0_indexed, y=y_train_0_indexed)\n",
        "class_weights_dict = dict(zip(class_labels_0_indexed, class_weights_balanced))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exVLSjhpZ88b"
      },
      "source": [
        "### Avvio della ricerca e valutazione\n",
        "\n",
        "- Eseguiamo la funzione random_search, fornendo:\n",
        "\n",
        " Il costruttore della rete MLP;\n",
        "\n",
        " Lo spazio degli iperparametri;\n",
        "\n",
        " Il dataset PyTorch e i pesi delle classi calcolati in precedenza;\n",
        "\n",
        "- Al termine, il modello con la miglior media di validation loss viene selezionato e restituito;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFrI6-zOZ_1h"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "print(\"MLP Results:\")\n",
        "best_model_mlp, model_params_mlp, train_params_mlp = random_search(\n",
        "    lambda **p: MLP(**p), mlp_param_dist, full_dataset,\n",
        "    class_weights=class_weights_dict, n_iter=15\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdriwTv3DO2H"
      },
      "source": [
        "---\n",
        "\n",
        "**Scelta del numero di iterazioni per RandomizedSearchCV con MLP grid**\n",
        "\n",
        "Il grid ha:\n",
        "\n",
        "- Configurazioni Totali:\n",
        "$$\n",
        "M = 72\n",
        "$$\n",
        "\n",
        "Supponiamo di voler avere una probabilità \\( P = 0.90 \\) di includere almeno una delle migliori \\( k = 10 \\) configurazioni tra queste 72.\n",
        "\n",
        "Usiamo la formula:\n",
        "\n",
        "$$\n",
        "n = \\frac{\\ln(1 - P)}{\\ln\\left(1 - \\frac{k}{M}\\right)}\n",
        "$$\n",
        "\n",
        "Calcoliamo:\n",
        "\n",
        "$$\n",
        "n = \\frac{\\ln(1 - 0.90)}{\\ln\\left(1 - \\frac{10}{72}\\right)} = \\frac{\\ln(0.10)}{\\ln\\left(\\frac{62}{72}\\right)} = \\frac{-2.3026}{\\ln(\\frac{62}{72})} \\approx \\frac{2.3026}{0.1495} \\approx 15.4\n",
        "$$\n",
        "\n",
        "Quindi, con **15 iterazioni** di Randomized Search, si ha circa il 90% di probabilità di testare almeno una delle 10 migliori configurazioni, risparmiando molto rispetto a un Grid Search completo con 72 combinazioni.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LROMl7eRPuJA"
      },
      "outputs": [],
      "source": [
        "X_test_tensor = torch.tensor(X_test_processed, dtype=torch.float32).to(device)\n",
        "y_test_tensor = torch.tensor(y_test_0_indexed, dtype=torch.long).to(device)\n",
        "\n",
        "get_torch_estimator_scores(\"MLP\", best_model_mlp,\n",
        "                           X_tensor.to(device), y_tensor.to(device),\n",
        "                           X_test_tensor, y_test_tensor,\n",
        "                           device, all_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvUB_bWyqvyH"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "print(\"KAN Results:\")\n",
        "best_model_kan, model_params_kan, train_params_kan = random_search(\n",
        "    lambda **p: build_kan(**p), kan_param_dist, full_dataset,\n",
        "    class_weights=class_weights_dict, n_iter=14\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SLtB7NWDO2I"
      },
      "source": [
        "---\n",
        "\n",
        "**Scelta del numero di iterazioni per RandomizedSearchCV con KAN grid**\n",
        "\n",
        "Il grid ha:\n",
        "\n",
        "- Configurazioni Totali:\n",
        "$$\n",
        "M = 64\n",
        "$$\n",
        "\n",
        "Supponiamo di voler avere una probabilità \\( P = 0.90 \\) di includere almeno una delle migliori \\( k = 10 \\) configurazioni tra queste 64.\n",
        "\n",
        "Usiamo la formula:\n",
        "\n",
        "$$\n",
        "n = \\frac{\\ln(1 - P)}{\\ln\\left(1 - \\frac{k}{M}\\right)}\n",
        "$$\n",
        "\n",
        "Calcoliamo:\n",
        "\n",
        "$$\n",
        "n = \\frac{\\ln(1 - 0.90)}{\\ln\\left(1 - \\frac{10}{64}\\right)} = \\frac{\\ln(0.10)}{\\ln\\left(\\frac{54}{64}\\right)} = \\frac{-2.3026}{\\ln(\\frac{54}{64})} \\approx \\frac{2.3026}{0.1699} \\approx 13.55\n",
        "$$\n",
        "\n",
        "Quindi, con **14 iterazioni** di Randomized Search, si ha circa il 90% di probabilità di testare almeno una delle 10 migliori configurazioni, risparmiando molto rispetto a un Grid Search completo con 64 combinazioni.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omigerVfqxl2"
      },
      "outputs": [],
      "source": [
        "X_test_tensor = torch.tensor(X_test_processed, dtype=torch.float32).to(device)\n",
        "y_test_tensor = torch.tensor(y_test_0_indexed, dtype=torch.long).to(device)\n",
        "\n",
        "get_torch_estimator_scores(\"KAN\", best_model_kan,\n",
        "                           X_tensor.to(device), y_tensor.to(device),\n",
        "                           X_test_tensor, y_test_tensor,\n",
        "                           device, all_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uIugxCaMC9R"
      },
      "source": [
        "# Confronto Visivo delle Prestazioni dei Modelli\n",
        "\n",
        "La funzione `plot_estimator_scores` consente di visualizzare in modo sintetico ed intuitivo le metriche di valutazione di tutti i modelli allenati.\n",
        "\n",
        "Questa visualizzazione finale è utile per trarre conclusioni sulla bontà predittiva di ciascun modello e guidare la scelta del miglior approccio da adottare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vxh_8pW9qkC"
      },
      "outputs": [],
      "source": [
        "def plot_estimator_scores(scores):\n",
        "\n",
        "    model_order = scores['Model'].tolist()\n",
        "    palette = sns.color_palette(\"viridis\", len(model_order))\n",
        "    model_colors = {model: palette[i] for i, model in enumerate(model_order)}\n",
        "\n",
        "    fig, axs = plt.subplots(2, 3, figsize=(20, 12))\n",
        "    fig.tight_layout(pad=4.0)\n",
        "\n",
        "    # --- Plot 1: Accuracy (Train vs Test) ---\n",
        "    axs[0, 0].set_title('Accuracy (Train vs Test)')\n",
        "    axs[0, 0].set_xlabel('Accuracy Score')\n",
        "    axs[0, 0].set_ylabel('Model')\n",
        "    axs[0, 0].set_xlim(0, 1)\n",
        "\n",
        "    bar_height = 0.4\n",
        "    for i, model in enumerate(model_order):\n",
        "        row = scores[scores['Model'] == model].iloc[0]\n",
        "        y_pos = i - bar_height/2\n",
        "\n",
        "        axs[0, 0].barh(\n",
        "            y_pos,\n",
        "            row['Accuracy_Train'],\n",
        "            height=bar_height,\n",
        "            color=model_colors[model],\n",
        "            label=f'{model} - Train' if i == 0 else \"\"\n",
        "        )\n",
        "\n",
        "        y_pos = i + bar_height/2\n",
        "        axs[0, 0].barh(\n",
        "            y_pos,\n",
        "            row['Accuracy_Test'],\n",
        "            height=bar_height,\n",
        "            color=model_colors[model],\n",
        "            alpha=0.6,\n",
        "            hatch='//',\n",
        "            label=f'{model} - Test' if i == 0 else \"\"\n",
        "        )\n",
        "\n",
        "    axs[0, 0].legend(handles=[\n",
        "        Patch(color='gray', label='Train'),\n",
        "        Patch(color='gray', label='Test', hatch='//', alpha=0.6)\n",
        "    ], title='Set', loc='lower right')\n",
        "\n",
        "    axs[0, 0].set_yticks(range(len(model_order)))\n",
        "    axs[0, 0].set_yticklabels(model_order)\n",
        "\n",
        "    # --- Helper function for consistent bar plots with CIs ---\n",
        "    def plot_barh_with_ci(ax, data, metric_col, ci_low_col, ci_high_col, title, model_order, model_colors):\n",
        "        ax.set_title(title)\n",
        "        data_ordered = data.set_index('Model').loc[model_order].reset_index()\n",
        "\n",
        "        for i, row in data_ordered.iterrows():\n",
        "            val = row.get(metric_col)\n",
        "            if pd.isna(val):\n",
        "                continue\n",
        "\n",
        "            err_low = [val - row.get(ci_low_col, val)] if pd.notna(row.get(ci_low_col)) else [0]\n",
        "            err_high = [row.get(ci_high_col, val) - val] if pd.notna(row.get(ci_high_col)) else [0]\n",
        "\n",
        "            ax.barh(\n",
        "                row['Model'],\n",
        "                val,\n",
        "                xerr=[err_low, err_high],\n",
        "                capsize=5,\n",
        "                color=model_colors[row['Model']]\n",
        "            )\n",
        "        ax.set_xlabel(title.split(' ')[0])\n",
        "        ax.invert_yaxis()\n",
        "        ax.set_xlim(0, 1)\n",
        "\n",
        "    # --- Plot 2: F1-Weighted Test Score ± CI95% ---\n",
        "    plot_barh_with_ci(axs[0, 1], scores, 'F1_Weighted_Test', 'F1_Weighted_CI_Low', 'F1_Weighted_CI_High',\n",
        "                      'F1-Weighted Test ± CI95%', model_order, model_colors)\n",
        "\n",
        "    # --- Plot 3: F1-Macro Test Score ± CI95% ---\n",
        "    plot_barh_with_ci(axs[0, 2], scores, 'F1_Macro_Test', 'F1_Macro_CI_Low', 'F1_Macro_CI_High',\n",
        "                      'F1-Macro Test ± CI95%', model_order, model_colors)\n",
        "\n",
        "    # --- Plot 4: AUC-ROC OVR Weighted Test Score ± CI95% ---\n",
        "    plot_barh_with_ci(axs[1, 0], scores, 'AUC_ROC_OVR_Weighted', 'AUC_ROC_CI_Low', 'AUC_ROC_CI_High',\n",
        "                      'AUC-ROC (OVR Weighted) Test ± CI95%', model_order, model_colors)\n",
        "\n",
        "    # --- Plot 5: AUC-PR OVR Weighted Test Score ± CI95% ---\n",
        "    plot_barh_with_ci(axs[1, 1], scores, 'AUC_PR_OVR_Weighted', 'AUC_PR_CI_Low', 'AUC_PR_CI_High',\n",
        "                      'AUC-PR (OVR Weighted) Test ± CI95%', model_order, model_colors)\n",
        "\n",
        "    # --- Plot 6: Model Complexity (Parameter/Node Count) ---\n",
        "    axs[1, 2].set_title('Model Complexity (Parameters/Nodes)')\n",
        "    scores_ordered = scores.set_index('Model').loc[model_order].reset_index()\n",
        "\n",
        "    bars = axs[1, 2].barh(scores_ordered['Model'], scores_ordered['Param_Count'],\n",
        "                          color=[model_colors[m] for m in scores_ordered['Model']])\n",
        "    axs[1, 2].set_xlabel('Parameter/Node Count')\n",
        "    axs[1, 2].invert_yaxis()\n",
        "\n",
        "    for i, (bar, count) in enumerate(zip(bars, scores_ordered['Param_Count'])):\n",
        "        if pd.notna(count) and count > 0:\n",
        "            axs[1, 2].text(bar.get_width() + max(scores_ordered['Param_Count']) * 0.01,\n",
        "                           bar.get_y() + bar.get_height()/2,\n",
        "                           f'{int(count):,}',\n",
        "                           va='center', fontsize=9)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTHU3LQ49qkD"
      },
      "outputs": [],
      "source": [
        "estimator_scores_df = pd.DataFrame(\n",
        "    all_scores,\n",
        "    columns = [\n",
        "        'Model', 'Param_Count',\n",
        "        'Accuracy_Train', 'Accuracy_Test',\n",
        "        'F1_Weighted_Train', 'F1_Weighted_Test', 'F1_Weighted_CI_Low', 'F1_Weighted_CI_High',\n",
        "        'F1_Macro_Train', 'F1_Macro_Test', 'F1_Macro_CI_Low', 'F1_Macro_CI_High',\n",
        "        'AUC_ROC_OVR_Weighted', 'AUC_ROC_CI_Low', 'AUC_ROC_CI_High',\n",
        "        'AUC_PR_OVR_Weighted', 'AUC_PR_CI_Low', 'AUC_PR_CI_High'\n",
        "    ]\n",
        ")\n",
        "plot_estimator_scores(estimator_scores_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvcGaDcg8Q8R"
      },
      "source": [
        "# Selezione del Miglior Modello\n",
        "Calcoliamo un punteggio complessivo per ciascun modello basandoci sulle metriche di valutazione."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0DEhNMS8M8S"
      },
      "outputs": [],
      "source": [
        "# 1) Define metrics and their optimization direction\n",
        "metrics = {\n",
        "    'Accuracy_Test': 'max',\n",
        "    'F1_Weighted_Test': 'max',\n",
        "    'F1_Macro_Test': 'max',\n",
        "    'AUC_ROC_OVR_Weighted': 'max',\n",
        "    'AUC_PR_OVR_Weighted': 'max'\n",
        "}\n",
        "\n",
        "# 2) Build ranking DataFrame\n",
        "df_ranks = estimator_scores_df.set_index('Model')\n",
        "ranks = pd.DataFrame(index=df_ranks.index)\n",
        "\n",
        "# Calculate ranks for performance metrics\n",
        "for metric, direction in metrics.items():\n",
        "    if direction == 'max':\n",
        "        # Per metriche 'max' (più alto è meglio), rank in ordine decrescente (rank 1 al migliore)\n",
        "        ranks[f\"{metric}_rank\"] = df_ranks[metric].rank(ascending=False, method='average')\n",
        "    elif direction == 'min':\n",
        "        # Per metriche 'min' (più basso è meglio), rank in ordine crescente (rank 1 al migliore)\n",
        "        ranks[f\"{metric}_rank\"] = df_ranks[metric].rank(ascending=True, method='average')\n",
        "\n",
        "# Calculate complexity rank (lower parameter count is better)\n",
        "ranks['Complexity_rank'] = df_ranks['Param_Count'].rank(ascending=True, method='average')\n",
        "\n",
        "# 3) Calculate weighted scores\n",
        "# Performance score (average of performance ranks)\n",
        "performance_cols = [col for col in ranks.columns if col.endswith('_rank') and col != 'Complexity_rank']\n",
        "ranks['performance_score'] = ranks[performance_cols].mean(axis=1)\n",
        "\n",
        "# Method 1: Equal weighting\n",
        "ranks['equal_weight_score'] = ranks['performance_score'] + ranks['Complexity_rank']\n",
        "\n",
        "# Method 2: Complexity heavily weighted (complexity counts 2x)\n",
        "ranks['complexity_weighted_score'] = ranks['performance_score'] + (2 * ranks['Complexity_rank'])\n",
        "\n",
        "# Method 3: Extreme complexity weighting (complexity counts 3x)\n",
        "ranks['extreme_complexity_score'] = ranks['performance_score'] + (3 * ranks['Complexity_rank'])\n",
        "\n",
        "# Method 4: Pareto efficiency approach (performance vs complexity)\n",
        "# Normalize scores to [0,1] for fair comparison\n",
        "performance_norm = (ranks['performance_score'] - ranks['performance_score'].min()) / (ranks['performance_score'].max() - ranks['performance_score'].min())\n",
        "complexity_norm = (ranks['Complexity_rank'] - ranks['Complexity_rank'].min()) / (ranks['Complexity_rank'].max() - ranks['Complexity_rank'].min())\n",
        "ranks['pareto_score'] = 0.4 * performance_norm + 0.6 * complexity_norm  # 60% weight on complexity\n",
        "\n",
        "# Display results for each method\n",
        "methods = {\n",
        "    'Equal Weight (1:1)': 'equal_weight_score',\n",
        "    'Complexity Weighted (1:2)': 'complexity_weighted_score',\n",
        "    'Extreme Complexity (1:3)': 'extreme_complexity_score',\n",
        "    'Pareto Approach (40:60)': 'pareto_score'\n",
        "}\n",
        "\n",
        "results_summary = pd.DataFrame(index=df_ranks.index)\n",
        "results_summary['Performance_Score'] = ranks['performance_score']\n",
        "results_summary['Complexity_Rank'] = ranks['Complexity_rank']\n",
        "results_summary['Param_Count'] = df_ranks['Param_Count']\n",
        "\n",
        "for method_name, score_col in methods.items():\n",
        "    best_model = ranks[score_col].idxmin() if 'pareto' not in score_col else ranks[score_col].idxmin()\n",
        "    best_score = ranks.loc[best_model, score_col]\n",
        "    results_summary[method_name] = ranks[score_col]\n",
        "    print(f\"{method_name:25} -> {best_model:15} (score: {best_score:.3f})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DETAILED RANKING TABLE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create comprehensive ranking table\n",
        "ranking_display = pd.DataFrame(index=df_ranks.index)\n",
        "ranking_display['Param_Count'] = df_ranks['Param_Count'].astype(int)\n",
        "ranking_display['Avg_Performance'] = ranks['performance_score'].round(2)\n",
        "ranking_display['Complexity_Rank'] = ranks['Complexity_rank'].astype(int)\n",
        "\n",
        "for method_name, score_col in methods.items():\n",
        "    ranking_display[f'{method_name.split()[0]}_Rank'] = ranks[score_col].rank().astype(int)\n",
        "\n",
        "# Sort by complexity-weighted score (our recommended approach)\n",
        "ranking_display_sorted = ranking_display.sort_values('Complexity_Rank')\n",
        "display(ranking_display_sorted)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RECOMMENDATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Our recommended model (complexity weighted approach)\n",
        "recommended_model = ranks['complexity_weighted_score'].idxmin()\n",
        "recommended_score = ranks.loc[recommended_model, 'complexity_weighted_score']\n",
        "recommended_params = df_ranks.loc[recommended_model, 'Param_Count']\n",
        "recommended_f1 = df_ranks.loc[recommended_model, 'F1_Weighted_Test']\n",
        "\n",
        "print(f\"   RECOMMENDED MODEL: {recommended_model}\")\n",
        "print(f\"   Reason: Best balance between performance and complexity\")\n",
        "print(f\"   Parameters: {int(recommended_params):,}\")\n",
        "print(f\"   F1-Weighted Score: {recommended_f1:.4f}\")\n",
        "print(f\"   Complexity-Weighted Rank Score: {recommended_score:.3f}\")\n",
        "\n",
        "# Show top 3 models for comparison\n",
        "print(f\"\\n  TOP 3 MODELS (Complexity-Weighted Ranking):\")\n",
        "top_3 = ranks.sort_values('complexity_weighted_score').head(3)\n",
        "for i, (model, row) in enumerate(top_3.iterrows(), 1):\n",
        "    params = int(df_ranks.loc[model, 'Param_Count'])\n",
        "    f1_value = df_ranks.loc[model, 'F1_Weighted_Test']\n",
        "    print(f\"   {i}. {model:15} | Params: {params:>8,} | F1: {f1_value:.4f} | Score: {row['complexity_weighted_score']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtGjfjViDQKq"
      },
      "source": [
        "# Ablation Study MLP e KAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGFO7NWkDRjO"
      },
      "outputs": [],
      "source": [
        "class StandalonePruningAblationStudy:\n",
        "    def __init__(self, device='cpu'):\n",
        "        self.device = device\n",
        "        self.pruning_results = []\n",
        "\n",
        "    def get_model_sparsity(self, model):\n",
        "        \"\"\"Calcola la sparsità per modelli MLP/KAN standalone\"\"\"\n",
        "        if hasattr(model, 'width') and hasattr(model, 'act_fun'):\n",
        "            # Modello KAN standalone\n",
        "            try:\n",
        "                # Calcola parametri totali KAN usando count_params esistente\n",
        "                total_params = count_params(model)\n",
        "\n",
        "                # Conta i parametri zero nella componente KAN\n",
        "                zero_params = 0\n",
        "                for i in range(len(model.width) - 1):\n",
        "                    if i < len(model.act_fun):\n",
        "                        layer = model.act_fun[i]\n",
        "                        # Accedi ai coefficienti spline (coef parameter)\n",
        "                        if hasattr(layer, 'coef') and layer.coef is not None:\n",
        "                            zero_params += float(torch.sum(layer.coef == 0))\n",
        "\n",
        "                return zero_params / total_params if total_params > 0 else 0.0\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Error calculating KAN sparsity: {e}\")\n",
        "                return 0.0\n",
        "\n",
        "        else:\n",
        "            # Modello MLP standalone\n",
        "            zero_params = 0\n",
        "            total_params = count_params(model)\n",
        "\n",
        "            # Conta parametri zero in tutti i layer Linear\n",
        "            for module in model.modules():\n",
        "                if isinstance(module, torch.nn.Linear):\n",
        "                    if hasattr(module, 'weight'):\n",
        "                        zero_params += float(torch.sum(module.weight == 0))\n",
        "                    if hasattr(module, 'bias') and module.bias is not None:\n",
        "                        zero_params += float(torch.sum(module.bias == 0))\n",
        "\n",
        "            return zero_params / total_params if total_params > 0 else 0.0\n",
        "\n",
        "    def count_active_parameters(self, model):\n",
        "        \"\"\"Conta i parametri attivi nei modelli MLP/KAN standalone\"\"\"\n",
        "        if hasattr(model, 'width') and hasattr(model, 'act_fun'):\n",
        "            # Modello KAN standalone\n",
        "            try:\n",
        "                active_params = 0\n",
        "                for i in range(len(model.width) - 1):\n",
        "                    if i < len(model.act_fun):\n",
        "                        layer = model.act_fun[i]\n",
        "                        if hasattr(layer, 'coef') and layer.coef is not None:\n",
        "                            active_params += float(torch.sum(layer.coef != 0))\n",
        "\n",
        "                return int(active_params)\n",
        "            except:\n",
        "                return count_params(model)\n",
        "\n",
        "        else:\n",
        "            # Modello MLP standalone\n",
        "            active_params = 0\n",
        "            for module in model.modules():\n",
        "                if isinstance(module, torch.nn.Linear):\n",
        "                    if hasattr(module, 'weight'):\n",
        "                        active_params += float(torch.sum(module.weight != 0))\n",
        "                    if hasattr(module, 'bias') and module.bias is not None:\n",
        "                        active_params += float(torch.sum(module.bias != 0))\n",
        "\n",
        "            return int(active_params)\n",
        "\n",
        "\n",
        "    def apply_l1_pruning_standalone(self, model, pruning_ratio):\n",
        "        \"\"\"Applica L1 norm pruning a modelli MLP/KAN standalone\"\"\"\n",
        "        pruned_model = copy.deepcopy(model)\n",
        "\n",
        "        if hasattr(model, 'width') and hasattr(model, 'act_fun'):\n",
        "            # Modello KAN standalone\n",
        "            try:\n",
        "                if pruning_ratio == 0.0:\n",
        "                    return pruned_model\n",
        "\n",
        "                # Colleziona tutti i parametri KAN per L1 pruning globale\n",
        "                kan_modules_to_prune = []\n",
        "                for i in range(len(model.width) - 1):\n",
        "                    if i < len(model.act_fun):\n",
        "                        layer = pruned_model.act_fun[i]\n",
        "                        if hasattr(layer, 'coef') and layer.coef is not None:\n",
        "                            temp_module = torch.nn.Linear(1, 1, bias=False)\n",
        "                            temp_module.weight = torch.nn.Parameter(layer.coef.view(-1, 1))\n",
        "                            kan_modules_to_prune.append((temp_module, 'weight'))\n",
        "\n",
        "\n",
        "                if kan_modules_to_prune:\n",
        "                    # Applica L1 pruning globale sui parametri KAN\n",
        "                    prune.global_unstructured(\n",
        "                        kan_modules_to_prune,\n",
        "                        pruning_method=prune.L1Unstructured,\n",
        "                        amount=pruning_ratio,\n",
        "                    )\n",
        "\n",
        "                    # Applica le maschere ai coefficienti originali\n",
        "                    idx = 0\n",
        "                    for i in range(len(model.width) - 1):\n",
        "                        if i < len(model.act_fun):\n",
        "                            layer = pruned_model.act_fun[i]\n",
        "                            if hasattr(layer, 'coef') and layer.coef is not None:\n",
        "                                original_shape = layer.coef.shape\n",
        "                                mask = kan_modules_to_prune[idx][0].weight_mask.view(original_shape)\n",
        "                                layer.coef.data = layer.coef.data * mask\n",
        "                                idx += 1\n",
        "\n",
        "                    # Rimuove le maschere temporanee\n",
        "                    for module, param_name in kan_modules_to_prune:\n",
        "                        prune.remove(module, param_name)\n",
        "\n",
        "                print(f\"  Applied L1 pruning to standalone KAN with ratio: {pruning_ratio:.3f}\")\n",
        "                return pruned_model\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Error during KAN L1 pruning: {e}\")\n",
        "                return model\n",
        "\n",
        "        else:\n",
        "            # Modello MLP standalone\n",
        "            modules_to_prune = []\n",
        "\n",
        "            # Colleziona tutti i layer Linear\n",
        "            for module in pruned_model.modules():\n",
        "                if isinstance(module, torch.nn.Linear):\n",
        "                    modules_to_prune.append((module, 'weight'))\n",
        "                    if hasattr(module, 'bias') and module.bias is not None:\n",
        "                        modules_to_prune.append((module, 'bias'))\n",
        "\n",
        "            if modules_to_prune:\n",
        "                # Applica L1 pruning globale\n",
        "                prune.global_unstructured(\n",
        "                    modules_to_prune,\n",
        "                    pruning_method=prune.L1Unstructured,\n",
        "                    amount=pruning_ratio,\n",
        "                )\n",
        "\n",
        "                # Rende permanente il pruning\n",
        "                for module, param_name in modules_to_prune:\n",
        "                    prune.remove(module, param_name)\n",
        "\n",
        "            print(f\"  Applied L1 pruning to standalone MLP with ratio: {pruning_ratio:.3f}\")\n",
        "            return pruned_model\n",
        "\n",
        "\n",
        "    def evaluate_pruned_model(self, model, model_name, X_test, y_test, X_train, y_train):\n",
        "        \"\"\"Valuta le prestazioni di un modello pruned su dati tabulari\"\"\"\n",
        "        # Ensure the model is on the correct device\n",
        "        model.to(self.device)\n",
        "        model.eval()\n",
        "\n",
        "        # Converte in tensori, se necessario, e sposta sul device\n",
        "        if not isinstance(X_test, torch.Tensor):\n",
        "            X_test = torch.FloatTensor(X_test).to(self.device)\n",
        "        else:\n",
        "            X_test = X_test.to(self.device)\n",
        "\n",
        "        if not isinstance(y_test, torch.Tensor):\n",
        "            y_test = torch.LongTensor(y_test).to(self.device)\n",
        "        else:\n",
        "             y_test = y_test.to(self.device)\n",
        "\n",
        "        if not isinstance(X_train, torch.Tensor):\n",
        "            X_train = torch.FloatTensor(X_train).to(self.device)\n",
        "        else:\n",
        "            X_train = X_train.to(self.device)\n",
        "\n",
        "        if not isinstance(y_train, torch.Tensor):\n",
        "            y_train = torch.LongTensor(y_train).to(self.device)\n",
        "        else:\n",
        "            y_train = y_train.to(self.device)\n",
        "\n",
        "\n",
        "        # Predizioni\n",
        "        with torch.no_grad():\n",
        "            # Test set\n",
        "            outputs_test = model(X_test)\n",
        "            y_pred_test = torch.argmax(outputs_test, dim=1)\n",
        "            y_proba_test = F.softmax(outputs_test, dim=1)\n",
        "\n",
        "            # Train set\n",
        "            outputs_train = model(X_train)\n",
        "            y_pred_train = torch.argmax(outputs_train, dim=1)\n",
        "\n",
        "        # Converte in NumPy per le metriche\n",
        "        y_pred_test = y_pred_test.cpu().numpy()\n",
        "        y_true_test = y_test.cpu().numpy()\n",
        "        y_proba_test = y_proba_test.cpu().numpy()\n",
        "        y_pred_train = y_pred_train.cpu().numpy()\n",
        "        y_true_train = y_train.cpu().numpy()\n",
        "\n",
        "\n",
        "        # Calcola metriche\n",
        "        accuracy = accuracy_score(y_true_test, y_pred_test)\n",
        "        f1_weighted = f1_score(y_true_test, y_pred_test, average='weighted', zero_division=0)\n",
        "        f1_macro = f1_score(y_true_test, y_pred_test, average='macro', zero_division=0)\n",
        "\n",
        "        # Metriche AUC\n",
        "        try:\n",
        "            auc_roc = roc_auc_score(y_true_test, y_proba_test, multi_class='ovr', average='weighted')\n",
        "            auc_pr = average_precision_score(pd.get_dummies(y_true_test), y_proba_test, average='weighted')\n",
        "        except ValueError:\n",
        "            auc_roc = np.nan\n",
        "            auc_pr = np.nan\n",
        "\n",
        "        return {\n",
        "            'model_name': model_name,\n",
        "            'accuracy': accuracy,\n",
        "            'f1_weighted': f1_weighted,\n",
        "            'f1_macro': f1_macro,\n",
        "            'auc_roc': auc_roc,\n",
        "            'auc_pr': auc_pr\n",
        "        }\n",
        "\n",
        "\n",
        "    def run_standalone_pruning_study(self, model, model_name, X_test, y_test, X_train, y_train,\n",
        "                                   pruning_ratios=[0.0, 0.1, 0.2, 0.3, 0.5, 0.7, 0.8, 0.9, 0.95]):\n",
        "        \"\"\"\n",
        "        Conduce lo studio di ablazione con L1 pruning su modelli standalone MLP/KAN\n",
        "        \"\"\"\n",
        "        print(f\"\\n=== Standalone L1 Pruning Study for {model_name} ===\")\n",
        "\n",
        "        # Determina il tipo di modello\n",
        "        if hasattr(model, 'width') and hasattr(model, 'act_fun'):\n",
        "            model_type = \"KAN\"\n",
        "            print(f\"NOTA: {model_name} è un modello KAN standalone\")\n",
        "        else:\n",
        "            model_type = \"MLP\"\n",
        "            print(f\"NOTA: {model_name} è un modello MLP standalone\")\n",
        "\n",
        "        # Parametri totali usando count_params\n",
        "        total_params = count_params(model)\n",
        "        print(f\"Total Parameters: {total_params:,}\")\n",
        "\n",
        "        for pruning_ratio in pruning_ratios:\n",
        "            print(f\"\\nTesting {model_type} pruning ratio: {pruning_ratio:.4f}\")\n",
        "\n",
        "            if pruning_ratio == 0.0:\n",
        "                # Modello originale\n",
        "                pruned_model = model\n",
        "                sparsity = 0.0\n",
        "                active_params = total_params\n",
        "            else:\n",
        "                # Applica L1 pruning\n",
        "                pruned_model = self.apply_l1_pruning_standalone(model, pruning_ratio)\n",
        "                pruned_model.to(self.device)\n",
        "                sparsity = self.get_model_sparsity(pruned_model)\n",
        "                active_params = self.count_active_parameters(pruned_model)\n",
        "\n",
        "            # Valuta prestazioni\n",
        "            metrics = self.evaluate_pruned_model(\n",
        "                pruned_model, model_name, X_test, y_test, X_train, y_train\n",
        "            )\n",
        "\n",
        "            # Calcola statistiche di compressione\n",
        "            compression_ratio = total_params / active_params if active_params > 0 else float('inf')\n",
        "\n",
        "            # Salva risultati\n",
        "            result = {\n",
        "                'model_name': model_name,\n",
        "                'model_type': model_type,\n",
        "                'pruning_ratio': pruning_ratio,\n",
        "                'sparsity': sparsity,\n",
        "                'total_params': total_params,\n",
        "                'active_params': active_params,\n",
        "                'compression_ratio': compression_ratio,\n",
        "                'accuracy': metrics['accuracy'],\n",
        "                'f1_weighted': metrics['f1_weighted'],\n",
        "                'f1_macro': metrics['f1_macro'],\n",
        "                'auc_roc': metrics['auc_roc'],\n",
        "                'auc_pr': metrics['auc_pr']\n",
        "            }\n",
        "\n",
        "            self.pruning_results.append(result)\n",
        "\n",
        "            print(f\"  Sparsity: {sparsity:.3f}\")\n",
        "            print(f\"  Active params: {active_params:,} / {total_params:,}\")\n",
        "            print(f\"  Compression: {compression_ratio:.2f}x\")\n",
        "            print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
        "            print(f\"  F1-Weighted: {metrics['f1_weighted']:.4f}\")\n",
        "\n",
        "\n",
        "    def plot_standalone_pruning_results(self, figsize=(16, 12)):\n",
        "        \"\"\"\n",
        "        Visualizza i risultati dello studio di pruning per modelli standalone\n",
        "        \"\"\"\n",
        "        if not self.pruning_results:\n",
        "            print(\"No pruning results to plot. Run pruning study first.\")\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(self.pruning_results)\n",
        "\n",
        "        fig, axes = plt.subplots(2, 3, figsize=figsize)\n",
        "        fig.suptitle('Standalone MLP/KAN L1 Pruning Study Results', fontsize=16, fontweight='bold')\n",
        "\n",
        "        models = df['model_name'].unique()\n",
        "        colors = sns.color_palette(\"husl\", len(models))\n",
        "\n",
        "        # Plot 1: Accuracy vs Pruning Ratio\n",
        "        ax = axes[0, 0]\n",
        "        for i, model in enumerate(models):\n",
        "            model_data = df[df['model_name'] == model]\n",
        "            model_type = model_data.iloc[0]['model_type']\n",
        "            ax.plot(model_data['pruning_ratio'], model_data['accuracy'],\n",
        "                   marker='o', label=f'{model} ({model_type})', color=colors[i], linewidth=2)\n",
        "        ax.set_xlabel('Pruning Ratio')\n",
        "        ax.set_ylabel('Test Accuracy')\n",
        "        ax.set_title('Accuracy vs Pruning Ratio')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 2: F1-Weighted vs Pruning Ratio\n",
        "        ax = axes[0, 1]\n",
        "        for i, model in enumerate(models):\n",
        "            model_data = df[df['model_name'] == model]\n",
        "            model_type = model_data.iloc[0]['model_type']\n",
        "            ax.plot(model_data['pruning_ratio'], model_data['f1_weighted'],\n",
        "                   marker='s', label=f'{model} ({model_type})', color=colors[i], linewidth=2)\n",
        "        ax.set_xlabel('Pruning Ratio')\n",
        "        ax.set_ylabel('F1-Weighted Score')\n",
        "        ax.set_title('F1-Weighted vs Pruning Ratio')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 3: Performance vs Compression Ratio\n",
        "        ax = axes[0, 2]\n",
        "        for i, model in enumerate(models):\n",
        "            model_data = df[df['model_name'] == model]\n",
        "            model_type = model_data.iloc[0]['model_type']\n",
        "            finite_mask = np.isfinite(model_data['compression_ratio'])\n",
        "            if finite_mask.any():\n",
        "                ax.scatter(model_data.loc[finite_mask, 'compression_ratio'],\n",
        "                          model_data.loc[finite_mask, 'f1_weighted'],\n",
        "                          label=f'{model} ({model_type})', color=colors[i], s=50, alpha=0.7)\n",
        "        ax.set_xlabel('Compression Ratio (x)')\n",
        "        ax.set_ylabel('F1-Weighted Score')\n",
        "        ax.set_title('Performance vs Compression Trade-off')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.set_xscale('log')\n",
        "\n",
        "        # Plot 4: Sparsity vs Performance\n",
        "        ax = axes[1, 0]\n",
        "        for i, model in enumerate(models):\n",
        "            model_data = df[df['model_name'] == model]\n",
        "            model_type = model_data.iloc[0]['model_type']\n",
        "            ax.plot(model_data['sparsity'], model_data['accuracy'],\n",
        "                   marker='d', label=f'{model} ({model_type})', color=colors[i], linewidth=2)\n",
        "        ax.set_xlabel('Model Sparsity')\n",
        "        ax.set_ylabel('Test Accuracy')\n",
        "        ax.set_title('Accuracy vs Model Sparsity')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 5: Active Parameters vs Performance\n",
        "        ax = axes[1, 1]\n",
        "        for i, model in enumerate(models):\n",
        "            model_data = df[df['model_name'] == model]\n",
        "            model_type = model_data.iloc[0]['model_type']\n",
        "            ax.semilogx(model_data['active_params'], model_data['f1_weighted'],\n",
        "                       marker='^', label=f'{model} ({model_type})', color=colors[i], linewidth=2)\n",
        "        ax.set_xlabel('Active Parameters (log scale)')\n",
        "        ax.set_ylabel('F1-Weighted Score')\n",
        "        ax.set_title('Performance vs Active Parameters')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 6: Performance Retention\n",
        "        ax = axes[1, 2]\n",
        "        for i, model in enumerate(models):\n",
        "            model_data = df[df['model_name'] == model].sort_values('pruning_ratio')\n",
        "            model_type = model_data.iloc[0]['model_type']\n",
        "            baseline_f1 = model_data.iloc[0]['f1_weighted']\n",
        "            performance_retention = model_data['f1_weighted'] / baseline_f1\n",
        "            ax.plot(model_data['pruning_ratio'], performance_retention,\n",
        "                   marker='s', label=f'{model} ({model_type})', color=colors[i], linewidth=2)\n",
        "        ax.set_xlabel('Pruning Ratio')\n",
        "        ax.set_ylabel('Performance Retention')\n",
        "        ax.set_title('Performance Retention vs Pruning')\n",
        "        ax.axhline(y=0.95, color='red', linestyle='--', alpha=0.7, label='95% threshold')\n",
        "        ax.axhline(y=0.90, color='orange', linestyle='--', alpha=0.7, label='90% threshold')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def generate_standalone_pruning_report(self):\n",
        "        \"\"\"\n",
        "        Genera un report dettagliato per i risultati del pruning standalone\n",
        "        \"\"\"\n",
        "        if not self.pruning_results:\n",
        "            print(\"No pruning results available. Run pruning study first.\")\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(self.pruning_results)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"STANDALONE MLP/KAN L1 PRUNING STUDY - DETAILED REPORT\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        for model_name in df['model_name'].unique():\n",
        "            model_data = df[df['model_name'] == model_name].sort_values('pruning_ratio')\n",
        "\n",
        "            print(f\"\\n{model_name} Results:\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "            # Informazioni base\n",
        "            baseline_row = model_data.iloc[0]\n",
        "            baseline_f1 = baseline_row['f1_weighted']\n",
        "            total_params = baseline_row['total_params']\n",
        "            model_type = baseline_row['model_type']\n",
        "\n",
        "            print(f\"Model Type: {model_type}\")\n",
        "            print(f\"Total Parameters: {total_params:,}\")\n",
        "            print(f\"Baseline F1-Weighted: {baseline_f1:.4f}\")\n",
        "\n",
        "            # Trova il punto di degradazione significativa (>5% loss in F1)\n",
        "            degradation_point = None\n",
        "            for _, row in model_data.iterrows():\n",
        "                f1_loss = (baseline_f1 - row['f1_weighted']) / baseline_f1\n",
        "                if f1_loss > 0.05:  # 5% degradation threshold\n",
        "                    degradation_point = row['pruning_ratio']\n",
        "                    break\n",
        "\n",
        "            if degradation_point:\n",
        "                print(f\"Significant degradation starts at: {degradation_point:.1%} pruning\")\n",
        "            else:\n",
        "                print(\"No significant degradation observed within tested range\")\n",
        "\n",
        "            # Migliore trade-off (massima compressione con <2% loss)\n",
        "            best_tradeoff = None\n",
        "            for _, row in model_data.iterrows():\n",
        "                f1_loss = (baseline_f1 - row['f1_weighted']) / baseline_f1\n",
        "                if f1_loss <= 0.02 and row['pruning_ratio'] > 0:\n",
        "                    best_tradeoff = row\n",
        "\n",
        "            if best_tradeoff is not None:\n",
        "                print(f\"\\nBest trade-off point:\")\n",
        "                print(f\"  Pruning ratio: {best_tradeoff['pruning_ratio']:.1%}\")\n",
        "                print(f\"  Compression: {best_tradeoff['compression_ratio']:.1f}x\")\n",
        "                print(f\"  F1_Weighted: {best_tradeoff['f1_weighted']:.4f}\")\n",
        "                print(f\"  Performance loss: {((baseline_f1 - best_tradeoff['f1_weighted'])/baseline_f1)*100:.1f}%\")\n",
        "\n",
        "            # Statistiche di compressione\n",
        "            max_compression = model_data['compression_ratio'].replace([np.inf, -np.inf], np.nan).max()\n",
        "            if not np.isnan(max_compression):\n",
        "                print(f\"\\nMaximum compression achieved: {max_compression:.1f}x\")\n",
        "\n",
        "        # Tabella comparativa\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(\"COMPARATIVE SUMMARY TABLE - STANDALONE MODELS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        summary_rows = []\n",
        "        for model_name in df['model_name'].unique():\n",
        "            model_data = df[df['model_name'] == model_name]\n",
        "            baseline = model_data[model_data['pruning_ratio'] == 0.0].iloc[0]\n",
        "            model_type = baseline['model_type']\n",
        "\n",
        "            # Trova risultati a diverse soglie di pruning\n",
        "            for target_ratio in [0.3, 0.5, 0.7, 0.9]:\n",
        "                closest = model_data.iloc[(model_data['pruning_ratio'] - target_ratio).abs().argsort()].iloc[0]\n",
        "                if abs(closest['pruning_ratio'] - target_ratio) < 0.1:  # Se abbastanza vicino\n",
        "                    performance_loss = ((baseline['f1_weighted'] - closest['f1_weighted']) / baseline['f1_weighted']) * 100\n",
        "                    summary_rows.append({\n",
        "                        'Model': model_name,\n",
        "                        'Type': model_type,\n",
        "                        'Pruning_Ratio': f\"{target_ratio:.0%}\",\n",
        "                        'Compression': f\"{closest['compression_ratio']:.1f}x\",\n",
        "                        'F1_Score': f\"{closest['f1_weighted']:.4f}\",\n",
        "                        'Perf_Loss': f\"{performance_loss:.1f}%\"\n",
        "                    })\n",
        "\n",
        "        if summary_rows:\n",
        "            summary_df = pd.DataFrame(summary_rows)\n",
        "            print(summary_df.to_string(index=False))\n",
        "\n",
        "print(\"=== STANDALONE MLP/KAN PRUNING STUDY ===\")\n",
        "\n",
        "standalone_pruning_study = StandalonePruningAblationStudy(device=device)\n",
        "\n",
        "standalone_pruning_study.run_standalone_pruning_study(\n",
        "    model=best_model_mlp,\n",
        "    model_name='MLP',\n",
        "    X_test=X_test_tensor,\n",
        "    y_test=y_test_tensor,\n",
        "    X_train=X_tensor,\n",
        "    y_train=y_tensor,\n",
        "    pruning_ratios=[0.0, 0.1, 0.2, 0.3, 0.5, 0.7, 0.8, 0.9, 0.95]\n",
        ")\n",
        "\n",
        "standalone_pruning_study.run_standalone_pruning_study(\n",
        "    model=best_model_kan,\n",
        "    model_name='KAN',\n",
        "    X_test=X_test_tensor,\n",
        "    y_test=y_test_tensor,\n",
        "    X_train=X_tensor,\n",
        "    y_train=y_tensor,\n",
        "    pruning_ratios=[0.0, 0.1, 0.2, 0.3, 0.5, 0.7, 0.8, 0.9, 0.95]\n",
        ")\n",
        "\n",
        "standalone_pruning_study.plot_standalone_pruning_results()\n",
        "standalone_pruning_study.generate_standalone_pruning_report()\n",
        "\n",
        "results_df = pd.DataFrame(standalone_pruning_study.pruning_results)\n",
        "print(f\"\\nResults saved to DataFrame with {len(results_df)} rows\")\n",
        "print(\"\\nPruning results:\")\n",
        "display(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVIP_87as9BR"
      },
      "source": [
        "# Ablation Study RandomForest e XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YphX7RKUtAK_"
      },
      "outputs": [],
      "source": [
        "# Ablation Study Ensemble Pruning: Random Forest e XGBoost\n",
        "\n",
        "class EnsemblePruningAblationStudy:\n",
        "    def __init__(self):\n",
        "        self.pruning_results = []\n",
        "\n",
        "    def rank_based_pruning_rf(self, rf_model, pruning_ratio):\n",
        "        \"\"\"\n",
        "        Implementa Rank-Based Pruning per Random Forest.\n",
        "        Rimuove gli alberi con le feature importance più basse.\n",
        "        \"\"\"\n",
        "        if pruning_ratio == 0.0:\n",
        "            return rf_model, list(range(len(rf_model.estimators_)))\n",
        "\n",
        "        # Calcola l'importanza di ogni albero basata sulla media delle feature importance\n",
        "        tree_importances = []\n",
        "        for i, tree in enumerate(rf_model.estimators_):\n",
        "            # L'importanza dell'albero è la somma delle importanze delle sue feature\n",
        "            tree_importance = np.sum(tree.feature_importances_)\n",
        "            tree_importances.append((i, tree_importance))\n",
        "\n",
        "        # Ordina gli alberi per importanza (decrescente)\n",
        "        tree_importances.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Calcola quanti alberi mantenere\n",
        "        n_trees_to_keep = max(1, int(len(rf_model.estimators_) * (1 - pruning_ratio)))\n",
        "\n",
        "        # Seleziona gli indici degli alberi da mantenere\n",
        "        selected_indices = [idx for idx, _ in tree_importances[:n_trees_to_keep]]\n",
        "        selected_indices.sort()  # Mantieni l'ordine originale\n",
        "\n",
        "        # Crea un nuovo modello con solo gli alberi selezionati\n",
        "        pruned_rf = copy.deepcopy(rf_model)\n",
        "        pruned_rf.estimators_ = [rf_model.estimators_[i] for i in selected_indices]\n",
        "        pruned_rf.n_estimators = len(selected_indices)\n",
        "\n",
        "        return pruned_rf, selected_indices\n",
        "\n",
        "    def cumulative_pruning_xgb(self, xgb_model, pruning_ratio):\n",
        "        \"\"\"\n",
        "        Implementa Cumulative Pruning per XGBoost.\n",
        "        Mantiene solo le prime iterazioni di boosting in base al pruning_ratio.\n",
        "        \"\"\"\n",
        "        if pruning_ratio == 0.0:\n",
        "            return xgb_model, list(range(xgb_model.n_estimators))\n",
        "\n",
        "        # Ottieni il booster\n",
        "        original_booster = xgb_model.get_booster()\n",
        "\n",
        "        # Calcola il numero di alberi da mantenere\n",
        "        total_trees = xgb_model.n_estimators\n",
        "        num_classes = xgb_model.n_classes_\n",
        "        total_rounds = total_trees // num_classes\n",
        "        keep_rounds = max(1, int(total_rounds * (1 - pruning_ratio)))\n",
        "        n_keep = keep_rounds * num_classes\n",
        "\n",
        "        # Crea una copia del modello originale\n",
        "        pruned_model = copy.deepcopy(xgb_model)\n",
        "        pruned_model.n_estimators = n_keep\n",
        "\n",
        "        def predict_pruned(self, X):\n",
        "            dmat = xgb.DMatrix(X)\n",
        "            raw_predictions = self.get_booster().predict(dmat, iteration_range=(0, n_keep))\n",
        "            predicted_labels = np.argmax(raw_predictions, axis=1)\n",
        "            return predicted_labels\n",
        "\n",
        "        def predict_proba(self, X):\n",
        "            dmat = xgb.DMatrix(X)\n",
        "            raw_predictions = self.get_booster().predict(dmat, iteration_range=(0, n_keep))\n",
        "            proba = np.exp(raw_predictions) / np.sum(np.exp(raw_predictions), axis=1, keepdims=True)\n",
        "            return proba\n",
        "\n",
        "        pruned_model.predict = types.MethodType(predict_pruned, pruned_model)\n",
        "        pruned_model.predict_proba = types.MethodType(predict_proba, pruned_model)\n",
        "\n",
        "        selected_indices = list(range(n_keep))\n",
        "        return pruned_model, selected_indices\n",
        "\n",
        "    def evaluate_pruned_ensemble(self, model, model_name, X_test, y_test, X_train, y_train):\n",
        "        \"\"\"Valuta le prestazioni di un modello ensemble pruned\"\"\"\n",
        "\n",
        "        # Previsioni\n",
        "        y_pred_test = model.predict(X_test)\n",
        "        y_pred_train = model.predict(X_train)\n",
        "\n",
        "        # Calcola metriche\n",
        "        accuracy = accuracy_score(y_test, y_pred_test)\n",
        "        f1_weighted = f1_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
        "        f1_macro = f1_score(y_test, y_pred_test, average='macro', zero_division=0)\n",
        "\n",
        "        # Metriche AUC (se il modello supporta predict_proba)\n",
        "        try:\n",
        "            if hasattr(model, 'predict_proba'):\n",
        "                y_proba_test = model.predict_proba(X_test)\n",
        "                auc_roc = roc_auc_score(y_test, y_proba_test, multi_class='ovr', average='weighted')\n",
        "                auc_pr = average_precision_score(pd.get_dummies(y_test), y_proba_test, average='weighted')\n",
        "            else:\n",
        "                auc_roc = np.nan\n",
        "                auc_pr = np.nan\n",
        "        except (ValueError, AttributeError):\n",
        "            auc_roc = np.nan\n",
        "            auc_pr = np.nan\n",
        "\n",
        "        return {\n",
        "            'model_name': model_name,\n",
        "            'accuracy': accuracy,\n",
        "            'f1_weighted': f1_weighted,\n",
        "            'f1_macro': f1_macro,\n",
        "            'auc_roc': auc_roc,\n",
        "            'auc_pr': auc_pr\n",
        "        }\n",
        "\n",
        "    def run_rf_pruning_study(self, rf_model, model_name, X_test, y_test, X_train, y_train,\n",
        "                           pruning_ratios=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]):\n",
        "        \"\"\"\n",
        "        Conduce lo studio di ablazione con Rank-Based Pruning su Random Forest\n",
        "        \"\"\"\n",
        "        print(f\"\\n=== Rank-Based Pruning Study for {model_name} ===\")\n",
        "\n",
        "        # Parametri totali (numero di alberi)\n",
        "        total_trees = len(rf_model.estimators_)\n",
        "        print(f\"Total Trees: {total_trees:,}\")\n",
        "\n",
        "        for pruning_ratio in pruning_ratios:\n",
        "            print(f\"\\nTesting RF pruning ratio: {pruning_ratio:.2f}\")\n",
        "\n",
        "            # Applica rank-based pruning\n",
        "            pruned_model, selected_indices = self.rank_based_pruning_rf(rf_model, pruning_ratio)\n",
        "\n",
        "            # Calcola statistiche\n",
        "            remaining_trees = len(selected_indices)\n",
        "            compression_ratio = total_trees / remaining_trees if remaining_trees > 0 else float('inf')\n",
        "\n",
        "            # Valuta prestazioni\n",
        "            metrics = self.evaluate_pruned_ensemble(\n",
        "                pruned_model, model_name, X_test, y_test, X_train, y_train\n",
        "            )\n",
        "\n",
        "            # Salva risultati\n",
        "            result = {\n",
        "                'model_name': model_name,\n",
        "                'model_type': 'Random Forest',\n",
        "                'pruning_method': 'Rank-Based',\n",
        "                'pruning_ratio': pruning_ratio,\n",
        "                'total_trees': total_trees,\n",
        "                'remaining_trees': remaining_trees,\n",
        "                'compression_ratio': compression_ratio,\n",
        "                'accuracy': metrics['accuracy'],\n",
        "                'f1_weighted': metrics['f1_weighted'],\n",
        "                'f1_macro': metrics['f1_macro'],\n",
        "                'auc_roc': metrics['auc_roc'],\n",
        "                'auc_pr': metrics['auc_pr']\n",
        "            }\n",
        "\n",
        "            self.pruning_results.append(result)\n",
        "\n",
        "            print(f\"  Remaining trees: {remaining_trees:,} / {total_trees:,}\")\n",
        "            print(f\"  Compression: {compression_ratio:.2f}x\")\n",
        "            print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
        "            print(f\"  F1-Weighted: {metrics['f1_weighted']:.4f}\")\n",
        "\n",
        "    def run_xgb_pruning_study(self, xgb_model, model_name, X_test, y_test, X_train, y_train,\n",
        "                            pruning_ratios=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]):\n",
        "        \"\"\"\n",
        "        Conduce lo studio di ablazione con Cumulative Pruning su XGBoost\n",
        "        \"\"\"\n",
        "        print(f\"\\n=== Cumulative Pruning Study for {model_name} ===\")\n",
        "\n",
        "        # Parametri totali (numero di alberi)\n",
        "        total_trees = xgb_model.n_estimators\n",
        "        print(f\"Total Trees: {total_trees:,}\")\n",
        "        print(f\"Number of Classes: {xgb_model.n_classes_}\")\n",
        "\n",
        "        for pruning_ratio in pruning_ratios:\n",
        "            print(f\"\\nTesting XGB pruning ratio: {pruning_ratio:.2f}\")\n",
        "\n",
        "            try:\n",
        "                # Applica cumulative pruning\n",
        "                pruned_model, selected_indices = self.cumulative_pruning_xgb(xgb_model, pruning_ratio)\n",
        "\n",
        "                # Calcola statistiche\n",
        "                remaining_trees = len(selected_indices)\n",
        "                compression_ratio = total_trees / remaining_trees if remaining_trees > 0 else float('inf')\n",
        "\n",
        "                # Valuta prestazioni\n",
        "                metrics = self.evaluate_pruned_ensemble(\n",
        "                    pruned_model, model_name, X_test, y_test, X_train, y_train\n",
        "                )\n",
        "\n",
        "                # Salva risultati\n",
        "                result = {\n",
        "                    'model_name': model_name,\n",
        "                    'model_type': 'XGBoost',\n",
        "                    'pruning_method': 'Cumulative',\n",
        "                    'pruning_ratio': pruning_ratio,\n",
        "                    'total_trees': total_trees,\n",
        "                    'remaining_trees': remaining_trees,\n",
        "                    'compression_ratio': compression_ratio,\n",
        "                    'accuracy': metrics['accuracy'],\n",
        "                    'f1_weighted': metrics['f1_weighted'],\n",
        "                    'f1_macro': metrics['f1_macro'],\n",
        "                    'auc_roc': metrics['auc_roc'],\n",
        "                    'auc_pr': metrics['auc_pr']\n",
        "                }\n",
        "\n",
        "                self.pruning_results.append(result)\n",
        "\n",
        "                print(f\"  Remaining trees: {remaining_trees:,} / {total_trees:,}\")\n",
        "                print(f\"  Compression: {compression_ratio:.2f}x\")\n",
        "                print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
        "                print(f\"  F1-Weighted: {metrics['f1_weighted']:.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Error during XGB pruning at ratio {pruning_ratio}: {e}\")\n",
        "                continue\n",
        "\n",
        "    def plot_ensemble_pruning_results(self, figsize=(18, 12)):\n",
        "        \"\"\"\n",
        "        Visualizza i risultati dello studio di pruning per modelli ensemble\n",
        "        \"\"\"\n",
        "        if not self.pruning_results:\n",
        "            print(\"No pruning results to plot. Run pruning studies first.\")\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(self.pruning_results)\n",
        "\n",
        "        fig, axes = plt.subplots(3, 3, figsize=figsize)\n",
        "        fig.suptitle('Ensemble Pruning Study Results - Random Forest & XGBoost',\n",
        "                     fontsize=16, fontweight='bold')\n",
        "\n",
        "        models = df['model_name'].unique()\n",
        "        colors = sns.color_palette(\"Set1\", len(models))\n",
        "\n",
        "        # Plot 1: Accuracy vs Pruning Ratio\n",
        "        ax = axes[0, 0]\n",
        "        for i, model in enumerate(models):\n",
        "            model_data = df[df['model_name'] == model]\n",
        "            method = model_data.iloc[0]['pruning_method']\n",
        "            ax.plot(model_data['pruning_ratio'], model_data['accuracy'],\n",
        "                   marker='o', label=f'{model} ({method})', color=colors[i], linewidth=2)\n",
        "        ax.set_xlabel('Pruning Ratio')\n",
        "        ax.set_ylabel('Test Accuracy')\n",
        "        ax.set_title('Accuracy vs Pruning Ratio')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 2: F1-Weighted vs Pruning Ratio\n",
        "        ax = axes[0, 1]\n",
        "        for i, model in enumerate(models):\n",
        "            model_data = df[df['model_name'] == model]\n",
        "            method = model_data.iloc[0]['pruning_method']\n",
        "            ax.plot(model_data['pruning_ratio'], model_data['f1_weighted'],\n",
        "                   marker='s', label=f'{model} ({method})', color=colors[i], linewidth=2)\n",
        "        ax.set_xlabel('Pruning Ratio')\n",
        "        ax.set_ylabel('F1-Weighted Score')\n",
        "        ax.set_title('F1-Weighted vs Pruning Ratio')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 3: F1-Macro vs Pruning Ratio\n",
        "        ax = axes[0, 2]\n",
        "        for i, model in enumerate(models):\n",
        "            model_data = df[df['model_name'] == model]\n",
        "            method = model_data.iloc[0]['pruning_method']\n",
        "            ax.plot(model_data['pruning_ratio'], model_data['f1_macro'],\n",
        "                   marker='^', label=f'{model} ({method})', color=colors[i], linewidth=2)\n",
        "        ax.set_xlabel('Pruning Ratio')\n",
        "        ax.set_ylabel('F1-Macro Score')\n",
        "        ax.set_title('F1-Macro vs Pruning Ratio')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 4: Performance vs Compression Ratio\n",
        "        ax = axes[1, 0]\n",
        "        for i, model in enumerate(models):\n",
        "            model_data = df[df['model_name'] == model]\n",
        "            method = model_data.iloc[0]['pruning_method']\n",
        "            finite_mask = np.isfinite(model_data['compression_ratio'])\n",
        "            if finite_mask.any():\n",
        "                ax.scatter(model_data.loc[finite_mask, 'compression_ratio'],\n",
        "                          model_data.loc[finite_mask, 'f1_weighted'],\n",
        "                          label=f'{model} ({method})', color=colors[i], s=60, alpha=0.7)\n",
        "        ax.set_xlabel('Compression Ratio (x)')\n",
        "        ax.set_ylabel('F1-Weighted Score')\n",
        "        ax.set_title('Performance vs Compression Trade-off')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.set_xscale('log')\n",
        "\n",
        "        # Plot 5: Remaining Trees vs Performance\n",
        "        ax = axes[1, 1]\n",
        "        for i, model in enumerate(models):\n",
        "            model_data = df[df['model_name'] == model]\n",
        "            method = model_data.iloc[0]['pruning_method']\n",
        "            ax.semilogx(model_data['remaining_trees'], model_data['accuracy'],\n",
        "                       marker='d', label=f'{model} ({method})', color=colors[i], linewidth=2)\n",
        "        ax.set_xlabel('Remaining Trees (log scale)')\n",
        "        ax.set_ylabel('Test Accuracy')\n",
        "        ax.set_title('Accuracy vs Remaining Trees')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 6: AUC-ROC vs Pruning Ratio\n",
        "        ax = axes[1, 2]\n",
        "        for i, model in enumerate(models):\n",
        "            model_data = df[df['model_name'] == model]\n",
        "            method = model_data.iloc[0]['pruning_method']\n",
        "            # Filtra i valori NaN per AUC\n",
        "            valid_mask = ~np.isnan(model_data['auc_roc'])\n",
        "            if valid_mask.any():\n",
        "                valid_data = model_data[valid_mask]\n",
        "                ax.plot(valid_data['pruning_ratio'], valid_data['auc_roc'],\n",
        "                       marker='*', label=f'{model} ({method})', color=colors[i], linewidth=2)\n",
        "        ax.set_xlabel('Pruning Ratio')\n",
        "        ax.set_ylabel('AUC-ROC Score')\n",
        "        ax.set_title('AUC-ROC vs Pruning Ratio')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 7: Performance Retention\n",
        "        ax = axes[2, 0]\n",
        "        for i, model in enumerate(models):\n",
        "            model_data = df[df['model_name'] == model].sort_values('pruning_ratio')\n",
        "            method = model_data.iloc[0]['pruning_method']\n",
        "            baseline_f1 = model_data.iloc[0]['f1_weighted']\n",
        "            performance_retention = model_data['f1_weighted'] / baseline_f1\n",
        "            ax.plot(model_data['pruning_ratio'], performance_retention,\n",
        "                   marker='s', label=f'{model} ({method})', color=colors[i], linewidth=2)\n",
        "        ax.set_xlabel('Pruning Ratio')\n",
        "        ax.set_ylabel('Performance Retention')\n",
        "        ax.set_title('Performance Retention vs Pruning')\n",
        "        ax.axhline(y=0.95, color='red', linestyle='--', alpha=0.7, label='95% threshold')\n",
        "        ax.axhline(y=0.90, color='orange', linestyle='--', alpha=0.7, label='90% threshold')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 8: Trees Reduction vs Performance Loss\n",
        "        ax = axes[2, 1]\n",
        "        for i, model in enumerate(models):\n",
        "            model_data = df[df['model_name'] == model].sort_values('pruning_ratio')\n",
        "            method = model_data.iloc[0]['pruning_method']\n",
        "            baseline_f1 = model_data.iloc[0]['f1_weighted']\n",
        "            performance_loss = (baseline_f1 - model_data['f1_weighted']) / baseline_f1 * 100\n",
        "            trees_reduction = (1 - model_data['remaining_trees'] / model_data['total_trees']) * 100\n",
        "            ax.plot(trees_reduction, performance_loss,\n",
        "                   marker='o', label=f'{model} ({method})', color=colors[i], linewidth=2)\n",
        "        ax.set_xlabel('Trees Reduction (%)')\n",
        "        ax.set_ylabel('Performance Loss (%)')\n",
        "        ax.set_title('Trees Reduction vs Performance Loss')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 9: Efficiency Comparison\n",
        "        ax = axes[2, 2]\n",
        "        for i, model in enumerate(models):\n",
        "            model_data = df[df['model_name'] == model]\n",
        "            method = model_data.iloc[0]['pruning_method']\n",
        "            baseline_trees = model_data.iloc[0]['total_trees']\n",
        "            baseline_f1 = model_data.iloc[0]['f1_weighted']\n",
        "\n",
        "            # Calcola l'efficienza come F1-score per albero\n",
        "            efficiency = model_data['f1_weighted'] / (model_data['remaining_trees'] / baseline_trees)\n",
        "            ax.plot(model_data['pruning_ratio'], efficiency,\n",
        "                   marker='x', label=f'{model} ({method})', color=colors[i], linewidth=2)\n",
        "        ax.set_xlabel('Pruning Ratio')\n",
        "        ax.set_ylabel('Efficiency (F1/Tree Fraction)')\n",
        "        ax.set_title('Model Efficiency vs Pruning')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def generate_ensemble_pruning_report(self):\n",
        "        \"\"\"\n",
        "        Genera un report dettagliato per i risultati del pruning ensemble\n",
        "        \"\"\"\n",
        "        if not self.pruning_results:\n",
        "            print(\"No pruning results available. Run pruning studies first.\")\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(self.pruning_results)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ENSEMBLE PRUNING STUDY - DETAILED REPORT\")\n",
        "        print(\"Random Forest: Rank-Based Pruning | XGBoost: Cumulative Pruning\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        for model_name in df['model_name'].unique():\n",
        "            model_data = df[df['model_name'] == model_name].sort_values('pruning_ratio')\n",
        "\n",
        "            print(f\"\\n{model_name} Results:\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "            # Informazioni base\n",
        "            baseline_row = model_data.iloc[0]\n",
        "            baseline_f1 = baseline_row['f1_weighted']\n",
        "            total_trees = baseline_row['total_trees']\n",
        "            model_type = baseline_row['model_type']\n",
        "            pruning_method = baseline_row['pruning_method']\n",
        "\n",
        "            print(f\"Model Type: {model_type}\")\n",
        "            print(f\"Pruning Method: {pruning_method}\")\n",
        "            print(f\"Total Trees: {total_trees:,}\")\n",
        "            print(f\"Baseline F1-Weighted: {baseline_f1:.4f}\")\n",
        "\n",
        "            # Trova il punto di degradazione significativa (>5% loss in F1)\n",
        "            degradation_point = None\n",
        "            for _, row in model_data.iterrows():\n",
        "                f1_loss = (baseline_f1 - row['f1_weighted']) / baseline_f1\n",
        "                if f1_loss > 0.05:  # 5% degradation threshold\n",
        "                    degradation_point = row['pruning_ratio']\n",
        "                    break\n",
        "\n",
        "            if degradation_point:\n",
        "                print(f\"Significant degradation starts at: {degradation_point:.1%} pruning\")\n",
        "            else:\n",
        "                print(\"No significant degradation observed within tested range\")\n",
        "\n",
        "            # Migliore trade-off (massima compressione con <2% loss)\n",
        "            best_tradeoff = None\n",
        "            for _, row in model_data.iterrows():\n",
        "                f1_loss = (baseline_f1 - row['f1_weighted']) / baseline_f1\n",
        "                if f1_loss <= 0.02 and row['pruning_ratio'] > 0:\n",
        "                    best_tradeoff = row\n",
        "\n",
        "            if best_tradeoff is not None:\n",
        "                print(f\"\\nBest trade-off point:\")\n",
        "                print(f\"  Pruning ratio: {best_tradeoff['pruning_ratio']:.1%}\")\n",
        "                print(f\"  Trees: {best_tradeoff['remaining_trees']:,} / {total_trees:,}\")\n",
        "                print(f\"  Compression: {best_tradeoff['compression_ratio']:.1f}x\")\n",
        "                print(f\"  F1_Weighted: {best_tradeoff['f1_weighted']:.4f}\")\n",
        "                print(f\"  Performance loss: {((baseline_f1 - best_tradeoff['f1_weighted'])/baseline_f1)*100:.1f}%\")\n",
        "\n",
        "            # Statistiche di compressione\n",
        "            max_compression = model_data['compression_ratio'].replace([np.inf, -np.inf], np.nan).max()\n",
        "            if not np.isnan(max_compression):\n",
        "                print(f\"\\nMaximum compression achieved: {max_compression:.1f}x\")\n",
        "\n",
        "        # Tabella comparativa\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(\"COMPARATIVE SUMMARY TABLE - ENSEMBLE MODELS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        summary_rows = []\n",
        "        for model_name in df['model_name'].unique():\n",
        "            model_data = df[df['model_name'] == model_name]\n",
        "            baseline = model_data[model_data['pruning_ratio'] == 0.0].iloc[0]\n",
        "\n",
        "            # Trova risultati a diverse soglie di pruning\n",
        "            for target_ratio in [0.3, 0.5, 0.7, 0.9]:\n",
        "                closest = model_data.iloc[(model_data['pruning_ratio'] - target_ratio).abs().argsort()].iloc[0]\n",
        "                if abs(closest['pruning_ratio'] - target_ratio) < 0.1:  # Se abbastanza vicino\n",
        "                    performance_loss = ((baseline['f1_weighted'] - closest['f1_weighted']) / baseline['f1_weighted']) * 100\n",
        "                    trees_reduction = ((baseline['total_trees'] - closest['remaining_trees']) / baseline['total_trees']) * 100\n",
        "                    summary_rows.append({\n",
        "                        'Model': model_name,\n",
        "                        'Method': baseline['pruning_method'],\n",
        "                        'Pruning_Ratio': f\"{target_ratio:.0%}\",\n",
        "                        'Trees_Kept': f\"{closest['remaining_trees']:,}\",\n",
        "                        'Trees_Reduction': f\"{trees_reduction:.0f}%\",\n",
        "                        'Compression': f\"{closest['compression_ratio']:.1f}x\",\n",
        "                        'F1_Score': f\"{closest['f1_weighted']:.4f}\",\n",
        "                        'Perf_Loss': f\"{performance_loss:.1f}%\"\n",
        "                    })\n",
        "\n",
        "        if summary_rows:\n",
        "            summary_df = pd.DataFrame(summary_rows)\n",
        "            print(summary_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "px_mxD7BB-1v"
      },
      "outputs": [],
      "source": [
        "print(\"ENSEMBLE PRUNING ABLATION STUDY\")\n",
        "print(\"Random Forest: Rank-Based Pruning | XGBoost: Cumulative Pruning\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "ensemble_pruning_study = EnsemblePruningAblationStudy()\n",
        "\n",
        "pruning_ratios_ensemble = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
        "\n",
        "try:\n",
        "    if 'tree' in gs_rf.best_estimator_.named_steps:\n",
        "        rf_final_model = gs_rf.best_estimator_.named_steps['tree']\n",
        "    else:\n",
        "        raise ValueError(\"Random Forest model not found\")\n",
        "\n",
        "    print(\"Random Forest model extracted successfully\")\n",
        "\n",
        "    ensemble_pruning_study.run_rf_pruning_study(\n",
        "        rf_model=rf_final_model,\n",
        "        model_name='Random Forest',\n",
        "        X_test=X_test_processed,\n",
        "        y_test=y_test_0_indexed,\n",
        "        X_train=X_train_processed,\n",
        "        y_train=y_train_0_indexed,\n",
        "        pruning_ratios=pruning_ratios_ensemble\n",
        "    )\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error with Random Forest model: {e}\")\n",
        "\n",
        "try:\n",
        "    if 'xgb' in gs_xgb.best_estimator_.named_steps:\n",
        "        xgb_final_model = gs_xgb.best_estimator_.named_steps['xgb']\n",
        "    else:\n",
        "        raise ValueError(\"XGBoost model not found\")\n",
        "\n",
        "    print(\"XGBoost model extracted successfully\")\n",
        "\n",
        "    ensemble_pruning_study.run_xgb_pruning_study(\n",
        "        xgb_model=xgb_final_model,\n",
        "        model_name='XGBoost',\n",
        "        X_test=X_test_processed,\n",
        "        y_test=y_test_0_indexed,\n",
        "        X_train=X_train_processed,\n",
        "        y_train=y_train_0_indexed,\n",
        "        pruning_ratios=pruning_ratios_ensemble\n",
        "    )\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error with XGBoost model: {e}\")\n",
        "\n",
        "ensemble_pruning_study.plot_ensemble_pruning_results()\n",
        "ensemble_pruning_study.generate_ensemble_pruning_report()\n",
        "\n",
        "ensemble_results_df = pd.DataFrame(ensemble_pruning_study.pruning_results)\n",
        "print(f\"\\nResults saved to DataFrame with {len(ensemble_results_df)} rows\")\n",
        "print(\"\\nPruning results:\")\n",
        "display(ensemble_results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTaC-bLMC8tW"
      },
      "source": [
        "# Ablation Study Comparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFZ5xeHQC-Wx"
      },
      "outputs": [],
      "source": [
        "def compare_all_pruning_methods():\n",
        "    \"\"\"\n",
        "    Compare pruning effectiveness across all model types\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*120)\n",
        "    print(\"COMPREHENSIVE PRUNING COMPARISON: NEURAL NETWORKS vs ENSEMBLE METHODS\")\n",
        "    print(\"=\"*120)\n",
        "\n",
        "    # Collect data from both studies\n",
        "    all_models_comparison = []\n",
        "\n",
        "    # Neural network results\n",
        "    for _, result in results_df.iterrows():\n",
        "        if result['pruning_ratio'] in [0.0, 0.3, 0.5, 0.7, 0.9]:\n",
        "            all_models_comparison.append({\n",
        "                'Model': result['model_name'],\n",
        "                'Type': 'Neural Network',\n",
        "                'Pruning_Method': 'L1 Norm',\n",
        "                'Pruning_Ratio': result['pruning_ratio'],\n",
        "                'Accuracy_Test': result['accuracy'],\n",
        "                'F1_Weighted_Test': result['f1_weighted'],\n",
        "                'F1_Macro_Test': result['f1_macro'],\n",
        "                'AUC_ROC_Test': result['auc_roc'],\n",
        "                'AUC_PR_Test': result['auc_pr'],\n",
        "                'Compression': result['compression_ratio'],\n",
        "                'Components': f\"{result['active_params']}/{result['total_params']}\"\n",
        "            })\n",
        "\n",
        "    # Ensemble results\n",
        "    for _, result in ensemble_results_df.iterrows():\n",
        "        if result['pruning_ratio'] in [0.0, 0.3, 0.5, 0.7, 0.9]:\n",
        "            pruning_method = 'Rank-Based' if result['model_name'] == 'Random Forest' else 'Cumulative'\n",
        "            all_models_comparison.append({\n",
        "                'Model': result['model_name'],\n",
        "                'Type': 'Ensemble',\n",
        "                'Pruning_Method': pruning_method,\n",
        "                'Pruning_Ratio': result['pruning_ratio'],\n",
        "                'Accuracy_Test': result['accuracy'],\n",
        "                'F1_Weighted_Test': result['f1_weighted'],\n",
        "                'F1_Macro_Test': result['f1_macro'],\n",
        "                'AUC_ROC_Test': result['auc_roc'],\n",
        "                'AUC_PR_Test': result['auc_pr'],\n",
        "                'Compression': result['compression_ratio'],\n",
        "                'Trees': f\"{result['remaining_trees']}/{result['total_trees']}\"\n",
        "            })\n",
        "\n",
        "    if all_models_comparison:\n",
        "        comparison_df = pd.DataFrame(all_models_comparison)\n",
        "\n",
        "        # Create pivot table for better visualization\n",
        "        pivot_accuracy = comparison_df.pivot_table(\n",
        "            values='Accuracy_Test',\n",
        "            index=['Model', 'Type', 'Pruning_Method'],\n",
        "            columns='Pruning_Ratio',\n",
        "            fill_value=np.nan\n",
        "        )\n",
        "        pivot_f1_weighted = comparison_df.pivot_table(\n",
        "            values='F1_Weighted_Test',\n",
        "            index=['Model', 'Type', 'Pruning_Method'],\n",
        "            columns='Pruning_Ratio',\n",
        "            fill_value=np.nan\n",
        "        )\n",
        "        pivot_auc_roc = comparison_df.pivot_table(\n",
        "            values='AUC_ROC_Test',\n",
        "            index=['Model', 'Type', 'Pruning_Method'],\n",
        "            columns='Pruning_Ratio',\n",
        "            fill_value=np.nan\n",
        "        )\n",
        "        pivot_auc_pr = comparison_df.pivot_table(\n",
        "            values='AUC_PR_Test',\n",
        "            index=['Model', 'Type', 'Pruning_Method'],\n",
        "            columns='Pruning_Ratio',\n",
        "            fill_value=np.nan\n",
        "        )\n",
        "\n",
        "        print(\"\\nAccuracy Performance Across Pruning Levels:\")\n",
        "        print(pivot_accuracy.round(4))\n",
        "\n",
        "        print(\"\\nF1 Weighted Performance Across Pruning Levels:\")\n",
        "        print(pivot_f1_weighted.round(4))\n",
        "\n",
        "        print(\"\\nAUC_ROC Weighted Performance Across Pruning Levels:\")\n",
        "        print(pivot_auc_roc.round(4))\n",
        "\n",
        "        print(\"\\nAUC_PR Weighted Performance Across Pruning Levels:\")\n",
        "        print(pivot_auc_pr.round(4))\n",
        "\n",
        "        pruning_levels = [0.3, 0.5, 0.7, 0.9]\n",
        "\n",
        "        print(f\"\\n{'='*120}\")\n",
        "        print(\"PERFORMANCE RETENTION AT DIFFERENT PRUNING LEVELS\")\n",
        "        print(\"=\"*120)\n",
        "\n",
        "        for pruning_level in pruning_levels:\n",
        "            print(f\"\\n{'-'*60}\")\n",
        "            print(f\"PERFORMANCE RETENTION AT {int(pruning_level*100)}% PRUNING\")\n",
        "            print(f\"{'-'*60}\")\n",
        "\n",
        "            retention_summary = []\n",
        "            for model in comparison_df['Model'].unique():\n",
        "                model_data = comparison_df[comparison_df['Model'] == model]\n",
        "                baseline = model_data[model_data['Pruning_Ratio'] == 0.0]\n",
        "                pruned = model_data[model_data['Pruning_Ratio'] == pruning_level]\n",
        "\n",
        "                if len(baseline) > 0 and len(pruned) > 0:\n",
        "                    baseline_acc = baseline.iloc[0]['Accuracy_Test']\n",
        "                    pruned_acc = pruned.iloc[0]['Accuracy_Test']\n",
        "                    retention = pruned_acc / baseline_acc if baseline_acc != 0 else 0\n",
        "\n",
        "                    retention_summary.append({\n",
        "                        'Model': model,\n",
        "                        'Type': baseline.iloc[0]['Type'],\n",
        "                        'Method': baseline.iloc[0]['Pruning_Method'],\n",
        "                        'Baseline_Accuracy': baseline_acc,\n",
        "                        'Pruned_Accuracy': pruned_acc,\n",
        "                        'Retention': retention,\n",
        "                        'Compression': pruned.iloc[0]['Compression']\n",
        "                    })\n",
        "\n",
        "            if retention_summary:\n",
        "                retention_df = pd.DataFrame(retention_summary).sort_values('Retention', ascending=False)\n",
        "                print(retention_df.round(4))\n",
        "\n",
        "            best_model = retention_df.iloc[0]\n",
        "            print(f\"\\nBEST PRUNING METHOD AT {int(pruning_level*100)}% LEVEL:\")\n",
        "            print(f\"Model: {best_model['Model']} ({best_model['Type']})\")\n",
        "            print(f\"Method: {best_model['Method']}\")\n",
        "            print(f\"Performance Retention: {best_model['Retention']:.1%}\")\n",
        "            print(f\"Compression Achieved: {best_model['Compression']:.1f}x\")\n",
        "\n",
        "    else:\n",
        "        print(\"No pruning results available for comparison.\")\n",
        "\n",
        "# Run comprehensive comparison\n",
        "compare_all_pruning_methods()\n",
        "\n",
        "print(\"\\n\" + \"=\"*120)\n",
        "print(\"ABLATION STUDY COMPLETE\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "vnXzcSTc2Z4u",
        "pcIEc735lViq",
        "GveBLWbLlViq",
        "H_bIhHTRKAzQ",
        "ZmQB-Bd0rMVH",
        "sM7vW1UglVi8",
        "7xPxYN70lVi_",
        "xlwH7ptOzQwq",
        "W1Ljjq9rZJZo",
        "Etc1WebYZlib",
        "CcnVXcxkzVxm",
        "OVD5006Oza85",
        "exVLSjhpZ88b",
        "5uIugxCaMC9R",
        "EvcGaDcg8Q8R",
        "WtGjfjViDQKq",
        "zVIP_87as9BR",
        "rTaC-bLMC8tW"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}